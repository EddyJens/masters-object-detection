{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8931abf-4b0b-4731-adf9-7c5b2840c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import gc\n",
    "\n",
    "from visualize_image import visualize_one_xy_slice_in_3d_image\n",
    "from loading_dataset import load_data\n",
    "from model import load_model\n",
    "import numpy as np\n",
    "from monai.data import box_utils\n",
    "from monai.apps.detection.metrics.matching import matching_batch\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from monai.apps.detection.metrics.coco import COCOMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6948feac-77f3-43ab-91e3-5e2a79c3dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'Eddy'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'Usp1#'\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = 'HC'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2c25b3-b7bc-4abc-856a-f2a4d88259bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box_mode = 'cccwhd'\n",
    "batch_size = 4\n",
    "patch_size = [96,96,40]\n",
    "# data_list_file_path = '/data/output/hc_train_val_resampled.json'\n",
    "data_list_file_path = '/data/output/LUNA16_datasplit/dataset_fold0.json'\n",
    "# data_base_dir = '/data/HC_Images_resample/'\n",
    "data_base_dir = '/data/LUNA16_Images_resample/'\n",
    "amp=True\n",
    "\n",
    "returned_layers = [1,2]\n",
    "base_anchor_shapes = [[6,8,4],[8,6,5],[10,10,6]]\n",
    "conv1_t_stride = [2,2,1]\n",
    "n_input_channels = 1\n",
    "spatial_dims = 3\n",
    "fg_labels = [0]\n",
    "verbose = False\n",
    "balanced_sampler_pos_fraction = 0.3\n",
    "score_thresh = 0.02\n",
    "nms_thresh = 0.22\n",
    "val_patch_size = [256,256,104]\n",
    "\n",
    "lr = 1e-2\n",
    "val_interval = 5\n",
    "coco_metric = COCOMetric(classes=[\"nodule\"], iou_list=[0.1], max_detection=[100])\n",
    "best_val_epoch_metric = 0.0\n",
    "best_val_epoch = -1\n",
    "max_epochs = 20\n",
    "w_cls = 1.0\n",
    "\n",
    "compute_dtype = torch.float32\n",
    "if amp:\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "# monai.config.print_config()\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ad2cc-31a4-4f4d-a131-256b6cfd9e61",
   "metadata": {},
   "source": [
    "### loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9bf6dd-f147-4974-8b8b-54204a268866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, len_train_ds = load_data(\n",
    "    gt_box_mode, patch_size, batch_size, amp, data_list_file_path, data_base_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e514-7ab6-45dd-bd31-2f7bf68ff2d6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf21214-041f-401e-a2f7-f0072929bf90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector, device = load_model(\n",
    "    returned_layers, base_anchor_shapes, conv1_t_stride, n_input_channels,\n",
    "    spatial_dims, fg_labels, verbose, balanced_sampler_pos_fraction,\n",
    "    score_thresh, nms_thresh, val_patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7938680-00c0-4d79-bb31-4320bac9e332",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Initialize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7489f0c5-5ffc-4005-994d-8d74c85cae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    detector.network.parameters(),\n",
    "    lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=3e-5,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "after_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=150, gamma=0.1\n",
    ")\n",
    "scheduler_warmup = GradualWarmupScheduler(\n",
    "    optimizer, multiplier=1, total_epoch=10, after_scheduler=after_scheduler\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler() if amp else None\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ca37e-e17d-4e84-b9bc-4029ba3dcaf2",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5beae8c-e323-4dd2-84b6-eeebf82bf10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/20\n",
      "1/507, train_loss: 1.2227\n",
      "2/507, train_loss: 1.7676\n",
      "3/507, train_loss: 1.1689\n",
      "4/507, train_loss: 0.7012\n",
      "5/507, train_loss: 1.3906\n",
      "6/507, train_loss: 1.1426\n",
      "7/507, train_loss: 1.4268\n",
      "8/507, train_loss: 1.0781\n",
      "9/507, train_loss: 1.2617\n",
      "10/507, train_loss: 0.3298\n",
      "11/507, train_loss: 1.8291\n",
      "12/507, train_loss: 0.8418\n",
      "13/507, train_loss: 1.2256\n",
      "14/507, train_loss: 1.0635\n",
      "15/507, train_loss: 1.1865\n",
      "16/507, train_loss: 0.7432\n",
      "17/507, train_loss: 0.8750\n",
      "18/507, train_loss: 0.8848\n",
      "19/507, train_loss: 1.3926\n",
      "20/507, train_loss: 0.9287\n",
      "21/507, train_loss: 1.1523\n",
      "22/507, train_loss: 0.9502\n",
      "23/507, train_loss: 1.1104\n",
      "24/507, train_loss: 1.2412\n",
      "25/507, train_loss: 1.4229\n",
      "26/507, train_loss: 0.8657\n",
      "27/507, train_loss: 0.9653\n",
      "28/507, train_loss: 0.9199\n",
      "29/507, train_loss: 0.7383\n",
      "30/507, train_loss: 0.9468\n",
      "31/507, train_loss: 0.8711\n",
      "32/507, train_loss: 0.7817\n",
      "33/507, train_loss: 0.9922\n",
      "34/507, train_loss: 0.7461\n",
      "35/507, train_loss: 0.8506\n",
      "36/507, train_loss: 0.7939\n",
      "37/507, train_loss: 0.7363\n",
      "38/507, train_loss: 0.8037\n",
      "39/507, train_loss: 0.2250\n",
      "40/507, train_loss: 0.1120\n",
      "41/507, train_loss: 0.7070\n",
      "42/507, train_loss: 0.5571\n",
      "43/507, train_loss: 1.1934\n",
      "44/507, train_loss: 0.5830\n",
      "45/507, train_loss: 0.9141\n",
      "46/507, train_loss: 0.5361\n",
      "47/507, train_loss: 0.8994\n",
      "48/507, train_loss: 0.6113\n",
      "49/507, train_loss: 0.8154\n",
      "50/507, train_loss: 0.9336\n",
      "51/507, train_loss: 1.1602\n",
      "52/507, train_loss: 0.4138\n",
      "53/507, train_loss: 0.8711\n",
      "54/507, train_loss: 0.5947\n",
      "55/507, train_loss: 1.0439\n",
      "56/507, train_loss: 0.8394\n",
      "57/507, train_loss: 0.7891\n",
      "58/507, train_loss: 0.3181\n",
      "59/507, train_loss: 1.1602\n",
      "60/507, train_loss: 1.0703\n",
      "61/507, train_loss: 0.8535\n",
      "62/507, train_loss: 0.7437\n",
      "63/507, train_loss: 0.8506\n",
      "64/507, train_loss: 0.7134\n",
      "65/507, train_loss: 0.8379\n",
      "66/507, train_loss: 0.8320\n",
      "67/507, train_loss: 0.7251\n",
      "68/507, train_loss: 0.5869\n",
      "69/507, train_loss: 0.7129\n",
      "70/507, train_loss: 0.6758\n",
      "71/507, train_loss: 0.8242\n",
      "72/507, train_loss: 0.3176\n",
      "73/507, train_loss: 0.6265\n",
      "74/507, train_loss: 0.6455\n",
      "75/507, train_loss: 0.6812\n",
      "76/507, train_loss: 0.0869\n",
      "77/507, train_loss: 0.6445\n",
      "78/507, train_loss: 0.8267\n",
      "79/507, train_loss: 0.8359\n",
      "80/507, train_loss: 0.7588\n",
      "81/507, train_loss: 0.8105\n",
      "82/507, train_loss: 0.1108\n",
      "83/507, train_loss: 0.7910\n",
      "84/507, train_loss: 0.6333\n",
      "85/507, train_loss: 0.5693\n",
      "86/507, train_loss: 0.4258\n",
      "87/507, train_loss: 0.8057\n",
      "88/507, train_loss: 0.8433\n",
      "89/507, train_loss: 0.9287\n",
      "90/507, train_loss: 0.6909\n",
      "91/507, train_loss: 0.5645\n",
      "92/507, train_loss: 0.5615\n",
      "93/507, train_loss: 0.6089\n",
      "94/507, train_loss: 0.8447\n",
      "95/507, train_loss: 0.6636\n",
      "96/507, train_loss: 0.8086\n",
      "97/507, train_loss: 0.4590\n",
      "98/507, train_loss: 0.8862\n",
      "99/507, train_loss: 0.5674\n",
      "100/507, train_loss: 0.8452\n",
      "101/507, train_loss: 0.7461\n",
      "102/507, train_loss: 0.6577\n",
      "103/507, train_loss: 0.6157\n",
      "104/507, train_loss: 0.8418\n",
      "105/507, train_loss: 1.0166\n",
      "106/507, train_loss: 0.6724\n",
      "107/507, train_loss: 0.6235\n",
      "108/507, train_loss: 0.6509\n",
      "109/507, train_loss: 0.5859\n",
      "110/507, train_loss: 0.6826\n",
      "111/507, train_loss: 0.9404\n",
      "112/507, train_loss: 0.7246\n",
      "113/507, train_loss: 0.4751\n",
      "114/507, train_loss: 0.6533\n",
      "115/507, train_loss: 0.5703\n",
      "116/507, train_loss: 0.6982\n",
      "117/507, train_loss: 0.6133\n",
      "118/507, train_loss: 0.5781\n",
      "119/507, train_loss: 0.5894\n",
      "120/507, train_loss: 0.7993\n",
      "121/507, train_loss: 0.9248\n",
      "122/507, train_loss: 0.6592\n",
      "123/507, train_loss: 0.8330\n",
      "124/507, train_loss: 0.6675\n",
      "125/507, train_loss: 0.5156\n",
      "126/507, train_loss: 0.5972\n",
      "127/507, train_loss: 0.3181\n",
      "128/507, train_loss: 0.8853\n",
      "129/507, train_loss: 0.5957\n",
      "130/507, train_loss: 0.6626\n",
      "131/507, train_loss: 0.5405\n",
      "132/507, train_loss: 0.7739\n",
      "133/507, train_loss: 0.7202\n",
      "134/507, train_loss: 0.6890\n",
      "135/507, train_loss: 0.4819\n",
      "136/507, train_loss: 0.8462\n",
      "137/507, train_loss: 0.8765\n",
      "138/507, train_loss: 0.4697\n",
      "139/507, train_loss: 0.9141\n",
      "140/507, train_loss: 0.4600\n",
      "141/507, train_loss: 0.7158\n",
      "142/507, train_loss: 0.5923\n",
      "143/507, train_loss: 0.7363\n",
      "144/507, train_loss: 0.7275\n",
      "145/507, train_loss: 0.5732\n",
      "146/507, train_loss: 0.6694\n",
      "147/507, train_loss: 0.6001\n",
      "148/507, train_loss: 0.6729\n",
      "149/507, train_loss: 0.6060\n",
      "150/507, train_loss: 0.5430\n",
      "151/507, train_loss: 0.7148\n",
      "152/507, train_loss: 0.6831\n",
      "153/507, train_loss: 0.6758\n",
      "154/507, train_loss: 0.4851\n",
      "155/507, train_loss: 0.1549\n",
      "156/507, train_loss: 0.7754\n",
      "157/507, train_loss: 0.9922\n",
      "158/507, train_loss: 0.9248\n",
      "159/507, train_loss: 0.5625\n",
      "160/507, train_loss: 0.4365\n",
      "161/507, train_loss: 0.6489\n",
      "162/507, train_loss: 0.6353\n",
      "163/507, train_loss: 0.4995\n",
      "164/507, train_loss: 0.4590\n",
      "165/507, train_loss: 0.6470\n",
      "166/507, train_loss: 0.8628\n",
      "167/507, train_loss: 0.6074\n",
      "168/507, train_loss: 0.3535\n",
      "169/507, train_loss: 0.3601\n",
      "170/507, train_loss: 0.7378\n",
      "171/507, train_loss: 0.6245\n",
      "172/507, train_loss: 0.7109\n",
      "173/507, train_loss: 0.6094\n",
      "174/507, train_loss: 0.1415\n",
      "175/507, train_loss: 0.5801\n",
      "176/507, train_loss: 0.4104\n",
      "177/507, train_loss: 0.4766\n",
      "178/507, train_loss: 0.6519\n",
      "179/507, train_loss: 0.8193\n",
      "180/507, train_loss: 0.7617\n",
      "181/507, train_loss: 0.8877\n",
      "182/507, train_loss: 0.6074\n",
      "183/507, train_loss: 0.7246\n",
      "184/507, train_loss: 0.7129\n",
      "185/507, train_loss: 0.6284\n",
      "186/507, train_loss: 0.7583\n",
      "187/507, train_loss: 0.7710\n",
      "188/507, train_loss: 0.6602\n",
      "189/507, train_loss: 0.7583\n",
      "190/507, train_loss: 0.5537\n",
      "191/507, train_loss: 0.4619\n",
      "192/507, train_loss: 0.6924\n",
      "193/507, train_loss: 0.5845\n",
      "194/507, train_loss: 0.5645\n",
      "195/507, train_loss: 0.5986\n",
      "196/507, train_loss: 0.6372\n",
      "197/507, train_loss: 0.1748\n",
      "198/507, train_loss: 0.5586\n",
      "199/507, train_loss: 0.4062\n",
      "200/507, train_loss: 0.6460\n",
      "201/507, train_loss: 0.1187\n",
      "202/507, train_loss: 0.8926\n",
      "203/507, train_loss: 0.7656\n",
      "204/507, train_loss: 0.5000\n",
      "205/507, train_loss: 0.7407\n",
      "206/507, train_loss: 0.6387\n",
      "207/507, train_loss: 0.4326\n",
      "208/507, train_loss: 0.6309\n",
      "209/507, train_loss: 0.6982\n",
      "210/507, train_loss: 0.7358\n",
      "211/507, train_loss: 0.4165\n",
      "212/507, train_loss: 0.5444\n",
      "213/507, train_loss: 0.1580\n",
      "214/507, train_loss: 0.4482\n",
      "215/507, train_loss: 0.4453\n",
      "216/507, train_loss: 0.4453\n",
      "217/507, train_loss: 0.6914\n",
      "218/507, train_loss: 0.6675\n",
      "219/507, train_loss: 0.7397\n",
      "220/507, train_loss: 0.9648\n",
      "221/507, train_loss: 0.6064\n",
      "222/507, train_loss: 0.7295\n",
      "223/507, train_loss: 0.2231\n",
      "224/507, train_loss: 0.7676\n",
      "225/507, train_loss: 0.4602\n",
      "226/507, train_loss: 0.7280\n",
      "227/507, train_loss: 0.6143\n",
      "228/507, train_loss: 0.6221\n",
      "229/507, train_loss: 0.4939\n",
      "230/507, train_loss: 0.1526\n",
      "231/507, train_loss: 0.4443\n",
      "232/507, train_loss: 0.6050\n",
      "233/507, train_loss: 0.7617\n",
      "234/507, train_loss: 0.7539\n",
      "235/507, train_loss: 0.8423\n",
      "236/507, train_loss: 0.4292\n",
      "237/507, train_loss: 0.6445\n",
      "238/507, train_loss: 0.9053\n",
      "239/507, train_loss: 0.7832\n",
      "240/507, train_loss: 0.6689\n",
      "241/507, train_loss: 0.6836\n",
      "242/507, train_loss: 0.6455\n",
      "243/507, train_loss: 0.7065\n",
      "244/507, train_loss: 0.5332\n",
      "245/507, train_loss: 0.2303\n",
      "246/507, train_loss: 0.6167\n",
      "247/507, train_loss: 0.6660\n",
      "248/507, train_loss: 0.7661\n",
      "249/507, train_loss: 0.8413\n",
      "250/507, train_loss: 0.5410\n",
      "251/507, train_loss: 1.0186\n",
      "252/507, train_loss: 0.6587\n",
      "253/507, train_loss: 0.2025\n",
      "254/507, train_loss: 0.7295\n",
      "255/507, train_loss: 0.6738\n",
      "256/507, train_loss: 0.4297\n",
      "257/507, train_loss: 0.6924\n",
      "258/507, train_loss: 0.8213\n",
      "259/507, train_loss: 0.5859\n",
      "260/507, train_loss: 0.7041\n",
      "261/507, train_loss: 0.7061\n",
      "262/507, train_loss: 0.9307\n",
      "263/507, train_loss: 0.1536\n",
      "264/507, train_loss: 0.5210\n",
      "265/507, train_loss: 0.8833\n",
      "266/507, train_loss: 0.6875\n",
      "267/507, train_loss: 0.6831\n",
      "268/507, train_loss: 0.7417\n",
      "269/507, train_loss: 0.7661\n",
      "270/507, train_loss: 0.5474\n",
      "271/507, train_loss: 0.1932\n",
      "272/507, train_loss: 0.7676\n",
      "273/507, train_loss: 0.6797\n",
      "274/507, train_loss: 0.7944\n",
      "275/507, train_loss: 0.7866\n",
      "276/507, train_loss: 0.7275\n",
      "277/507, train_loss: 0.8892\n",
      "278/507, train_loss: 0.6328\n",
      "279/507, train_loss: 0.7993\n",
      "280/507, train_loss: 0.7261\n",
      "281/507, train_loss: 0.5117\n",
      "282/507, train_loss: 0.7109\n",
      "283/507, train_loss: 0.8892\n",
      "284/507, train_loss: 0.5938\n",
      "285/507, train_loss: 0.5938\n",
      "286/507, train_loss: 0.6182\n",
      "287/507, train_loss: 0.5498\n",
      "288/507, train_loss: 0.8105\n",
      "289/507, train_loss: 0.5425\n",
      "290/507, train_loss: 0.6572\n",
      "291/507, train_loss: 0.5889\n",
      "292/507, train_loss: 0.7559\n",
      "293/507, train_loss: 0.6221\n",
      "294/507, train_loss: 0.8511\n",
      "295/507, train_loss: 0.4939\n",
      "296/507, train_loss: 0.6709\n",
      "297/507, train_loss: 0.4883\n",
      "298/507, train_loss: 0.6963\n",
      "299/507, train_loss: 0.5908\n",
      "300/507, train_loss: 0.1719\n",
      "301/507, train_loss: 0.1519\n",
      "302/507, train_loss: 0.5933\n",
      "303/507, train_loss: 0.4639\n",
      "304/507, train_loss: 0.6006\n",
      "305/507, train_loss: 0.6357\n",
      "306/507, train_loss: 0.5410\n",
      "307/507, train_loss: 0.4526\n",
      "308/507, train_loss: 0.4009\n",
      "309/507, train_loss: 0.5576\n",
      "310/507, train_loss: 0.6260\n",
      "311/507, train_loss: 0.2771\n",
      "312/507, train_loss: 0.1300\n",
      "313/507, train_loss: 0.1168\n",
      "314/507, train_loss: 0.9917\n",
      "315/507, train_loss: 0.6592\n",
      "316/507, train_loss: 0.6357\n",
      "317/507, train_loss: 0.6182\n",
      "318/507, train_loss: 0.6221\n",
      "319/507, train_loss: 0.6279\n",
      "320/507, train_loss: 0.4321\n",
      "321/507, train_loss: 0.7046\n",
      "322/507, train_loss: 0.6597\n",
      "323/507, train_loss: 0.5645\n",
      "324/507, train_loss: 0.5225\n",
      "325/507, train_loss: 0.6226\n",
      "326/507, train_loss: 0.4128\n",
      "327/507, train_loss: 0.6904\n",
      "328/507, train_loss: 0.5796\n",
      "329/507, train_loss: 0.5557\n",
      "330/507, train_loss: 0.5596\n",
      "331/507, train_loss: 0.1606\n",
      "332/507, train_loss: 0.1359\n",
      "333/507, train_loss: 0.4644\n",
      "334/507, train_loss: 0.5010\n",
      "335/507, train_loss: 0.7197\n",
      "336/507, train_loss: 0.4395\n",
      "337/507, train_loss: 0.0898\n",
      "338/507, train_loss: 0.5469\n",
      "339/507, train_loss: 1.2412\n",
      "340/507, train_loss: 0.7256\n",
      "341/507, train_loss: 0.1145\n",
      "342/507, train_loss: 0.6675\n",
      "343/507, train_loss: 0.7739\n",
      "344/507, train_loss: 0.8564\n",
      "345/507, train_loss: 0.7441\n",
      "346/507, train_loss: 0.6123\n",
      "347/507, train_loss: 0.5605\n",
      "348/507, train_loss: 0.6846\n",
      "349/507, train_loss: 0.4580\n",
      "350/507, train_loss: 0.2051\n",
      "351/507, train_loss: 0.6201\n",
      "352/507, train_loss: 0.5229\n",
      "353/507, train_loss: 0.9258\n",
      "354/507, train_loss: 0.8340\n",
      "355/507, train_loss: 0.4663\n",
      "356/507, train_loss: 0.6265\n",
      "357/507, train_loss: 0.4243\n",
      "358/507, train_loss: 0.9619\n",
      "359/507, train_loss: 0.5469\n",
      "360/507, train_loss: 0.7896\n",
      "361/507, train_loss: 0.8696\n",
      "362/507, train_loss: 0.7017\n",
      "363/507, train_loss: 0.5103\n",
      "364/507, train_loss: 0.7520\n",
      "365/507, train_loss: 0.4753\n",
      "366/507, train_loss: 0.7373\n",
      "367/507, train_loss: 0.1700\n",
      "368/507, train_loss: 0.6616\n",
      "369/507, train_loss: 0.7759\n",
      "370/507, train_loss: 0.8442\n",
      "371/507, train_loss: 0.6514\n",
      "372/507, train_loss: 0.6353\n",
      "373/507, train_loss: 0.5718\n",
      "374/507, train_loss: 0.6504\n",
      "375/507, train_loss: 0.5879\n",
      "376/507, train_loss: 0.7456\n",
      "377/507, train_loss: 0.5479\n",
      "378/507, train_loss: 0.6572\n",
      "379/507, train_loss: 0.4348\n",
      "380/507, train_loss: 0.7329\n",
      "381/507, train_loss: 0.8208\n",
      "382/507, train_loss: 0.6436\n",
      "383/507, train_loss: 0.6807\n",
      "384/507, train_loss: 0.6846\n",
      "385/507, train_loss: 0.4380\n",
      "386/507, train_loss: 0.6958\n",
      "387/507, train_loss: 0.6479\n",
      "388/507, train_loss: 0.6309\n",
      "389/507, train_loss: 0.7725\n",
      "390/507, train_loss: 0.6768\n",
      "391/507, train_loss: 0.4214\n",
      "392/507, train_loss: 0.6021\n",
      "393/507, train_loss: 0.5605\n",
      "394/507, train_loss: 0.5015\n",
      "395/507, train_loss: 0.7651\n",
      "396/507, train_loss: 0.8291\n",
      "397/507, train_loss: 0.7456\n",
      "398/507, train_loss: 0.4519\n",
      "399/507, train_loss: 0.4065\n",
      "400/507, train_loss: 0.8569\n",
      "401/507, train_loss: 0.6343\n",
      "402/507, train_loss: 0.7241\n",
      "403/507, train_loss: 0.5972\n",
      "404/507, train_loss: 0.8452\n",
      "405/507, train_loss: 0.5327\n",
      "406/507, train_loss: 0.6846\n",
      "407/507, train_loss: 0.4561\n",
      "408/507, train_loss: 0.6348\n",
      "409/507, train_loss: 0.5801\n",
      "410/507, train_loss: 0.7710\n",
      "411/507, train_loss: 0.6128\n",
      "412/507, train_loss: 0.6538\n",
      "413/507, train_loss: 0.6162\n",
      "414/507, train_loss: 0.6675\n",
      "415/507, train_loss: 0.7925\n",
      "416/507, train_loss: 0.5479\n",
      "417/507, train_loss: 0.6919\n",
      "418/507, train_loss: 0.6143\n",
      "419/507, train_loss: 0.8296\n",
      "420/507, train_loss: 0.5938\n",
      "421/507, train_loss: 0.5254\n",
      "422/507, train_loss: 0.7246\n",
      "423/507, train_loss: 0.7397\n",
      "424/507, train_loss: 0.6582\n",
      "425/507, train_loss: 0.7471\n",
      "426/507, train_loss: 0.5850\n",
      "427/507, train_loss: 0.4968\n",
      "428/507, train_loss: 0.6289\n",
      "429/507, train_loss: 0.1874\n",
      "430/507, train_loss: 0.3792\n",
      "431/507, train_loss: 0.5718\n",
      "432/507, train_loss: 0.5444\n",
      "433/507, train_loss: 0.6777\n",
      "434/507, train_loss: 0.3877\n",
      "435/507, train_loss: 0.5229\n",
      "436/507, train_loss: 0.7207\n",
      "437/507, train_loss: 0.9219\n",
      "438/507, train_loss: 0.6553\n",
      "439/507, train_loss: 0.5962\n",
      "440/507, train_loss: 0.6582\n",
      "441/507, train_loss: 0.4080\n",
      "442/507, train_loss: 0.7197\n",
      "443/507, train_loss: 0.5059\n",
      "444/507, train_loss: 0.6045\n",
      "445/507, train_loss: 0.8979\n",
      "446/507, train_loss: 0.5786\n",
      "447/507, train_loss: 0.9434\n",
      "448/507, train_loss: 0.4600\n",
      "449/507, train_loss: 0.6987\n",
      "450/507, train_loss: 0.3787\n",
      "451/507, train_loss: 0.5781\n",
      "452/507, train_loss: 0.4331\n",
      "453/507, train_loss: 0.6206\n",
      "454/507, train_loss: 0.5332\n",
      "455/507, train_loss: 0.7207\n",
      "456/507, train_loss: 0.6743\n",
      "457/507, train_loss: 0.5010\n",
      "458/507, train_loss: 0.5776\n",
      "459/507, train_loss: 0.1500\n",
      "460/507, train_loss: 0.5688\n",
      "461/507, train_loss: 0.1296\n",
      "462/507, train_loss: 0.5166\n",
      "463/507, train_loss: 0.5112\n",
      "464/507, train_loss: 0.5498\n",
      "465/507, train_loss: 0.8286\n",
      "466/507, train_loss: 0.5854\n",
      "467/507, train_loss: 0.6860\n",
      "468/507, train_loss: 0.4834\n",
      "469/507, train_loss: 0.4106\n",
      "470/507, train_loss: 0.4961\n",
      "471/507, train_loss: 0.5996\n",
      "472/507, train_loss: 0.1333\n",
      "473/507, train_loss: 0.7720\n",
      "474/507, train_loss: 0.6421\n",
      "475/507, train_loss: 0.6807\n",
      "476/507, train_loss: 0.6357\n",
      "477/507, train_loss: 0.6372\n",
      "478/507, train_loss: 0.4067\n",
      "479/507, train_loss: 0.6001\n",
      "480/507, train_loss: 0.8057\n",
      "481/507, train_loss: 0.4587\n",
      "482/507, train_loss: 0.7798\n",
      "483/507, train_loss: 0.5137\n",
      "484/507, train_loss: 0.6724\n",
      "485/507, train_loss: 0.8120\n",
      "486/507, train_loss: 0.6357\n",
      "487/507, train_loss: 0.6714\n",
      "488/507, train_loss: 0.2295\n",
      "489/507, train_loss: 0.6724\n",
      "490/507, train_loss: 0.6816\n",
      "491/507, train_loss: 0.6982\n",
      "492/507, train_loss: 0.7324\n",
      "493/507, train_loss: 0.4458\n",
      "494/507, train_loss: 0.5000\n",
      "495/507, train_loss: 0.5591\n",
      "496/507, train_loss: 0.5254\n",
      "497/507, train_loss: 0.5625\n",
      "498/507, train_loss: 0.7900\n",
      "499/507, train_loss: 0.9268\n",
      "500/507, train_loss: 0.9165\n",
      "501/507, train_loss: 0.4041\n",
      "502/507, train_loss: 0.7295\n",
      "503/507, train_loss: 0.6475\n",
      "504/507, train_loss: 0.4443\n",
      "505/507, train_loss: 0.5093\n",
      "506/507, train_loss: 0.4316\n",
      "507/507, train_loss: 0.4485\n",
      "epoch 1 average loss: 0.6560\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 05:23:25 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 05:23:28 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 2/20\n",
      "1/507, train_loss: 0.5347\n",
      "2/507, train_loss: 0.4849\n",
      "3/507, train_loss: 0.4937\n",
      "4/507, train_loss: 0.6538\n",
      "5/507, train_loss: 0.6011\n",
      "6/507, train_loss: 0.1361\n",
      "7/507, train_loss: 0.1171\n",
      "8/507, train_loss: 0.4771\n",
      "9/507, train_loss: 0.5176\n",
      "10/507, train_loss: 0.6094\n",
      "11/507, train_loss: 0.8423\n",
      "12/507, train_loss: 0.6372\n",
      "13/507, train_loss: 0.8003\n",
      "14/507, train_loss: 0.8179\n",
      "15/507, train_loss: 0.8818\n",
      "16/507, train_loss: 0.5820\n",
      "17/507, train_loss: 0.4546\n",
      "18/507, train_loss: 0.7256\n",
      "19/507, train_loss: 0.8271\n",
      "20/507, train_loss: 0.8892\n",
      "21/507, train_loss: 0.7222\n",
      "22/507, train_loss: 0.8242\n",
      "23/507, train_loss: 0.8164\n",
      "24/507, train_loss: 0.5264\n",
      "25/507, train_loss: 0.3496\n",
      "26/507, train_loss: 0.7529\n",
      "27/507, train_loss: 0.4292\n",
      "28/507, train_loss: 0.6758\n",
      "29/507, train_loss: 0.1277\n",
      "30/507, train_loss: 0.6704\n",
      "31/507, train_loss: 0.1042\n",
      "32/507, train_loss: 0.6719\n",
      "33/507, train_loss: 0.5967\n",
      "34/507, train_loss: 0.5469\n",
      "35/507, train_loss: 0.6807\n",
      "36/507, train_loss: 0.4939\n",
      "37/507, train_loss: 0.5762\n",
      "38/507, train_loss: 0.5552\n",
      "39/507, train_loss: 0.6094\n",
      "40/507, train_loss: 0.8940\n",
      "41/507, train_loss: 0.7256\n",
      "42/507, train_loss: 0.6514\n",
      "43/507, train_loss: 0.5884\n",
      "44/507, train_loss: 0.7349\n",
      "45/507, train_loss: 0.2334\n",
      "46/507, train_loss: 0.6592\n",
      "47/507, train_loss: 0.5088\n",
      "48/507, train_loss: 0.5781\n",
      "49/507, train_loss: 0.5669\n",
      "50/507, train_loss: 0.3940\n",
      "51/507, train_loss: 0.5352\n",
      "52/507, train_loss: 0.2585\n",
      "53/507, train_loss: 0.8613\n",
      "54/507, train_loss: 0.7764\n",
      "55/507, train_loss: 0.1037\n",
      "56/507, train_loss: 0.9209\n",
      "57/507, train_loss: 0.5068\n",
      "58/507, train_loss: 0.7251\n",
      "59/507, train_loss: 0.4648\n",
      "60/507, train_loss: 0.4673\n",
      "61/507, train_loss: 0.1403\n",
      "62/507, train_loss: 0.7153\n",
      "63/507, train_loss: 0.8340\n",
      "64/507, train_loss: 0.7812\n",
      "65/507, train_loss: 0.7480\n",
      "66/507, train_loss: 0.7280\n",
      "67/507, train_loss: 0.6260\n",
      "68/507, train_loss: 0.2402\n",
      "69/507, train_loss: 0.7056\n",
      "70/507, train_loss: 0.7212\n",
      "71/507, train_loss: 0.7202\n",
      "72/507, train_loss: 0.6157\n",
      "73/507, train_loss: 0.4668\n",
      "74/507, train_loss: 0.5928\n",
      "75/507, train_loss: 0.8105\n",
      "76/507, train_loss: 0.4336\n",
      "77/507, train_loss: 0.7979\n",
      "78/507, train_loss: 0.5654\n",
      "79/507, train_loss: 0.5474\n",
      "80/507, train_loss: 0.6128\n",
      "81/507, train_loss: 0.7891\n",
      "82/507, train_loss: 0.6167\n",
      "83/507, train_loss: 0.6533\n",
      "84/507, train_loss: 0.4956\n",
      "85/507, train_loss: 0.6016\n",
      "86/507, train_loss: 0.8223\n",
      "87/507, train_loss: 0.8320\n",
      "88/507, train_loss: 0.8047\n",
      "89/507, train_loss: 0.5767\n",
      "90/507, train_loss: 0.4568\n",
      "91/507, train_loss: 0.7266\n",
      "92/507, train_loss: 0.5181\n",
      "93/507, train_loss: 0.5703\n",
      "94/507, train_loss: 0.6021\n",
      "95/507, train_loss: 0.7612\n",
      "96/507, train_loss: 0.6904\n",
      "97/507, train_loss: 0.7070\n",
      "98/507, train_loss: 0.4863\n",
      "99/507, train_loss: 0.6172\n",
      "100/507, train_loss: 0.8496\n",
      "101/507, train_loss: 0.5278\n",
      "102/507, train_loss: 0.4075\n",
      "103/507, train_loss: 0.4780\n",
      "104/507, train_loss: 0.7578\n",
      "105/507, train_loss: 0.7266\n",
      "106/507, train_loss: 0.6230\n",
      "107/507, train_loss: 0.7690\n",
      "108/507, train_loss: 0.6250\n",
      "109/507, train_loss: 0.7051\n",
      "110/507, train_loss: 0.5723\n",
      "111/507, train_loss: 0.4858\n",
      "112/507, train_loss: 0.6943\n",
      "113/507, train_loss: 0.2241\n",
      "114/507, train_loss: 0.6592\n",
      "115/507, train_loss: 0.4790\n",
      "116/507, train_loss: 0.5488\n",
      "117/507, train_loss: 0.9688\n",
      "118/507, train_loss: 0.9170\n",
      "119/507, train_loss: 0.6289\n",
      "120/507, train_loss: 0.6821\n",
      "121/507, train_loss: 0.3862\n",
      "122/507, train_loss: 0.5474\n",
      "123/507, train_loss: 0.6851\n",
      "124/507, train_loss: 0.5737\n",
      "125/507, train_loss: 0.5859\n",
      "126/507, train_loss: 0.7285\n",
      "127/507, train_loss: 0.5859\n",
      "128/507, train_loss: 0.4800\n",
      "129/507, train_loss: 0.6772\n",
      "130/507, train_loss: 0.5337\n",
      "131/507, train_loss: 0.5645\n",
      "132/507, train_loss: 0.4302\n",
      "133/507, train_loss: 0.3909\n",
      "134/507, train_loss: 0.4321\n",
      "135/507, train_loss: 0.6685\n",
      "136/507, train_loss: 0.1312\n",
      "137/507, train_loss: 0.5562\n",
      "138/507, train_loss: 0.1047\n",
      "139/507, train_loss: 0.8218\n",
      "140/507, train_loss: 0.8643\n",
      "141/507, train_loss: 0.5366\n",
      "142/507, train_loss: 0.8574\n",
      "143/507, train_loss: 0.4055\n",
      "144/507, train_loss: 0.8228\n",
      "145/507, train_loss: 0.7417\n",
      "146/507, train_loss: 0.5649\n",
      "147/507, train_loss: 0.6357\n",
      "148/507, train_loss: 0.8105\n",
      "149/507, train_loss: 0.2590\n",
      "150/507, train_loss: 0.7183\n",
      "151/507, train_loss: 0.4390\n",
      "152/507, train_loss: 0.5059\n",
      "153/507, train_loss: 0.7920\n",
      "154/507, train_loss: 0.6895\n",
      "155/507, train_loss: 0.4924\n",
      "156/507, train_loss: 0.7285\n",
      "157/507, train_loss: 0.5654\n",
      "158/507, train_loss: 0.5879\n",
      "159/507, train_loss: 0.5898\n",
      "160/507, train_loss: 0.6816\n",
      "161/507, train_loss: 0.4883\n",
      "162/507, train_loss: 0.6562\n",
      "163/507, train_loss: 0.5801\n",
      "164/507, train_loss: 0.4197\n",
      "165/507, train_loss: 0.8125\n",
      "166/507, train_loss: 0.4353\n",
      "167/507, train_loss: 0.4966\n",
      "168/507, train_loss: 0.5894\n",
      "169/507, train_loss: 0.3804\n",
      "170/507, train_loss: 0.7109\n",
      "171/507, train_loss: 0.4136\n",
      "172/507, train_loss: 0.3262\n",
      "173/507, train_loss: 0.6982\n",
      "174/507, train_loss: 0.5342\n",
      "175/507, train_loss: 0.6396\n",
      "176/507, train_loss: 0.7632\n",
      "177/507, train_loss: 0.3430\n",
      "178/507, train_loss: 0.4912\n",
      "179/507, train_loss: 0.5942\n",
      "180/507, train_loss: 0.5127\n",
      "181/507, train_loss: 0.7178\n",
      "182/507, train_loss: 0.5420\n",
      "183/507, train_loss: 0.3350\n",
      "184/507, train_loss: 0.8838\n",
      "185/507, train_loss: 0.7354\n",
      "186/507, train_loss: 0.4004\n",
      "187/507, train_loss: 0.1595\n",
      "188/507, train_loss: 0.5649\n",
      "189/507, train_loss: 0.4685\n",
      "190/507, train_loss: 0.4229\n",
      "191/507, train_loss: 0.4233\n",
      "192/507, train_loss: 0.6475\n",
      "193/507, train_loss: 0.6089\n",
      "194/507, train_loss: 0.4646\n",
      "195/507, train_loss: 0.1100\n",
      "196/507, train_loss: 0.6548\n",
      "197/507, train_loss: 0.6802\n",
      "198/507, train_loss: 0.8120\n",
      "199/507, train_loss: 0.5815\n",
      "200/507, train_loss: 0.3594\n",
      "201/507, train_loss: 0.7109\n",
      "202/507, train_loss: 0.4065\n",
      "203/507, train_loss: 0.6938\n",
      "204/507, train_loss: 0.5503\n",
      "205/507, train_loss: 0.6372\n",
      "206/507, train_loss: 0.4502\n",
      "207/507, train_loss: 0.4741\n",
      "208/507, train_loss: 0.8447\n",
      "209/507, train_loss: 0.1560\n",
      "210/507, train_loss: 0.4490\n",
      "211/507, train_loss: 0.7466\n",
      "212/507, train_loss: 0.4915\n",
      "213/507, train_loss: 0.8301\n",
      "214/507, train_loss: 0.3896\n",
      "215/507, train_loss: 0.7705\n",
      "216/507, train_loss: 0.2944\n",
      "217/507, train_loss: 0.7690\n",
      "218/507, train_loss: 0.4539\n",
      "219/507, train_loss: 0.6182\n",
      "220/507, train_loss: 0.5605\n",
      "221/507, train_loss: 0.7378\n",
      "222/507, train_loss: 0.4878\n",
      "223/507, train_loss: 0.4614\n",
      "224/507, train_loss: 0.7012\n",
      "225/507, train_loss: 0.9888\n",
      "226/507, train_loss: 0.5469\n",
      "227/507, train_loss: 0.5220\n",
      "228/507, train_loss: 0.1614\n",
      "229/507, train_loss: 0.7222\n",
      "230/507, train_loss: 0.9326\n",
      "231/507, train_loss: 0.5225\n",
      "232/507, train_loss: 0.3176\n",
      "233/507, train_loss: 0.7124\n",
      "234/507, train_loss: 0.5664\n",
      "235/507, train_loss: 0.5908\n",
      "236/507, train_loss: 0.8877\n",
      "237/507, train_loss: 0.4688\n",
      "238/507, train_loss: 0.7129\n",
      "239/507, train_loss: 0.7104\n",
      "240/507, train_loss: 0.6357\n",
      "241/507, train_loss: 0.2091\n",
      "242/507, train_loss: 0.1719\n",
      "243/507, train_loss: 0.5371\n",
      "244/507, train_loss: 0.5757\n",
      "245/507, train_loss: 0.6616\n",
      "246/507, train_loss: 0.4231\n",
      "247/507, train_loss: 0.7407\n",
      "248/507, train_loss: 0.8213\n",
      "249/507, train_loss: 0.3916\n",
      "250/507, train_loss: 0.1558\n",
      "251/507, train_loss: 0.5752\n",
      "252/507, train_loss: 0.7021\n",
      "253/507, train_loss: 0.5225\n",
      "254/507, train_loss: 0.4255\n",
      "255/507, train_loss: 0.4504\n",
      "256/507, train_loss: 0.3745\n",
      "257/507, train_loss: 0.7217\n",
      "258/507, train_loss: 0.5854\n",
      "259/507, train_loss: 0.7422\n",
      "260/507, train_loss: 0.5664\n",
      "261/507, train_loss: 0.5649\n",
      "262/507, train_loss: 0.5869\n",
      "263/507, train_loss: 0.4626\n",
      "264/507, train_loss: 0.3770\n",
      "265/507, train_loss: 0.4678\n",
      "266/507, train_loss: 0.5996\n",
      "267/507, train_loss: 0.4431\n",
      "268/507, train_loss: 0.4673\n",
      "269/507, train_loss: 0.6206\n",
      "270/507, train_loss: 0.6392\n",
      "271/507, train_loss: 0.1250\n",
      "272/507, train_loss: 0.5161\n",
      "273/507, train_loss: 0.5488\n",
      "274/507, train_loss: 0.6860\n",
      "275/507, train_loss: 0.7114\n",
      "276/507, train_loss: 0.6504\n",
      "277/507, train_loss: 0.5659\n",
      "278/507, train_loss: 0.7202\n",
      "279/507, train_loss: 0.2502\n",
      "280/507, train_loss: 0.7983\n",
      "281/507, train_loss: 0.4370\n",
      "282/507, train_loss: 0.6143\n",
      "283/507, train_loss: 0.7891\n",
      "284/507, train_loss: 0.4922\n",
      "285/507, train_loss: 0.3911\n",
      "286/507, train_loss: 0.1194\n",
      "287/507, train_loss: 0.5947\n",
      "288/507, train_loss: 0.3396\n",
      "289/507, train_loss: 0.3120\n",
      "290/507, train_loss: 0.7490\n",
      "291/507, train_loss: 0.0880\n",
      "292/507, train_loss: 0.4211\n",
      "293/507, train_loss: 0.6641\n",
      "294/507, train_loss: 0.5884\n",
      "295/507, train_loss: 0.6885\n",
      "296/507, train_loss: 0.9033\n",
      "297/507, train_loss: 0.1642\n",
      "298/507, train_loss: 0.6699\n",
      "299/507, train_loss: 0.8774\n",
      "300/507, train_loss: 0.5215\n",
      "301/507, train_loss: 0.4585\n",
      "302/507, train_loss: 0.3333\n",
      "303/507, train_loss: 0.5723\n",
      "304/507, train_loss: 0.5547\n",
      "305/507, train_loss: 0.5801\n",
      "306/507, train_loss: 0.6934\n",
      "307/507, train_loss: 0.8877\n",
      "308/507, train_loss: 0.4565\n",
      "309/507, train_loss: 0.6865\n",
      "310/507, train_loss: 0.5625\n",
      "311/507, train_loss: 0.4824\n",
      "312/507, train_loss: 0.3010\n",
      "313/507, train_loss: 0.4414\n",
      "314/507, train_loss: 0.3726\n",
      "315/507, train_loss: 0.3887\n",
      "316/507, train_loss: 0.4001\n",
      "317/507, train_loss: 0.5752\n",
      "318/507, train_loss: 0.5054\n",
      "319/507, train_loss: 0.1300\n",
      "320/507, train_loss: 0.1047\n",
      "321/507, train_loss: 0.2871\n",
      "322/507, train_loss: 0.3472\n",
      "323/507, train_loss: 0.4644\n",
      "324/507, train_loss: 0.8281\n",
      "325/507, train_loss: 0.4404\n",
      "326/507, train_loss: 0.3447\n",
      "327/507, train_loss: 0.5938\n",
      "328/507, train_loss: 0.5742\n",
      "329/507, train_loss: 0.7852\n",
      "330/507, train_loss: 0.1387\n",
      "331/507, train_loss: 0.5508\n",
      "332/507, train_loss: 0.6450\n",
      "333/507, train_loss: 0.3833\n",
      "334/507, train_loss: 0.5542\n",
      "335/507, train_loss: 0.5225\n",
      "336/507, train_loss: 0.3406\n",
      "337/507, train_loss: 0.8701\n",
      "338/507, train_loss: 0.7222\n",
      "339/507, train_loss: 0.6436\n",
      "340/507, train_loss: 0.4846\n",
      "341/507, train_loss: 0.6792\n",
      "342/507, train_loss: 0.5269\n",
      "343/507, train_loss: 0.3633\n",
      "344/507, train_loss: 0.2915\n",
      "345/507, train_loss: 0.3293\n",
      "346/507, train_loss: 0.7266\n",
      "347/507, train_loss: 0.7280\n",
      "348/507, train_loss: 0.4121\n",
      "349/507, train_loss: 0.6377\n",
      "350/507, train_loss: 0.4309\n",
      "351/507, train_loss: 0.4165\n",
      "352/507, train_loss: 0.7959\n",
      "353/507, train_loss: 0.3591\n",
      "354/507, train_loss: 0.3499\n",
      "355/507, train_loss: 0.6494\n",
      "356/507, train_loss: 0.4978\n",
      "357/507, train_loss: 0.6982\n",
      "358/507, train_loss: 0.6963\n",
      "359/507, train_loss: 0.6309\n",
      "360/507, train_loss: 0.1209\n",
      "361/507, train_loss: 0.5630\n",
      "362/507, train_loss: 0.3440\n",
      "363/507, train_loss: 0.9009\n",
      "364/507, train_loss: 0.6006\n",
      "365/507, train_loss: 0.7544\n",
      "366/507, train_loss: 0.4404\n",
      "367/507, train_loss: 0.4966\n",
      "368/507, train_loss: 0.3364\n",
      "369/507, train_loss: 0.5693\n",
      "370/507, train_loss: 0.6079\n",
      "371/507, train_loss: 0.1593\n",
      "372/507, train_loss: 0.3196\n",
      "373/507, train_loss: 0.3013\n",
      "374/507, train_loss: 0.4189\n",
      "375/507, train_loss: 0.7974\n",
      "376/507, train_loss: 0.4785\n",
      "377/507, train_loss: 0.6768\n",
      "378/507, train_loss: 0.5293\n",
      "379/507, train_loss: 0.5806\n",
      "380/507, train_loss: 0.8130\n",
      "381/507, train_loss: 0.4180\n",
      "382/507, train_loss: 0.8706\n",
      "383/507, train_loss: 0.4595\n",
      "384/507, train_loss: 0.5986\n",
      "385/507, train_loss: 0.7012\n",
      "386/507, train_loss: 0.4922\n",
      "387/507, train_loss: 0.7261\n",
      "388/507, train_loss: 0.4185\n",
      "389/507, train_loss: 0.4177\n",
      "390/507, train_loss: 0.3662\n",
      "391/507, train_loss: 0.8940\n",
      "392/507, train_loss: 0.7280\n",
      "393/507, train_loss: 0.3311\n",
      "394/507, train_loss: 0.4102\n",
      "395/507, train_loss: 0.4377\n",
      "396/507, train_loss: 0.3616\n",
      "397/507, train_loss: 0.4951\n",
      "398/507, train_loss: 0.4578\n",
      "399/507, train_loss: 0.2039\n",
      "400/507, train_loss: 0.4404\n",
      "401/507, train_loss: 0.2493\n",
      "402/507, train_loss: 0.3018\n",
      "403/507, train_loss: 1.0010\n",
      "404/507, train_loss: 0.2703\n",
      "405/507, train_loss: 0.5977\n",
      "406/507, train_loss: 0.4329\n",
      "407/507, train_loss: 0.6768\n",
      "408/507, train_loss: 0.5522\n",
      "409/507, train_loss: 0.3647\n",
      "410/507, train_loss: 0.0853\n",
      "411/507, train_loss: 0.4512\n",
      "412/507, train_loss: 0.4753\n",
      "413/507, train_loss: 0.6372\n",
      "414/507, train_loss: 0.4568\n",
      "415/507, train_loss: 0.5244\n",
      "416/507, train_loss: 0.6562\n",
      "417/507, train_loss: 0.5547\n",
      "418/507, train_loss: 0.5557\n",
      "419/507, train_loss: 0.4866\n",
      "420/507, train_loss: 0.3638\n",
      "421/507, train_loss: 0.4282\n",
      "422/507, train_loss: 0.5620\n",
      "423/507, train_loss: 0.2737\n",
      "424/507, train_loss: 0.7603\n",
      "425/507, train_loss: 0.4475\n",
      "426/507, train_loss: 0.0869\n",
      "427/507, train_loss: 0.3828\n",
      "428/507, train_loss: 0.4177\n",
      "429/507, train_loss: 0.6968\n",
      "430/507, train_loss: 0.4229\n",
      "431/507, train_loss: 0.3318\n",
      "432/507, train_loss: 0.4785\n",
      "433/507, train_loss: 0.6499\n",
      "434/507, train_loss: 0.7256\n",
      "435/507, train_loss: 0.8560\n",
      "436/507, train_loss: 0.1198\n",
      "437/507, train_loss: 0.9521\n",
      "438/507, train_loss: 0.8320\n",
      "439/507, train_loss: 0.5405\n",
      "440/507, train_loss: 0.7124\n",
      "441/507, train_loss: 0.5908\n",
      "442/507, train_loss: 0.5498\n",
      "443/507, train_loss: 0.3315\n",
      "444/507, train_loss: 0.4829\n",
      "445/507, train_loss: 0.6318\n",
      "446/507, train_loss: 0.5298\n",
      "447/507, train_loss: 0.1384\n",
      "448/507, train_loss: 0.7129\n",
      "449/507, train_loss: 0.7544\n",
      "450/507, train_loss: 0.3298\n",
      "451/507, train_loss: 0.7891\n",
      "452/507, train_loss: 0.2363\n",
      "453/507, train_loss: 0.3752\n",
      "454/507, train_loss: 0.6064\n",
      "455/507, train_loss: 0.0795\n",
      "456/507, train_loss: 0.9243\n",
      "457/507, train_loss: 0.4629\n",
      "458/507, train_loss: 0.0883\n",
      "459/507, train_loss: 0.3406\n",
      "460/507, train_loss: 0.5742\n",
      "461/507, train_loss: 0.3921\n",
      "462/507, train_loss: 0.0850\n",
      "463/507, train_loss: 0.0909\n",
      "464/507, train_loss: 0.4614\n",
      "465/507, train_loss: 0.2451\n",
      "466/507, train_loss: 0.5879\n",
      "467/507, train_loss: 0.7852\n",
      "468/507, train_loss: 0.8413\n",
      "469/507, train_loss: 0.3384\n",
      "470/507, train_loss: 0.3149\n",
      "471/507, train_loss: 0.5029\n",
      "472/507, train_loss: 0.4116\n",
      "473/507, train_loss: 0.4575\n",
      "474/507, train_loss: 0.3674\n",
      "475/507, train_loss: 0.6719\n",
      "476/507, train_loss: 0.4941\n",
      "477/507, train_loss: 0.6030\n",
      "478/507, train_loss: 0.6455\n",
      "479/507, train_loss: 0.3079\n",
      "480/507, train_loss: 0.4102\n",
      "481/507, train_loss: 0.4404\n",
      "482/507, train_loss: 0.4041\n",
      "483/507, train_loss: 0.3716\n",
      "484/507, train_loss: 0.0812\n",
      "485/507, train_loss: 0.1559\n",
      "486/507, train_loss: 0.4314\n",
      "487/507, train_loss: 0.2720\n",
      "488/507, train_loss: 0.4673\n",
      "489/507, train_loss: 0.2610\n",
      "490/507, train_loss: 0.7910\n",
      "491/507, train_loss: 0.3120\n",
      "492/507, train_loss: 0.6738\n",
      "493/507, train_loss: 0.3877\n",
      "494/507, train_loss: 0.3438\n",
      "495/507, train_loss: 0.0462\n",
      "496/507, train_loss: 0.3066\n",
      "497/507, train_loss: 0.8062\n",
      "498/507, train_loss: 0.5698\n",
      "499/507, train_loss: 0.4871\n",
      "500/507, train_loss: 0.5815\n",
      "501/507, train_loss: 0.3171\n",
      "502/507, train_loss: 0.5142\n",
      "503/507, train_loss: 0.5605\n",
      "504/507, train_loss: 0.5176\n",
      "505/507, train_loss: 0.4326\n",
      "506/507, train_loss: 0.4700\n",
      "507/507, train_loss: 0.3545\n",
      "epoch 2 average loss: 0.5413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 07:24:57 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 07:25:00 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 3/20\n",
      "1/507, train_loss: 0.5298\n",
      "2/507, train_loss: 0.7056\n",
      "3/507, train_loss: 0.9951\n",
      "4/507, train_loss: 0.5186\n",
      "5/507, train_loss: 0.3240\n",
      "6/507, train_loss: 0.5649\n",
      "7/507, train_loss: 0.5996\n",
      "8/507, train_loss: 0.5728\n",
      "9/507, train_loss: 0.3667\n",
      "10/507, train_loss: 0.3796\n",
      "11/507, train_loss: 0.3623\n",
      "12/507, train_loss: 0.0734\n",
      "13/507, train_loss: 0.1146\n",
      "14/507, train_loss: 0.4541\n",
      "15/507, train_loss: 0.7686\n",
      "16/507, train_loss: 0.2117\n",
      "17/507, train_loss: 0.3682\n",
      "18/507, train_loss: 0.3369\n",
      "19/507, train_loss: 0.8086\n",
      "20/507, train_loss: 0.2761\n",
      "21/507, train_loss: 0.0894\n",
      "22/507, train_loss: 1.0352\n",
      "23/507, train_loss: 0.5049\n",
      "24/507, train_loss: 0.0616\n",
      "25/507, train_loss: 0.7764\n",
      "26/507, train_loss: 0.5464\n",
      "27/507, train_loss: 0.6851\n",
      "28/507, train_loss: 0.6299\n",
      "29/507, train_loss: 0.4595\n",
      "30/507, train_loss: 0.5063\n",
      "31/507, train_loss: 0.5508\n",
      "32/507, train_loss: 0.2900\n",
      "33/507, train_loss: 0.5444\n",
      "34/507, train_loss: 0.9302\n",
      "35/507, train_loss: 0.4500\n",
      "36/507, train_loss: 0.9175\n",
      "37/507, train_loss: 0.6934\n",
      "38/507, train_loss: 0.2517\n",
      "39/507, train_loss: 0.5649\n",
      "40/507, train_loss: 0.6294\n",
      "41/507, train_loss: 0.6108\n",
      "42/507, train_loss: 0.5586\n",
      "43/507, train_loss: 0.7368\n",
      "44/507, train_loss: 0.4448\n",
      "45/507, train_loss: 0.4700\n",
      "46/507, train_loss: 0.5020\n",
      "47/507, train_loss: 0.6572\n",
      "48/507, train_loss: 0.9521\n",
      "49/507, train_loss: 0.4629\n",
      "50/507, train_loss: 0.7324\n",
      "51/507, train_loss: 0.6963\n",
      "52/507, train_loss: 0.2817\n",
      "53/507, train_loss: 0.4028\n",
      "54/507, train_loss: 0.5356\n",
      "55/507, train_loss: 0.5557\n",
      "56/507, train_loss: 0.7837\n",
      "57/507, train_loss: 0.7114\n",
      "58/507, train_loss: 0.1181\n",
      "59/507, train_loss: 0.3711\n",
      "60/507, train_loss: 0.3623\n",
      "61/507, train_loss: 0.6230\n",
      "62/507, train_loss: 0.6450\n",
      "63/507, train_loss: 0.6426\n",
      "64/507, train_loss: 0.2644\n",
      "65/507, train_loss: 0.8247\n",
      "66/507, train_loss: 0.5913\n",
      "67/507, train_loss: 0.6328\n",
      "68/507, train_loss: 0.5176\n",
      "69/507, train_loss: 0.4038\n",
      "70/507, train_loss: 0.3127\n",
      "71/507, train_loss: 0.5371\n",
      "72/507, train_loss: 0.3389\n",
      "73/507, train_loss: 0.8525\n",
      "74/507, train_loss: 0.7241\n",
      "75/507, train_loss: 0.7715\n",
      "76/507, train_loss: 0.5000\n",
      "77/507, train_loss: 0.2203\n",
      "78/507, train_loss: 0.5405\n",
      "79/507, train_loss: 0.3386\n",
      "80/507, train_loss: 0.3115\n",
      "81/507, train_loss: 0.2468\n",
      "82/507, train_loss: 0.8848\n",
      "83/507, train_loss: 0.7573\n",
      "84/507, train_loss: 0.7710\n",
      "85/507, train_loss: 0.2930\n",
      "86/507, train_loss: 0.5923\n",
      "87/507, train_loss: 0.4695\n",
      "88/507, train_loss: 0.4216\n",
      "89/507, train_loss: 0.2781\n",
      "90/507, train_loss: 0.5581\n",
      "91/507, train_loss: 0.1112\n",
      "92/507, train_loss: 0.6074\n",
      "93/507, train_loss: 0.4854\n",
      "94/507, train_loss: 0.4673\n",
      "95/507, train_loss: 0.5537\n",
      "96/507, train_loss: 0.4731\n",
      "97/507, train_loss: 0.4043\n",
      "98/507, train_loss: 0.0902\n",
      "99/507, train_loss: 0.1075\n",
      "100/507, train_loss: 0.3174\n",
      "101/507, train_loss: 0.4365\n",
      "102/507, train_loss: 0.3457\n",
      "103/507, train_loss: 1.0635\n",
      "104/507, train_loss: 0.8154\n",
      "105/507, train_loss: 0.6963\n",
      "106/507, train_loss: 0.4868\n",
      "107/507, train_loss: 0.4067\n",
      "108/507, train_loss: 0.3691\n",
      "109/507, train_loss: 0.1892\n",
      "110/507, train_loss: 0.7031\n",
      "111/507, train_loss: 0.3767\n",
      "112/507, train_loss: 0.3606\n",
      "113/507, train_loss: 0.5967\n",
      "114/507, train_loss: 0.4780\n",
      "115/507, train_loss: 0.3921\n",
      "116/507, train_loss: 0.2485\n",
      "117/507, train_loss: 0.2981\n",
      "118/507, train_loss: 0.4985\n",
      "119/507, train_loss: 0.6245\n",
      "120/507, train_loss: 0.5918\n",
      "121/507, train_loss: 0.0813\n",
      "122/507, train_loss: 0.2551\n",
      "123/507, train_loss: 0.6870\n",
      "124/507, train_loss: 0.4829\n",
      "125/507, train_loss: 0.2700\n",
      "126/507, train_loss: 0.1947\n",
      "127/507, train_loss: 0.3335\n",
      "128/507, train_loss: 0.1750\n",
      "129/507, train_loss: 0.3110\n",
      "130/507, train_loss: 0.3154\n",
      "131/507, train_loss: 0.3872\n",
      "132/507, train_loss: 0.4937\n",
      "133/507, train_loss: 0.3140\n",
      "134/507, train_loss: 0.7964\n",
      "135/507, train_loss: 0.2976\n",
      "136/507, train_loss: 0.3081\n",
      "137/507, train_loss: 0.1865\n",
      "138/507, train_loss: 0.3704\n",
      "139/507, train_loss: 0.2993\n",
      "140/507, train_loss: 0.1151\n",
      "141/507, train_loss: 0.3276\n",
      "142/507, train_loss: 0.6309\n",
      "143/507, train_loss: 0.7451\n",
      "144/507, train_loss: 0.1409\n",
      "145/507, train_loss: 0.1663\n",
      "146/507, train_loss: 0.6289\n",
      "147/507, train_loss: 0.1848\n",
      "148/507, train_loss: 0.2649\n",
      "149/507, train_loss: 0.2009\n",
      "150/507, train_loss: 0.3091\n",
      "151/507, train_loss: 0.4741\n",
      "152/507, train_loss: 1.0273\n",
      "153/507, train_loss: 0.6758\n",
      "154/507, train_loss: 0.1694\n",
      "155/507, train_loss: 0.8413\n",
      "156/507, train_loss: 0.4641\n",
      "157/507, train_loss: 0.8203\n",
      "158/507, train_loss: 0.6660\n",
      "159/507, train_loss: 0.3438\n",
      "160/507, train_loss: 0.1948\n",
      "161/507, train_loss: 0.3430\n",
      "162/507, train_loss: 0.5039\n",
      "163/507, train_loss: 0.0711\n",
      "164/507, train_loss: 0.2744\n",
      "165/507, train_loss: 0.5659\n",
      "166/507, train_loss: 0.6509\n",
      "167/507, train_loss: 0.6650\n",
      "168/507, train_loss: 0.3496\n",
      "169/507, train_loss: 0.2827\n",
      "170/507, train_loss: 0.5244\n",
      "171/507, train_loss: 0.0442\n",
      "172/507, train_loss: 1.1133\n",
      "173/507, train_loss: 0.2529\n",
      "174/507, train_loss: 0.3364\n",
      "175/507, train_loss: 0.2644\n",
      "176/507, train_loss: 0.6738\n",
      "177/507, train_loss: 0.8076\n",
      "178/507, train_loss: 1.0254\n",
      "179/507, train_loss: 0.8115\n",
      "180/507, train_loss: 0.3123\n",
      "181/507, train_loss: 0.2649\n",
      "182/507, train_loss: 0.3020\n",
      "183/507, train_loss: 0.5342\n",
      "184/507, train_loss: 0.6138\n",
      "185/507, train_loss: 0.5972\n",
      "186/507, train_loss: 0.8662\n",
      "187/507, train_loss: 0.3198\n",
      "188/507, train_loss: 0.9434\n",
      "189/507, train_loss: 0.4360\n",
      "190/507, train_loss: 0.2695\n",
      "191/507, train_loss: 0.5127\n",
      "192/507, train_loss: 0.5581\n",
      "193/507, train_loss: 0.2891\n",
      "194/507, train_loss: 0.5439\n",
      "195/507, train_loss: 0.2632\n",
      "196/507, train_loss: 0.3606\n",
      "197/507, train_loss: 0.2883\n",
      "198/507, train_loss: 0.4705\n",
      "199/507, train_loss: 0.3384\n",
      "200/507, train_loss: 0.6064\n",
      "201/507, train_loss: 0.5098\n",
      "202/507, train_loss: 0.0723\n",
      "203/507, train_loss: 0.6841\n",
      "204/507, train_loss: 0.5215\n",
      "205/507, train_loss: 0.3701\n",
      "206/507, train_loss: 1.1680\n",
      "207/507, train_loss: 0.2183\n",
      "208/507, train_loss: 0.3027\n",
      "209/507, train_loss: 0.3015\n",
      "210/507, train_loss: 0.6870\n",
      "211/507, train_loss: 0.3284\n",
      "212/507, train_loss: 0.7031\n",
      "213/507, train_loss: 0.8745\n",
      "214/507, train_loss: 0.3572\n",
      "215/507, train_loss: 0.6177\n",
      "216/507, train_loss: 0.5645\n",
      "217/507, train_loss: 0.2812\n",
      "218/507, train_loss: 0.5737\n",
      "219/507, train_loss: 0.0829\n",
      "220/507, train_loss: 0.2974\n",
      "221/507, train_loss: 0.2905\n",
      "222/507, train_loss: 0.4639\n",
      "223/507, train_loss: 0.4944\n",
      "224/507, train_loss: 0.0688\n",
      "225/507, train_loss: 0.4905\n",
      "226/507, train_loss: 0.2739\n",
      "227/507, train_loss: 0.2527\n",
      "228/507, train_loss: 0.3186\n",
      "229/507, train_loss: 0.4688\n",
      "230/507, train_loss: 0.6050\n",
      "231/507, train_loss: 0.6377\n",
      "232/507, train_loss: 0.2878\n",
      "233/507, train_loss: 0.3940\n",
      "234/507, train_loss: 0.5596\n",
      "235/507, train_loss: 0.7900\n",
      "236/507, train_loss: 0.4011\n",
      "237/507, train_loss: 0.2303\n",
      "238/507, train_loss: 0.3516\n",
      "239/507, train_loss: 0.3901\n",
      "240/507, train_loss: 0.2681\n",
      "241/507, train_loss: 0.6187\n",
      "242/507, train_loss: 0.2520\n",
      "243/507, train_loss: 0.4060\n",
      "244/507, train_loss: 0.1445\n",
      "245/507, train_loss: 1.0322\n",
      "246/507, train_loss: 0.6270\n",
      "247/507, train_loss: 0.8755\n",
      "248/507, train_loss: 0.7197\n",
      "249/507, train_loss: 0.3247\n",
      "250/507, train_loss: 0.5327\n",
      "251/507, train_loss: 0.3271\n",
      "252/507, train_loss: 0.4319\n",
      "253/507, train_loss: 0.2406\n",
      "254/507, train_loss: 0.1975\n",
      "255/507, train_loss: 0.3247\n",
      "256/507, train_loss: 0.4248\n",
      "257/507, train_loss: 0.2025\n",
      "258/507, train_loss: 0.4412\n",
      "259/507, train_loss: 0.5776\n",
      "260/507, train_loss: 0.1937\n",
      "261/507, train_loss: 0.0561\n",
      "262/507, train_loss: 0.7803\n",
      "263/507, train_loss: 0.1221\n",
      "264/507, train_loss: 0.6895\n",
      "265/507, train_loss: 0.2961\n",
      "266/507, train_loss: 0.0943\n",
      "267/507, train_loss: 0.5083\n",
      "268/507, train_loss: 0.5430\n",
      "269/507, train_loss: 0.5796\n",
      "270/507, train_loss: 0.8706\n",
      "271/507, train_loss: 0.4150\n",
      "272/507, train_loss: 0.1619\n",
      "273/507, train_loss: 0.6240\n",
      "274/507, train_loss: 0.2319\n",
      "275/507, train_loss: 0.3306\n",
      "276/507, train_loss: 0.5957\n",
      "277/507, train_loss: 0.3276\n",
      "278/507, train_loss: 0.7148\n",
      "279/507, train_loss: 0.7153\n",
      "280/507, train_loss: 0.3408\n",
      "281/507, train_loss: 0.3459\n",
      "282/507, train_loss: 0.4434\n",
      "283/507, train_loss: 0.2134\n",
      "284/507, train_loss: 0.0626\n",
      "285/507, train_loss: 0.3020\n",
      "286/507, train_loss: 0.3550\n",
      "287/507, train_loss: 0.0893\n",
      "288/507, train_loss: 0.2764\n",
      "289/507, train_loss: 0.2969\n",
      "290/507, train_loss: 0.3911\n",
      "291/507, train_loss: 0.5439\n",
      "292/507, train_loss: 0.2354\n",
      "293/507, train_loss: 0.3286\n",
      "294/507, train_loss: 0.3064\n",
      "295/507, train_loss: 0.9863\n",
      "296/507, train_loss: 0.2029\n",
      "297/507, train_loss: 0.6309\n",
      "298/507, train_loss: 0.2358\n",
      "299/507, train_loss: 0.1042\n",
      "300/507, train_loss: 0.4636\n",
      "301/507, train_loss: 0.3716\n",
      "302/507, train_loss: 0.3118\n",
      "303/507, train_loss: 0.3059\n",
      "304/507, train_loss: 0.6875\n",
      "305/507, train_loss: 0.3003\n",
      "306/507, train_loss: 0.4360\n",
      "307/507, train_loss: 0.2212\n",
      "308/507, train_loss: 0.3672\n",
      "309/507, train_loss: 0.4744\n",
      "310/507, train_loss: 0.3203\n",
      "311/507, train_loss: 0.4917\n",
      "312/507, train_loss: 0.3181\n",
      "313/507, train_loss: 0.2290\n",
      "314/507, train_loss: 0.3179\n",
      "315/507, train_loss: 0.4751\n",
      "316/507, train_loss: 0.2466\n",
      "317/507, train_loss: 0.2102\n",
      "318/507, train_loss: 0.5942\n",
      "319/507, train_loss: 0.2144\n",
      "320/507, train_loss: 0.3582\n",
      "321/507, train_loss: 0.5483\n",
      "322/507, train_loss: 0.2461\n",
      "323/507, train_loss: 0.1963\n",
      "324/507, train_loss: 0.2389\n",
      "325/507, train_loss: 0.2639\n",
      "326/507, train_loss: 0.6201\n",
      "327/507, train_loss: 1.1230\n",
      "328/507, train_loss: 0.1143\n",
      "329/507, train_loss: 0.3154\n",
      "330/507, train_loss: 0.3037\n",
      "331/507, train_loss: 0.9258\n",
      "332/507, train_loss: 0.3250\n",
      "333/507, train_loss: 0.2842\n",
      "334/507, train_loss: 0.4185\n",
      "335/507, train_loss: 0.4753\n",
      "336/507, train_loss: 0.3286\n",
      "337/507, train_loss: 0.1982\n",
      "338/507, train_loss: 0.4888\n",
      "339/507, train_loss: 0.3457\n",
      "340/507, train_loss: 0.2449\n",
      "341/507, train_loss: 0.4124\n",
      "342/507, train_loss: 0.2354\n",
      "343/507, train_loss: 0.2832\n",
      "344/507, train_loss: 0.3047\n",
      "345/507, train_loss: 0.3667\n",
      "346/507, train_loss: 0.7783\n",
      "347/507, train_loss: 0.1620\n",
      "348/507, train_loss: 0.5596\n",
      "349/507, train_loss: 0.4312\n",
      "350/507, train_loss: 0.6787\n",
      "351/507, train_loss: 0.3572\n",
      "352/507, train_loss: 0.0560\n",
      "353/507, train_loss: 0.8394\n",
      "354/507, train_loss: 0.1562\n",
      "355/507, train_loss: 0.2198\n",
      "356/507, train_loss: 0.8311\n",
      "357/507, train_loss: 0.1282\n",
      "358/507, train_loss: 0.3428\n",
      "359/507, train_loss: 0.0488\n",
      "360/507, train_loss: 0.3530\n",
      "361/507, train_loss: 0.4963\n",
      "362/507, train_loss: 0.4438\n",
      "363/507, train_loss: 0.3794\n",
      "364/507, train_loss: 0.4124\n",
      "365/507, train_loss: 0.5801\n",
      "366/507, train_loss: 0.8994\n",
      "367/507, train_loss: 0.5879\n",
      "368/507, train_loss: 0.6475\n",
      "369/507, train_loss: 0.3716\n",
      "370/507, train_loss: 0.2335\n",
      "371/507, train_loss: 0.3809\n",
      "372/507, train_loss: 0.2983\n",
      "373/507, train_loss: 0.2812\n",
      "374/507, train_loss: 0.1650\n",
      "375/507, train_loss: 0.3628\n",
      "376/507, train_loss: 0.1738\n",
      "377/507, train_loss: 0.4875\n",
      "378/507, train_loss: 0.2202\n",
      "379/507, train_loss: 0.3730\n",
      "380/507, train_loss: 0.2891\n",
      "381/507, train_loss: 0.2734\n",
      "382/507, train_loss: 0.2126\n",
      "383/507, train_loss: 0.0894\n",
      "384/507, train_loss: 0.1786\n",
      "385/507, train_loss: 0.6934\n",
      "386/507, train_loss: 0.1610\n",
      "387/507, train_loss: 0.2651\n",
      "388/507, train_loss: 0.3032\n",
      "389/507, train_loss: 0.2317\n",
      "390/507, train_loss: 1.4395\n",
      "391/507, train_loss: 0.2100\n",
      "392/507, train_loss: 0.1053\n",
      "393/507, train_loss: 0.1137\n",
      "394/507, train_loss: 0.6875\n",
      "395/507, train_loss: 0.4919\n",
      "396/507, train_loss: 0.1394\n",
      "397/507, train_loss: 0.5229\n",
      "398/507, train_loss: 0.2087\n",
      "399/507, train_loss: 0.4485\n",
      "400/507, train_loss: 0.2185\n",
      "401/507, train_loss: 0.3499\n",
      "402/507, train_loss: 0.0535\n",
      "403/507, train_loss: 0.0559\n",
      "404/507, train_loss: 0.7993\n",
      "405/507, train_loss: 0.3472\n",
      "406/507, train_loss: 0.5195\n",
      "407/507, train_loss: 1.3496\n",
      "408/507, train_loss: 1.0596\n",
      "409/507, train_loss: 0.2493\n",
      "410/507, train_loss: 0.6836\n",
      "411/507, train_loss: 0.2900\n",
      "412/507, train_loss: 0.1802\n",
      "413/507, train_loss: 0.8389\n",
      "414/507, train_loss: 0.2937\n",
      "415/507, train_loss: 0.1377\n",
      "416/507, train_loss: 0.2632\n",
      "417/507, train_loss: 0.1012\n",
      "418/507, train_loss: 0.5176\n",
      "419/507, train_loss: 0.3496\n",
      "420/507, train_loss: 0.3044\n",
      "421/507, train_loss: 0.6401\n",
      "422/507, train_loss: 0.3457\n",
      "423/507, train_loss: 0.0710\n",
      "424/507, train_loss: 0.1969\n",
      "425/507, train_loss: 0.4722\n",
      "426/507, train_loss: 0.2219\n",
      "427/507, train_loss: 0.2145\n",
      "428/507, train_loss: 0.2185\n",
      "429/507, train_loss: 0.3574\n",
      "430/507, train_loss: 1.0312\n",
      "431/507, train_loss: 0.6680\n",
      "432/507, train_loss: 0.2068\n",
      "433/507, train_loss: 0.9771\n",
      "434/507, train_loss: 0.5869\n",
      "435/507, train_loss: 0.7446\n",
      "436/507, train_loss: 0.1941\n",
      "437/507, train_loss: 0.7188\n",
      "438/507, train_loss: 0.5728\n",
      "439/507, train_loss: 0.2510\n",
      "440/507, train_loss: 0.1689\n",
      "441/507, train_loss: 0.3848\n",
      "442/507, train_loss: 0.5742\n",
      "443/507, train_loss: 0.3142\n",
      "444/507, train_loss: 0.2617\n",
      "445/507, train_loss: 0.2922\n",
      "446/507, train_loss: 0.1940\n",
      "447/507, train_loss: 0.3118\n",
      "448/507, train_loss: 0.1429\n",
      "449/507, train_loss: 0.6201\n",
      "450/507, train_loss: 0.5840\n",
      "451/507, train_loss: 0.8872\n",
      "452/507, train_loss: 0.1683\n",
      "453/507, train_loss: 0.3547\n",
      "454/507, train_loss: 0.0485\n",
      "455/507, train_loss: 0.1959\n",
      "456/507, train_loss: 0.6689\n",
      "457/507, train_loss: 0.4834\n",
      "458/507, train_loss: 0.0503\n",
      "459/507, train_loss: 0.0532\n",
      "460/507, train_loss: 0.5801\n",
      "461/507, train_loss: 0.4966\n",
      "462/507, train_loss: 0.0994\n",
      "463/507, train_loss: 0.2788\n",
      "464/507, train_loss: 0.5088\n",
      "465/507, train_loss: 0.2625\n",
      "466/507, train_loss: 0.6426\n",
      "467/507, train_loss: 0.4058\n",
      "468/507, train_loss: 0.3193\n",
      "469/507, train_loss: 0.7607\n",
      "470/507, train_loss: 0.3528\n",
      "471/507, train_loss: 0.9521\n",
      "472/507, train_loss: 0.2493\n",
      "473/507, train_loss: 0.4292\n",
      "474/507, train_loss: 0.8091\n",
      "475/507, train_loss: 0.2612\n",
      "476/507, train_loss: 0.3044\n",
      "477/507, train_loss: 0.3821\n",
      "478/507, train_loss: 0.2900\n",
      "479/507, train_loss: 0.2600\n",
      "480/507, train_loss: 0.0835\n",
      "481/507, train_loss: 0.3425\n",
      "482/507, train_loss: 0.2612\n",
      "483/507, train_loss: 0.8193\n",
      "484/507, train_loss: 0.2839\n",
      "485/507, train_loss: 0.1526\n",
      "486/507, train_loss: 0.3687\n",
      "487/507, train_loss: 0.1558\n",
      "488/507, train_loss: 0.4304\n",
      "489/507, train_loss: 0.2612\n",
      "490/507, train_loss: 0.1576\n",
      "491/507, train_loss: 0.6318\n",
      "492/507, train_loss: 0.1555\n",
      "493/507, train_loss: 0.4551\n",
      "494/507, train_loss: 0.0396\n",
      "495/507, train_loss: 0.4871\n",
      "496/507, train_loss: 0.2798\n",
      "497/507, train_loss: 0.2935\n",
      "498/507, train_loss: 0.5347\n",
      "499/507, train_loss: 0.2197\n",
      "500/507, train_loss: 0.7842\n",
      "501/507, train_loss: 0.1379\n",
      "502/507, train_loss: 0.3237\n",
      "503/507, train_loss: 0.3560\n",
      "504/507, train_loss: 0.2622\n",
      "505/507, train_loss: 0.4175\n",
      "506/507, train_loss: 0.2029\n",
      "507/507, train_loss: 0.5815\n",
      "epoch 3 average loss: 0.4310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 09:26:22 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 09:26:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 4/20\n",
      "1/507, train_loss: 0.1451\n",
      "2/507, train_loss: 0.5566\n",
      "3/507, train_loss: 0.7002\n",
      "4/507, train_loss: 0.6416\n",
      "5/507, train_loss: 0.0587\n",
      "6/507, train_loss: 0.3406\n",
      "7/507, train_loss: 0.2477\n",
      "8/507, train_loss: 0.4275\n",
      "9/507, train_loss: 0.1737\n",
      "10/507, train_loss: 0.6196\n",
      "11/507, train_loss: 0.5366\n",
      "12/507, train_loss: 0.9048\n",
      "13/507, train_loss: 0.2974\n",
      "14/507, train_loss: 0.1624\n",
      "15/507, train_loss: 0.4866\n",
      "16/507, train_loss: 1.1348\n",
      "17/507, train_loss: 0.7490\n",
      "18/507, train_loss: 0.3213\n",
      "19/507, train_loss: 0.3018\n",
      "20/507, train_loss: 0.3018\n",
      "21/507, train_loss: 0.3953\n",
      "22/507, train_loss: 0.3086\n",
      "23/507, train_loss: 0.4695\n",
      "24/507, train_loss: 0.9258\n",
      "25/507, train_loss: 0.5381\n",
      "26/507, train_loss: 0.5859\n",
      "27/507, train_loss: 0.6929\n",
      "28/507, train_loss: 0.5264\n",
      "29/507, train_loss: 0.3035\n",
      "30/507, train_loss: 0.3608\n",
      "31/507, train_loss: 0.3499\n",
      "32/507, train_loss: 0.0903\n",
      "33/507, train_loss: 0.8672\n",
      "34/507, train_loss: 0.2235\n",
      "35/507, train_loss: 0.4287\n",
      "36/507, train_loss: 0.0941\n",
      "37/507, train_loss: 0.2563\n",
      "38/507, train_loss: 0.2441\n",
      "39/507, train_loss: 0.1621\n",
      "40/507, train_loss: 0.4097\n",
      "41/507, train_loss: 0.3645\n",
      "42/507, train_loss: 0.1301\n",
      "43/507, train_loss: 0.3428\n",
      "44/507, train_loss: 0.2329\n",
      "45/507, train_loss: 0.5400\n",
      "46/507, train_loss: 0.5234\n",
      "47/507, train_loss: 0.5190\n",
      "48/507, train_loss: 0.1576\n",
      "49/507, train_loss: 0.2773\n",
      "50/507, train_loss: 0.3486\n",
      "51/507, train_loss: 0.2585\n",
      "52/507, train_loss: 0.6382\n",
      "53/507, train_loss: 0.5762\n",
      "54/507, train_loss: 0.2345\n",
      "55/507, train_loss: 0.1973\n",
      "56/507, train_loss: 0.5728\n",
      "57/507, train_loss: 0.3618\n",
      "58/507, train_loss: 0.2600\n",
      "59/507, train_loss: 0.4067\n",
      "60/507, train_loss: 0.8672\n",
      "61/507, train_loss: 0.1395\n",
      "62/507, train_loss: 0.2661\n",
      "63/507, train_loss: 0.5366\n",
      "64/507, train_loss: 0.3145\n",
      "65/507, train_loss: 0.1962\n",
      "66/507, train_loss: 0.0532\n",
      "67/507, train_loss: 0.2686\n",
      "68/507, train_loss: 1.0869\n",
      "69/507, train_loss: 0.2092\n",
      "70/507, train_loss: 0.3506\n",
      "71/507, train_loss: 0.9170\n",
      "72/507, train_loss: 0.6719\n",
      "73/507, train_loss: 0.2844\n",
      "74/507, train_loss: 0.5522\n",
      "75/507, train_loss: 0.3716\n",
      "76/507, train_loss: 0.5244\n",
      "77/507, train_loss: 0.1749\n",
      "78/507, train_loss: 0.1625\n",
      "79/507, train_loss: 0.1570\n",
      "80/507, train_loss: 0.2441\n",
      "81/507, train_loss: 0.0562\n",
      "82/507, train_loss: 0.2263\n",
      "83/507, train_loss: 0.3621\n",
      "84/507, train_loss: 0.6475\n",
      "85/507, train_loss: 0.3049\n",
      "86/507, train_loss: 0.6270\n",
      "87/507, train_loss: 1.0664\n",
      "88/507, train_loss: 0.4565\n",
      "89/507, train_loss: 0.2030\n",
      "90/507, train_loss: 0.4971\n",
      "91/507, train_loss: 0.1711\n",
      "92/507, train_loss: 0.4333\n",
      "93/507, train_loss: 0.3367\n",
      "94/507, train_loss: 0.3071\n",
      "95/507, train_loss: 0.5591\n",
      "96/507, train_loss: 0.3032\n",
      "97/507, train_loss: 0.3364\n",
      "98/507, train_loss: 0.1543\n",
      "99/507, train_loss: 0.2949\n",
      "100/507, train_loss: 0.1670\n",
      "101/507, train_loss: 0.4788\n",
      "102/507, train_loss: 0.0746\n",
      "103/507, train_loss: 0.2393\n",
      "104/507, train_loss: 0.5508\n",
      "105/507, train_loss: 0.3196\n",
      "106/507, train_loss: 0.3154\n",
      "107/507, train_loss: 0.4646\n",
      "108/507, train_loss: 0.1411\n",
      "109/507, train_loss: 0.9878\n",
      "110/507, train_loss: 0.7051\n",
      "111/507, train_loss: 0.3760\n",
      "112/507, train_loss: 0.4548\n",
      "113/507, train_loss: 0.3477\n",
      "114/507, train_loss: 0.5630\n",
      "115/507, train_loss: 1.0498\n",
      "116/507, train_loss: 0.5522\n",
      "117/507, train_loss: 0.5488\n",
      "118/507, train_loss: 0.5322\n",
      "119/507, train_loss: 0.4211\n",
      "120/507, train_loss: 0.3308\n",
      "121/507, train_loss: 0.6943\n",
      "122/507, train_loss: 0.9336\n",
      "123/507, train_loss: 0.5190\n",
      "124/507, train_loss: 0.5005\n",
      "125/507, train_loss: 0.4087\n",
      "126/507, train_loss: 0.3630\n",
      "127/507, train_loss: 0.3552\n",
      "128/507, train_loss: 0.3530\n",
      "129/507, train_loss: 0.3159\n",
      "130/507, train_loss: 0.2769\n",
      "131/507, train_loss: 0.5859\n",
      "132/507, train_loss: 0.1637\n",
      "133/507, train_loss: 0.3472\n",
      "134/507, train_loss: 0.4185\n",
      "135/507, train_loss: 0.4180\n",
      "136/507, train_loss: 0.2974\n",
      "137/507, train_loss: 1.0107\n",
      "138/507, train_loss: 0.1699\n",
      "139/507, train_loss: 0.3433\n",
      "140/507, train_loss: 0.3645\n",
      "141/507, train_loss: 0.1399\n",
      "142/507, train_loss: 0.5825\n",
      "143/507, train_loss: 0.1949\n",
      "144/507, train_loss: 0.3257\n",
      "145/507, train_loss: 0.6055\n",
      "146/507, train_loss: 0.3301\n",
      "147/507, train_loss: 0.6152\n",
      "148/507, train_loss: 1.0781\n",
      "149/507, train_loss: 0.6797\n",
      "150/507, train_loss: 1.2500\n",
      "151/507, train_loss: 0.2355\n",
      "152/507, train_loss: 0.2546\n",
      "153/507, train_loss: 0.3496\n",
      "154/507, train_loss: 0.2715\n",
      "155/507, train_loss: 0.5884\n",
      "156/507, train_loss: 0.3184\n",
      "157/507, train_loss: 0.6494\n",
      "158/507, train_loss: 0.2058\n",
      "159/507, train_loss: 0.0651\n",
      "160/507, train_loss: 1.0850\n",
      "161/507, train_loss: 0.5142\n",
      "162/507, train_loss: 0.3452\n",
      "163/507, train_loss: 0.5635\n",
      "164/507, train_loss: 0.4265\n",
      "165/507, train_loss: 0.3865\n",
      "166/507, train_loss: 0.1094\n",
      "167/507, train_loss: 0.4348\n",
      "168/507, train_loss: 0.3774\n",
      "169/507, train_loss: 0.6211\n",
      "170/507, train_loss: 0.5093\n",
      "171/507, train_loss: 0.0667\n",
      "172/507, train_loss: 0.1990\n",
      "173/507, train_loss: 0.0611\n",
      "174/507, train_loss: 0.5146\n",
      "175/507, train_loss: 0.4478\n",
      "176/507, train_loss: 0.4670\n",
      "177/507, train_loss: 0.5029\n",
      "178/507, train_loss: 0.2869\n",
      "179/507, train_loss: 0.2612\n",
      "180/507, train_loss: 0.5801\n",
      "181/507, train_loss: 0.2466\n",
      "182/507, train_loss: 0.1823\n",
      "183/507, train_loss: 0.3198\n",
      "184/507, train_loss: 0.3201\n",
      "185/507, train_loss: 0.7275\n",
      "186/507, train_loss: 0.3438\n",
      "187/507, train_loss: 0.3428\n",
      "188/507, train_loss: 0.3870\n",
      "189/507, train_loss: 0.2961\n",
      "190/507, train_loss: 0.3022\n",
      "191/507, train_loss: 0.5757\n",
      "192/507, train_loss: 0.1242\n",
      "193/507, train_loss: 0.2554\n",
      "194/507, train_loss: 0.3540\n",
      "195/507, train_loss: 0.2249\n",
      "196/507, train_loss: 0.2242\n",
      "197/507, train_loss: 0.5513\n",
      "198/507, train_loss: 0.1956\n",
      "199/507, train_loss: 0.4849\n",
      "200/507, train_loss: 0.1365\n",
      "201/507, train_loss: 0.2261\n",
      "202/507, train_loss: 0.1539\n",
      "203/507, train_loss: 1.0996\n",
      "204/507, train_loss: 0.0537\n",
      "205/507, train_loss: 0.5425\n",
      "206/507, train_loss: 0.0419\n",
      "207/507, train_loss: 0.0401\n",
      "208/507, train_loss: 0.1896\n",
      "209/507, train_loss: 0.1880\n",
      "210/507, train_loss: 0.9004\n",
      "211/507, train_loss: 0.1488\n",
      "212/507, train_loss: 0.7993\n",
      "213/507, train_loss: 0.2476\n",
      "214/507, train_loss: 0.0411\n",
      "215/507, train_loss: 0.0989\n",
      "216/507, train_loss: 0.3115\n",
      "217/507, train_loss: 0.5928\n",
      "218/507, train_loss: 0.2231\n",
      "219/507, train_loss: 0.6440\n",
      "220/507, train_loss: 0.4004\n",
      "221/507, train_loss: 0.2896\n",
      "222/507, train_loss: 0.2334\n",
      "223/507, train_loss: 0.3032\n",
      "224/507, train_loss: 0.1327\n",
      "225/507, train_loss: 0.2375\n",
      "226/507, train_loss: 0.7197\n",
      "227/507, train_loss: 0.6567\n",
      "228/507, train_loss: 0.2385\n",
      "229/507, train_loss: 0.3713\n",
      "230/507, train_loss: 0.1685\n",
      "231/507, train_loss: 0.1846\n",
      "232/507, train_loss: 0.3264\n",
      "233/507, train_loss: 0.9175\n",
      "234/507, train_loss: 0.3882\n",
      "235/507, train_loss: 0.2542\n",
      "236/507, train_loss: 0.3582\n",
      "237/507, train_loss: 0.1011\n",
      "238/507, train_loss: 0.0668\n",
      "239/507, train_loss: 0.9609\n",
      "240/507, train_loss: 0.7798\n",
      "241/507, train_loss: 0.2617\n",
      "242/507, train_loss: 0.3359\n",
      "243/507, train_loss: 0.2947\n",
      "244/507, train_loss: 0.4500\n",
      "245/507, train_loss: 0.2878\n",
      "246/507, train_loss: 0.1086\n",
      "247/507, train_loss: 0.3196\n",
      "248/507, train_loss: 0.0554\n",
      "249/507, train_loss: 0.2209\n",
      "250/507, train_loss: 0.5107\n",
      "251/507, train_loss: 0.7241\n",
      "252/507, train_loss: 0.8242\n",
      "253/507, train_loss: 0.2196\n",
      "254/507, train_loss: 0.4919\n",
      "255/507, train_loss: 0.1663\n",
      "256/507, train_loss: 0.4473\n",
      "257/507, train_loss: 0.2393\n",
      "258/507, train_loss: 0.2324\n",
      "259/507, train_loss: 1.2979\n",
      "260/507, train_loss: 0.7373\n",
      "261/507, train_loss: 0.5566\n",
      "262/507, train_loss: 0.4368\n",
      "263/507, train_loss: 0.5713\n",
      "264/507, train_loss: 0.6865\n",
      "265/507, train_loss: 0.2856\n",
      "266/507, train_loss: 0.4478\n",
      "267/507, train_loss: 0.5225\n",
      "268/507, train_loss: 0.3472\n",
      "269/507, train_loss: 0.2959\n",
      "270/507, train_loss: 0.1632\n",
      "271/507, train_loss: 0.2808\n",
      "272/507, train_loss: 0.2302\n",
      "273/507, train_loss: 0.8086\n",
      "274/507, train_loss: 0.5288\n",
      "275/507, train_loss: 0.1935\n",
      "276/507, train_loss: 0.7715\n",
      "277/507, train_loss: 0.4871\n",
      "278/507, train_loss: 0.4685\n",
      "279/507, train_loss: 0.1481\n",
      "280/507, train_loss: 0.4683\n",
      "281/507, train_loss: 0.1575\n",
      "282/507, train_loss: 0.1896\n",
      "283/507, train_loss: 0.4824\n",
      "284/507, train_loss: 0.5405\n",
      "285/507, train_loss: 0.3250\n",
      "286/507, train_loss: 0.0413\n",
      "287/507, train_loss: 0.2229\n",
      "288/507, train_loss: 0.4629\n",
      "289/507, train_loss: 0.4441\n",
      "290/507, train_loss: 0.1064\n",
      "291/507, train_loss: 0.2791\n",
      "292/507, train_loss: 0.4043\n",
      "293/507, train_loss: 0.1465\n",
      "294/507, train_loss: 0.1729\n",
      "295/507, train_loss: 0.4878\n",
      "296/507, train_loss: 0.1313\n",
      "297/507, train_loss: 0.1077\n",
      "298/507, train_loss: 0.3789\n",
      "299/507, train_loss: 0.6719\n",
      "300/507, train_loss: 0.1392\n",
      "301/507, train_loss: 0.2729\n",
      "302/507, train_loss: 0.2191\n",
      "303/507, train_loss: 0.3008\n",
      "304/507, train_loss: 0.1316\n",
      "305/507, train_loss: 0.3345\n",
      "306/507, train_loss: 0.0569\n",
      "307/507, train_loss: 0.2815\n",
      "308/507, train_loss: 0.4082\n",
      "309/507, train_loss: 0.4082\n",
      "310/507, train_loss: 0.2661\n",
      "311/507, train_loss: 0.2563\n",
      "312/507, train_loss: 0.3281\n",
      "313/507, train_loss: 0.5708\n",
      "314/507, train_loss: 0.4080\n",
      "315/507, train_loss: 0.2588\n",
      "316/507, train_loss: 0.2351\n",
      "317/507, train_loss: 0.6719\n",
      "318/507, train_loss: 0.9458\n",
      "319/507, train_loss: 0.6616\n",
      "320/507, train_loss: 0.7041\n",
      "321/507, train_loss: 0.1617\n",
      "322/507, train_loss: 0.4414\n",
      "323/507, train_loss: 0.2031\n",
      "324/507, train_loss: 0.2196\n",
      "325/507, train_loss: 0.0452\n",
      "326/507, train_loss: 1.2852\n",
      "327/507, train_loss: 0.3115\n",
      "328/507, train_loss: 0.1411\n",
      "329/507, train_loss: 0.4595\n",
      "330/507, train_loss: 0.2708\n",
      "331/507, train_loss: 0.2463\n",
      "332/507, train_loss: 0.7939\n",
      "333/507, train_loss: 0.1970\n",
      "334/507, train_loss: 0.6782\n",
      "335/507, train_loss: 0.1343\n",
      "336/507, train_loss: 0.3652\n",
      "337/507, train_loss: 0.5820\n",
      "338/507, train_loss: 0.1847\n",
      "339/507, train_loss: 0.3213\n",
      "340/507, train_loss: 0.3755\n",
      "341/507, train_loss: 0.2120\n",
      "342/507, train_loss: 0.2493\n",
      "343/507, train_loss: 0.7256\n",
      "344/507, train_loss: 0.3076\n",
      "345/507, train_loss: 0.3098\n",
      "346/507, train_loss: 0.1101\n",
      "347/507, train_loss: 0.4565\n",
      "348/507, train_loss: 0.2603\n",
      "349/507, train_loss: 0.8271\n",
      "350/507, train_loss: 0.8857\n",
      "351/507, train_loss: 0.6504\n",
      "352/507, train_loss: 0.3247\n",
      "353/507, train_loss: 0.2949\n",
      "354/507, train_loss: 0.7500\n",
      "355/507, train_loss: 0.2207\n",
      "356/507, train_loss: 0.0776\n",
      "357/507, train_loss: 0.3337\n",
      "358/507, train_loss: 0.3716\n",
      "359/507, train_loss: 0.1318\n",
      "360/507, train_loss: 0.6992\n",
      "361/507, train_loss: 0.4219\n",
      "362/507, train_loss: 0.1454\n",
      "363/507, train_loss: 0.3623\n",
      "364/507, train_loss: 0.2969\n",
      "365/507, train_loss: 0.4219\n",
      "366/507, train_loss: 0.2450\n",
      "367/507, train_loss: 0.2866\n",
      "368/507, train_loss: 0.3477\n",
      "369/507, train_loss: 0.1925\n",
      "370/507, train_loss: 0.4023\n",
      "371/507, train_loss: 0.3330\n",
      "372/507, train_loss: 0.6982\n",
      "373/507, train_loss: 0.7690\n",
      "374/507, train_loss: 0.2488\n",
      "375/507, train_loss: 0.3535\n",
      "376/507, train_loss: 0.0954\n",
      "377/507, train_loss: 0.5024\n",
      "378/507, train_loss: 0.2166\n",
      "379/507, train_loss: 0.5771\n",
      "380/507, train_loss: 0.0781\n",
      "381/507, train_loss: 0.3704\n",
      "382/507, train_loss: 0.3760\n",
      "383/507, train_loss: 0.7471\n",
      "384/507, train_loss: 0.3589\n",
      "385/507, train_loss: 0.4736\n",
      "386/507, train_loss: 0.4070\n",
      "387/507, train_loss: 0.6655\n",
      "388/507, train_loss: 0.2507\n",
      "389/507, train_loss: 0.1157\n",
      "390/507, train_loss: 0.3477\n",
      "391/507, train_loss: 0.3723\n",
      "392/507, train_loss: 0.5557\n",
      "393/507, train_loss: 0.3159\n",
      "394/507, train_loss: 0.2883\n",
      "395/507, train_loss: 0.1317\n",
      "396/507, train_loss: 0.2661\n",
      "397/507, train_loss: 0.6548\n",
      "398/507, train_loss: 0.1981\n",
      "399/507, train_loss: 0.3906\n",
      "400/507, train_loss: 0.3208\n",
      "401/507, train_loss: 0.1655\n",
      "402/507, train_loss: 0.1742\n",
      "403/507, train_loss: 0.0343\n",
      "404/507, train_loss: 0.1708\n",
      "405/507, train_loss: 0.5986\n",
      "406/507, train_loss: 0.5537\n",
      "407/507, train_loss: 0.2576\n",
      "408/507, train_loss: 0.2269\n",
      "409/507, train_loss: 0.2676\n",
      "410/507, train_loss: 0.0290\n",
      "411/507, train_loss: 0.2053\n",
      "412/507, train_loss: 0.1346\n",
      "413/507, train_loss: 0.2798\n",
      "414/507, train_loss: 0.3652\n",
      "415/507, train_loss: 0.1714\n",
      "416/507, train_loss: 0.1497\n",
      "417/507, train_loss: 0.4390\n",
      "418/507, train_loss: 0.1899\n",
      "419/507, train_loss: 0.5210\n",
      "420/507, train_loss: 0.6104\n",
      "421/507, train_loss: 0.2227\n",
      "422/507, train_loss: 0.3562\n",
      "423/507, train_loss: 0.5098\n",
      "424/507, train_loss: 0.3787\n",
      "425/507, train_loss: 0.1750\n",
      "426/507, train_loss: 0.4551\n",
      "427/507, train_loss: 0.1295\n",
      "428/507, train_loss: 0.1490\n",
      "429/507, train_loss: 0.6201\n",
      "430/507, train_loss: 0.6572\n",
      "431/507, train_loss: 0.1050\n",
      "432/507, train_loss: 0.3633\n",
      "433/507, train_loss: 0.3284\n",
      "434/507, train_loss: 0.3735\n",
      "435/507, train_loss: 0.2803\n",
      "436/507, train_loss: 0.4614\n",
      "437/507, train_loss: 0.2030\n",
      "438/507, train_loss: 0.3770\n",
      "439/507, train_loss: 0.3386\n",
      "440/507, train_loss: 0.2642\n",
      "441/507, train_loss: 0.4673\n",
      "442/507, train_loss: 0.1550\n",
      "443/507, train_loss: 0.1429\n",
      "444/507, train_loss: 0.1755\n",
      "445/507, train_loss: 0.5132\n",
      "446/507, train_loss: 0.2222\n",
      "447/507, train_loss: 0.5332\n",
      "448/507, train_loss: 0.2073\n",
      "449/507, train_loss: 0.2520\n",
      "450/507, train_loss: 0.1973\n",
      "451/507, train_loss: 0.6953\n",
      "452/507, train_loss: 0.2644\n",
      "453/507, train_loss: 0.2915\n",
      "454/507, train_loss: 0.1689\n",
      "455/507, train_loss: 0.7466\n",
      "456/507, train_loss: 0.2107\n",
      "457/507, train_loss: 0.3513\n",
      "458/507, train_loss: 0.4365\n",
      "459/507, train_loss: 0.1960\n",
      "460/507, train_loss: 0.1311\n",
      "461/507, train_loss: 0.3501\n",
      "462/507, train_loss: 0.4297\n",
      "463/507, train_loss: 0.3433\n",
      "464/507, train_loss: 0.1443\n",
      "465/507, train_loss: 0.8242\n",
      "466/507, train_loss: 0.3345\n",
      "467/507, train_loss: 0.2749\n",
      "468/507, train_loss: 0.4097\n",
      "469/507, train_loss: 0.0865\n",
      "470/507, train_loss: 0.2688\n",
      "471/507, train_loss: 0.9082\n",
      "472/507, train_loss: 0.2578\n",
      "473/507, train_loss: 0.6299\n",
      "474/507, train_loss: 0.7471\n",
      "475/507, train_loss: 0.5308\n",
      "476/507, train_loss: 0.3877\n",
      "477/507, train_loss: 0.1925\n",
      "478/507, train_loss: 0.4985\n",
      "479/507, train_loss: 0.4250\n",
      "480/507, train_loss: 0.1075\n",
      "481/507, train_loss: 0.2673\n",
      "482/507, train_loss: 0.4641\n",
      "483/507, train_loss: 0.3284\n",
      "484/507, train_loss: 0.1697\n",
      "485/507, train_loss: 0.6104\n",
      "486/507, train_loss: 0.1758\n",
      "487/507, train_loss: 0.3452\n",
      "488/507, train_loss: 0.5010\n",
      "489/507, train_loss: 0.2996\n",
      "490/507, train_loss: 0.1134\n",
      "491/507, train_loss: 0.6958\n",
      "492/507, train_loss: 0.1127\n",
      "493/507, train_loss: 0.2379\n",
      "494/507, train_loss: 0.4502\n",
      "495/507, train_loss: 0.2271\n",
      "496/507, train_loss: 0.2002\n",
      "497/507, train_loss: 0.3118\n",
      "498/507, train_loss: 0.1738\n",
      "499/507, train_loss: 0.1986\n",
      "500/507, train_loss: 0.8623\n",
      "501/507, train_loss: 0.3315\n",
      "502/507, train_loss: 0.2749\n",
      "503/507, train_loss: 0.3560\n",
      "504/507, train_loss: 0.2329\n",
      "505/507, train_loss: 0.2205\n",
      "506/507, train_loss: 0.1873\n",
      "507/507, train_loss: 0.3765\n",
      "epoch 4 average loss: 0.3815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 11:28:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 11:28:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 5/20\n",
      "1/507, train_loss: 0.3181\n",
      "2/507, train_loss: 0.5352\n",
      "3/507, train_loss: 0.3672\n",
      "4/507, train_loss: 0.2090\n",
      "5/507, train_loss: 0.3328\n",
      "6/507, train_loss: 0.1938\n",
      "7/507, train_loss: 0.4438\n",
      "8/507, train_loss: 0.1399\n",
      "9/507, train_loss: 0.3718\n",
      "10/507, train_loss: 0.6318\n",
      "11/507, train_loss: 0.2402\n",
      "12/507, train_loss: 0.3250\n",
      "13/507, train_loss: 0.2910\n",
      "14/507, train_loss: 0.1628\n",
      "15/507, train_loss: 0.1222\n",
      "16/507, train_loss: 0.9346\n",
      "17/507, train_loss: 0.2695\n",
      "18/507, train_loss: 0.3015\n",
      "19/507, train_loss: 0.0441\n",
      "20/507, train_loss: 0.7207\n",
      "21/507, train_loss: 0.1754\n",
      "22/507, train_loss: 0.2373\n",
      "23/507, train_loss: 0.2432\n",
      "24/507, train_loss: 0.1501\n",
      "25/507, train_loss: 0.3381\n",
      "26/507, train_loss: 0.1321\n",
      "27/507, train_loss: 0.2256\n",
      "28/507, train_loss: 0.3848\n",
      "29/507, train_loss: 0.1749\n",
      "30/507, train_loss: 0.2249\n",
      "31/507, train_loss: 0.3098\n",
      "32/507, train_loss: 0.6870\n",
      "33/507, train_loss: 0.2671\n",
      "34/507, train_loss: 0.4897\n",
      "35/507, train_loss: 0.1495\n",
      "36/507, train_loss: 0.2417\n",
      "37/507, train_loss: 0.1692\n",
      "38/507, train_loss: 0.3037\n",
      "39/507, train_loss: 0.2212\n",
      "40/507, train_loss: 1.0039\n",
      "41/507, train_loss: 0.6094\n",
      "42/507, train_loss: 0.2520\n",
      "43/507, train_loss: 0.4910\n",
      "44/507, train_loss: 0.0769\n",
      "45/507, train_loss: 0.2366\n",
      "46/507, train_loss: 0.4617\n",
      "47/507, train_loss: 0.3845\n",
      "48/507, train_loss: 0.2527\n",
      "49/507, train_loss: 0.6245\n",
      "50/507, train_loss: 0.0374\n",
      "51/507, train_loss: 0.3452\n",
      "52/507, train_loss: 0.1592\n",
      "53/507, train_loss: 0.4861\n",
      "54/507, train_loss: 0.2354\n",
      "55/507, train_loss: 0.2881\n",
      "56/507, train_loss: 0.1377\n",
      "57/507, train_loss: 0.3184\n",
      "58/507, train_loss: 0.8452\n",
      "59/507, train_loss: 0.3525\n",
      "60/507, train_loss: 0.2272\n",
      "61/507, train_loss: 1.0713\n",
      "62/507, train_loss: 0.1873\n",
      "63/507, train_loss: 0.2281\n",
      "64/507, train_loss: 0.2395\n",
      "65/507, train_loss: 0.3999\n",
      "66/507, train_loss: 0.3171\n",
      "67/507, train_loss: 0.2322\n",
      "68/507, train_loss: 0.1039\n",
      "69/507, train_loss: 0.5312\n",
      "70/507, train_loss: 0.5977\n",
      "71/507, train_loss: 0.1655\n",
      "72/507, train_loss: 0.0503\n",
      "73/507, train_loss: 0.7988\n",
      "74/507, train_loss: 0.2141\n",
      "75/507, train_loss: 0.5107\n",
      "76/507, train_loss: 0.1692\n",
      "77/507, train_loss: 0.7705\n",
      "78/507, train_loss: 0.3379\n",
      "79/507, train_loss: 0.2922\n",
      "80/507, train_loss: 0.4971\n",
      "81/507, train_loss: 0.2402\n",
      "82/507, train_loss: 0.0591\n",
      "83/507, train_loss: 0.4023\n",
      "84/507, train_loss: 0.1367\n",
      "85/507, train_loss: 0.0468\n",
      "86/507, train_loss: 0.1738\n",
      "87/507, train_loss: 0.2681\n",
      "88/507, train_loss: 0.1346\n",
      "89/507, train_loss: 0.4385\n",
      "90/507, train_loss: 0.3899\n",
      "91/507, train_loss: 0.5244\n",
      "92/507, train_loss: 0.5576\n",
      "93/507, train_loss: 0.4937\n",
      "94/507, train_loss: 0.6123\n",
      "95/507, train_loss: 0.5034\n",
      "96/507, train_loss: 0.3982\n",
      "97/507, train_loss: 0.5049\n",
      "98/507, train_loss: 0.2306\n",
      "99/507, train_loss: 0.3682\n",
      "100/507, train_loss: 0.0721\n",
      "101/507, train_loss: 0.4509\n",
      "102/507, train_loss: 0.1754\n",
      "103/507, train_loss: 0.5127\n",
      "104/507, train_loss: 0.4568\n",
      "105/507, train_loss: 0.2095\n",
      "106/507, train_loss: 0.5620\n",
      "107/507, train_loss: 0.2898\n",
      "108/507, train_loss: 0.3340\n",
      "109/507, train_loss: 0.3916\n",
      "110/507, train_loss: 0.5410\n",
      "111/507, train_loss: 0.2590\n",
      "112/507, train_loss: 0.2109\n",
      "113/507, train_loss: 0.7354\n",
      "114/507, train_loss: 0.6113\n",
      "115/507, train_loss: 0.5078\n",
      "116/507, train_loss: 0.5542\n",
      "117/507, train_loss: 0.2291\n",
      "118/507, train_loss: 0.2795\n",
      "119/507, train_loss: 0.2490\n",
      "120/507, train_loss: 0.5029\n",
      "121/507, train_loss: 0.1757\n",
      "122/507, train_loss: 0.2383\n",
      "123/507, train_loss: 0.2356\n",
      "124/507, train_loss: 0.2048\n",
      "125/507, train_loss: 0.1895\n",
      "126/507, train_loss: 0.2517\n",
      "127/507, train_loss: 0.2681\n",
      "128/507, train_loss: 0.3809\n",
      "129/507, train_loss: 0.6323\n",
      "130/507, train_loss: 0.5322\n",
      "131/507, train_loss: 0.9814\n",
      "132/507, train_loss: 0.6787\n",
      "133/507, train_loss: 0.3467\n",
      "134/507, train_loss: 0.6006\n",
      "135/507, train_loss: 0.1890\n",
      "136/507, train_loss: 0.2075\n",
      "137/507, train_loss: 0.1868\n",
      "138/507, train_loss: 0.6055\n",
      "139/507, train_loss: 0.4614\n",
      "140/507, train_loss: 0.3101\n",
      "141/507, train_loss: 0.3567\n",
      "142/507, train_loss: 0.2600\n",
      "143/507, train_loss: 0.2449\n",
      "144/507, train_loss: 1.0703\n",
      "145/507, train_loss: 0.1858\n",
      "146/507, train_loss: 0.2793\n",
      "147/507, train_loss: 0.5640\n",
      "148/507, train_loss: 0.4011\n",
      "149/507, train_loss: 0.6982\n",
      "150/507, train_loss: 0.3562\n",
      "151/507, train_loss: 0.1661\n",
      "152/507, train_loss: 0.0587\n",
      "153/507, train_loss: 0.1904\n",
      "154/507, train_loss: 0.5864\n",
      "155/507, train_loss: 0.0915\n",
      "156/507, train_loss: 0.3442\n",
      "157/507, train_loss: 0.1848\n",
      "158/507, train_loss: 0.2661\n",
      "159/507, train_loss: 0.8496\n",
      "160/507, train_loss: 0.2925\n",
      "161/507, train_loss: 0.1548\n",
      "162/507, train_loss: 0.5029\n",
      "163/507, train_loss: 0.4612\n",
      "164/507, train_loss: 0.6372\n",
      "165/507, train_loss: 0.0302\n",
      "166/507, train_loss: 0.3135\n",
      "167/507, train_loss: 0.1316\n",
      "168/507, train_loss: 0.1732\n",
      "169/507, train_loss: 0.1299\n",
      "170/507, train_loss: 0.1257\n",
      "171/507, train_loss: 0.8774\n",
      "172/507, train_loss: 0.1477\n",
      "173/507, train_loss: 0.2969\n",
      "174/507, train_loss: 0.1780\n",
      "175/507, train_loss: 0.1321\n",
      "176/507, train_loss: 0.4028\n",
      "177/507, train_loss: 0.3533\n",
      "178/507, train_loss: 0.0435\n",
      "179/507, train_loss: 0.9229\n",
      "180/507, train_loss: 0.1827\n",
      "181/507, train_loss: 0.3596\n",
      "182/507, train_loss: 0.5830\n",
      "183/507, train_loss: 0.4958\n",
      "184/507, train_loss: 0.1868\n",
      "185/507, train_loss: 0.6138\n",
      "186/507, train_loss: 0.0541\n",
      "187/507, train_loss: 0.1526\n",
      "188/507, train_loss: 0.3477\n",
      "189/507, train_loss: 0.5220\n",
      "190/507, train_loss: 0.8848\n",
      "191/507, train_loss: 0.1956\n",
      "192/507, train_loss: 0.2786\n",
      "193/507, train_loss: 0.6792\n",
      "194/507, train_loss: 0.2434\n",
      "195/507, train_loss: 0.9536\n",
      "196/507, train_loss: 0.0521\n",
      "197/507, train_loss: 0.2683\n",
      "198/507, train_loss: 0.2200\n",
      "199/507, train_loss: 0.6489\n",
      "200/507, train_loss: 0.1393\n",
      "201/507, train_loss: 0.1010\n",
      "202/507, train_loss: 0.2469\n",
      "203/507, train_loss: 0.0748\n",
      "204/507, train_loss: 0.2080\n",
      "205/507, train_loss: 0.1709\n",
      "206/507, train_loss: 0.2427\n",
      "207/507, train_loss: 0.7754\n",
      "208/507, train_loss: 0.5005\n",
      "209/507, train_loss: 0.1982\n",
      "210/507, train_loss: 0.1113\n",
      "211/507, train_loss: 0.2339\n",
      "212/507, train_loss: 0.5317\n",
      "213/507, train_loss: 0.4280\n",
      "214/507, train_loss: 0.4275\n",
      "215/507, train_loss: 0.1550\n",
      "216/507, train_loss: 0.1431\n",
      "217/507, train_loss: 0.3289\n",
      "218/507, train_loss: 0.3550\n",
      "219/507, train_loss: 0.0280\n",
      "220/507, train_loss: 0.4907\n",
      "221/507, train_loss: 0.0359\n",
      "222/507, train_loss: 0.2683\n",
      "223/507, train_loss: 0.5649\n",
      "224/507, train_loss: 0.3545\n",
      "225/507, train_loss: 0.2505\n",
      "226/507, train_loss: 0.6343\n",
      "227/507, train_loss: 0.2869\n",
      "228/507, train_loss: 0.2893\n",
      "229/507, train_loss: 0.0730\n",
      "230/507, train_loss: 0.1519\n",
      "231/507, train_loss: 0.0286\n",
      "232/507, train_loss: 0.2522\n",
      "233/507, train_loss: 1.2959\n",
      "234/507, train_loss: 0.3311\n",
      "235/507, train_loss: 0.5474\n",
      "236/507, train_loss: 0.5020\n",
      "237/507, train_loss: 0.1807\n",
      "238/507, train_loss: 0.3501\n",
      "239/507, train_loss: 0.3286\n",
      "240/507, train_loss: 0.3091\n",
      "241/507, train_loss: 0.4429\n",
      "242/507, train_loss: 0.6733\n",
      "243/507, train_loss: 0.2827\n",
      "244/507, train_loss: 0.3894\n",
      "245/507, train_loss: 0.2808\n",
      "246/507, train_loss: 0.3523\n",
      "247/507, train_loss: 0.2085\n",
      "248/507, train_loss: 0.2314\n",
      "249/507, train_loss: 0.0523\n",
      "250/507, train_loss: 0.0339\n",
      "251/507, train_loss: 0.2058\n",
      "252/507, train_loss: 0.3474\n",
      "253/507, train_loss: 0.2485\n",
      "254/507, train_loss: 0.1459\n",
      "255/507, train_loss: 0.7520\n",
      "256/507, train_loss: 0.1135\n",
      "257/507, train_loss: 0.1285\n",
      "258/507, train_loss: 0.3284\n",
      "259/507, train_loss: 0.1948\n",
      "260/507, train_loss: 0.1588\n",
      "261/507, train_loss: 0.4036\n",
      "262/507, train_loss: 0.1026\n",
      "263/507, train_loss: 0.2415\n",
      "264/507, train_loss: 0.5425\n",
      "265/507, train_loss: 0.7373\n",
      "266/507, train_loss: 0.0397\n",
      "267/507, train_loss: 0.1711\n",
      "268/507, train_loss: 0.2391\n",
      "269/507, train_loss: 0.1207\n",
      "270/507, train_loss: 0.1477\n",
      "271/507, train_loss: 0.2144\n",
      "272/507, train_loss: 0.2778\n",
      "273/507, train_loss: 0.1560\n",
      "274/507, train_loss: 0.0180\n",
      "275/507, train_loss: 0.2786\n",
      "276/507, train_loss: 0.4463\n",
      "277/507, train_loss: 0.7241\n",
      "278/507, train_loss: 0.1934\n",
      "279/507, train_loss: 0.7534\n",
      "280/507, train_loss: 0.4624\n",
      "281/507, train_loss: 0.4204\n",
      "282/507, train_loss: 0.0318\n",
      "283/507, train_loss: 0.6738\n",
      "284/507, train_loss: 0.2590\n",
      "285/507, train_loss: 0.1896\n",
      "286/507, train_loss: 0.1992\n",
      "287/507, train_loss: 0.0339\n",
      "288/507, train_loss: 0.1763\n",
      "289/507, train_loss: 0.4390\n",
      "290/507, train_loss: 0.3862\n",
      "291/507, train_loss: 0.3379\n",
      "292/507, train_loss: 0.4050\n",
      "293/507, train_loss: 0.0714\n",
      "294/507, train_loss: 0.9341\n",
      "295/507, train_loss: 0.6743\n",
      "296/507, train_loss: 0.2620\n",
      "297/507, train_loss: 0.7788\n",
      "298/507, train_loss: 0.2180\n",
      "299/507, train_loss: 0.5078\n",
      "300/507, train_loss: 0.2725\n",
      "301/507, train_loss: 0.2070\n",
      "302/507, train_loss: 0.4302\n",
      "303/507, train_loss: 0.3093\n",
      "304/507, train_loss: 0.2490\n",
      "305/507, train_loss: 0.2078\n",
      "306/507, train_loss: 0.0734\n",
      "307/507, train_loss: 0.3171\n",
      "308/507, train_loss: 0.3918\n",
      "309/507, train_loss: 0.0816\n",
      "310/507, train_loss: 0.6250\n",
      "311/507, train_loss: 0.4507\n",
      "312/507, train_loss: 0.2729\n",
      "313/507, train_loss: 0.7080\n",
      "314/507, train_loss: 0.4653\n",
      "315/507, train_loss: 0.2659\n",
      "316/507, train_loss: 0.3054\n",
      "317/507, train_loss: 0.4473\n",
      "318/507, train_loss: 0.4619\n",
      "319/507, train_loss: 0.5254\n",
      "320/507, train_loss: 0.7256\n",
      "321/507, train_loss: 0.8564\n",
      "322/507, train_loss: 0.3521\n",
      "323/507, train_loss: 0.3030\n",
      "324/507, train_loss: 0.2351\n",
      "325/507, train_loss: 0.6641\n",
      "326/507, train_loss: 0.2886\n",
      "327/507, train_loss: 0.2920\n",
      "328/507, train_loss: 0.3340\n",
      "329/507, train_loss: 0.2656\n",
      "330/507, train_loss: 0.1321\n",
      "331/507, train_loss: 0.6479\n",
      "332/507, train_loss: 0.0529\n",
      "333/507, train_loss: 0.3164\n",
      "334/507, train_loss: 0.1803\n",
      "335/507, train_loss: 0.3127\n",
      "336/507, train_loss: 0.1705\n",
      "337/507, train_loss: 0.3054\n",
      "338/507, train_loss: 0.2078\n",
      "339/507, train_loss: 0.3879\n",
      "340/507, train_loss: 0.0909\n",
      "341/507, train_loss: 0.0471\n",
      "342/507, train_loss: 0.1719\n",
      "343/507, train_loss: 0.1770\n",
      "344/507, train_loss: 0.2683\n",
      "345/507, train_loss: 0.2404\n",
      "346/507, train_loss: 0.2651\n",
      "347/507, train_loss: 0.2810\n",
      "348/507, train_loss: 0.4578\n",
      "349/507, train_loss: 0.3508\n",
      "350/507, train_loss: 0.1874\n",
      "351/507, train_loss: 0.1948\n",
      "352/507, train_loss: 0.3074\n",
      "353/507, train_loss: 0.4329\n",
      "354/507, train_loss: 0.1221\n",
      "355/507, train_loss: 0.1119\n",
      "356/507, train_loss: 0.1790\n",
      "357/507, train_loss: 0.0844\n",
      "358/507, train_loss: 0.3003\n",
      "359/507, train_loss: 0.3547\n",
      "360/507, train_loss: 0.2213\n",
      "361/507, train_loss: 0.1494\n",
      "362/507, train_loss: 0.2651\n",
      "363/507, train_loss: 0.2061\n",
      "364/507, train_loss: 0.2930\n",
      "365/507, train_loss: 1.0244\n",
      "366/507, train_loss: 0.2695\n",
      "367/507, train_loss: 0.4292\n",
      "368/507, train_loss: 0.3350\n",
      "369/507, train_loss: 0.1970\n",
      "370/507, train_loss: 0.2793\n",
      "371/507, train_loss: 0.2900\n",
      "372/507, train_loss: 0.6646\n",
      "373/507, train_loss: 0.0797\n",
      "374/507, train_loss: 0.9023\n",
      "375/507, train_loss: 0.2500\n",
      "376/507, train_loss: 0.3418\n",
      "377/507, train_loss: 0.9033\n",
      "378/507, train_loss: 0.0360\n",
      "379/507, train_loss: 0.0799\n",
      "380/507, train_loss: 0.3887\n",
      "381/507, train_loss: 0.2832\n",
      "382/507, train_loss: 0.3953\n",
      "383/507, train_loss: 0.4072\n",
      "384/507, train_loss: 0.2295\n",
      "385/507, train_loss: 0.2654\n",
      "386/507, train_loss: 0.2324\n",
      "387/507, train_loss: 0.0708\n",
      "388/507, train_loss: 0.8765\n",
      "389/507, train_loss: 0.1096\n",
      "390/507, train_loss: 0.1703\n",
      "391/507, train_loss: 0.1763\n",
      "392/507, train_loss: 0.4106\n",
      "393/507, train_loss: 0.5171\n",
      "394/507, train_loss: 0.2620\n",
      "395/507, train_loss: 0.4451\n",
      "396/507, train_loss: 0.2173\n",
      "397/507, train_loss: 0.3945\n",
      "398/507, train_loss: 0.3325\n",
      "399/507, train_loss: 0.2639\n",
      "400/507, train_loss: 0.8545\n",
      "401/507, train_loss: 0.3516\n",
      "402/507, train_loss: 0.2678\n",
      "403/507, train_loss: 0.1389\n",
      "404/507, train_loss: 0.1796\n",
      "405/507, train_loss: 0.5884\n",
      "406/507, train_loss: 0.2152\n",
      "407/507, train_loss: 0.5186\n",
      "408/507, train_loss: 0.6172\n",
      "409/507, train_loss: 0.2217\n",
      "410/507, train_loss: 0.0680\n",
      "411/507, train_loss: 0.0400\n",
      "412/507, train_loss: 0.0583\n",
      "413/507, train_loss: 0.3533\n",
      "414/507, train_loss: 0.1646\n",
      "415/507, train_loss: 0.1838\n",
      "416/507, train_loss: 0.7324\n",
      "417/507, train_loss: 0.9199\n",
      "418/507, train_loss: 0.7783\n",
      "419/507, train_loss: 0.3091\n",
      "420/507, train_loss: 0.5342\n",
      "421/507, train_loss: 0.5508\n",
      "422/507, train_loss: 0.2429\n",
      "423/507, train_loss: 0.2004\n",
      "424/507, train_loss: 0.7578\n",
      "425/507, train_loss: 0.3074\n",
      "426/507, train_loss: 0.2092\n",
      "427/507, train_loss: 0.2234\n",
      "428/507, train_loss: 0.4834\n",
      "429/507, train_loss: 0.1785\n",
      "430/507, train_loss: 0.3123\n",
      "431/507, train_loss: 0.0535\n",
      "432/507, train_loss: 0.7280\n",
      "433/507, train_loss: 0.8022\n",
      "434/507, train_loss: 0.2222\n",
      "435/507, train_loss: 0.1747\n",
      "436/507, train_loss: 0.4028\n",
      "437/507, train_loss: 0.2827\n",
      "438/507, train_loss: 0.3479\n",
      "439/507, train_loss: 0.1580\n",
      "440/507, train_loss: 0.2668\n",
      "441/507, train_loss: 0.0410\n",
      "442/507, train_loss: 0.2747\n",
      "443/507, train_loss: 0.1672\n",
      "444/507, train_loss: 0.2061\n",
      "445/507, train_loss: 0.1954\n",
      "446/507, train_loss: 0.0360\n",
      "447/507, train_loss: 0.5723\n",
      "448/507, train_loss: 0.1886\n",
      "449/507, train_loss: 0.6719\n",
      "450/507, train_loss: 0.2449\n",
      "451/507, train_loss: 0.2002\n",
      "452/507, train_loss: 0.0753\n",
      "453/507, train_loss: 0.1127\n",
      "454/507, train_loss: 0.3730\n",
      "455/507, train_loss: 0.3945\n",
      "456/507, train_loss: 0.3140\n",
      "457/507, train_loss: 0.0890\n",
      "458/507, train_loss: 0.6631\n",
      "459/507, train_loss: 0.1124\n",
      "460/507, train_loss: 0.3225\n",
      "461/507, train_loss: 0.3472\n",
      "462/507, train_loss: 0.0282\n",
      "463/507, train_loss: 0.1594\n",
      "464/507, train_loss: 0.6479\n",
      "465/507, train_loss: 0.7612\n",
      "466/507, train_loss: 0.3564\n",
      "467/507, train_loss: 1.2109\n",
      "468/507, train_loss: 1.1426\n",
      "469/507, train_loss: 0.0761\n",
      "470/507, train_loss: 0.5996\n",
      "471/507, train_loss: 0.1996\n",
      "472/507, train_loss: 0.8667\n",
      "473/507, train_loss: 0.3682\n",
      "474/507, train_loss: 0.4795\n",
      "475/507, train_loss: 0.5015\n",
      "476/507, train_loss: 0.5371\n",
      "477/507, train_loss: 0.1853\n",
      "478/507, train_loss: 0.4294\n",
      "479/507, train_loss: 0.0856\n",
      "480/507, train_loss: 0.7505\n",
      "481/507, train_loss: 0.2690\n",
      "482/507, train_loss: 0.4102\n",
      "483/507, train_loss: 0.4424\n",
      "484/507, train_loss: 0.3413\n",
      "485/507, train_loss: 0.2754\n",
      "486/507, train_loss: 0.2751\n",
      "487/507, train_loss: 0.1063\n",
      "488/507, train_loss: 0.1962\n",
      "489/507, train_loss: 0.2795\n",
      "490/507, train_loss: 0.2556\n",
      "491/507, train_loss: 0.3086\n",
      "492/507, train_loss: 0.3442\n",
      "493/507, train_loss: 0.2749\n",
      "494/507, train_loss: 0.1931\n",
      "495/507, train_loss: 0.0301\n",
      "496/507, train_loss: 0.1519\n",
      "497/507, train_loss: 0.2233\n",
      "498/507, train_loss: 0.3481\n",
      "499/507, train_loss: 0.4375\n",
      "500/507, train_loss: 0.5088\n",
      "501/507, train_loss: 0.0470\n",
      "502/507, train_loss: 0.5503\n",
      "503/507, train_loss: 0.1893\n",
      "504/507, train_loss: 0.1229\n",
      "505/507, train_loss: 0.1971\n",
      "506/507, train_loss: 0.1375\n",
      "507/507, train_loss: 0.5503\n",
      "epoch 5 average loss: 0.3410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 13:30:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 13:30:07 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.46203613285962963, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.46203613285962963, 'AP_IoU_0.10_MaxDet_100': 0.48403505383446666, 'nodule_AP_IoU_0.10_MaxDet_100': 0.48403505383446666, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.7759562730789185, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.7759562730789185, 'AR_IoU_0.10_MaxDet_100': 0.8360655903816223, 'nodule_AR_IoU_0.10_MaxDet_100': 0.8360655903816223}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 13:43:24 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 13:43:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 5 current metric: 0.6395 best metric: 0.6395 at epoch 5\n",
      "----------\n",
      "epoch 6/20\n",
      "1/507, train_loss: 0.1169\n",
      "2/507, train_loss: 0.9238\n",
      "3/507, train_loss: 0.3972\n",
      "4/507, train_loss: 0.3262\n",
      "5/507, train_loss: 0.1974\n",
      "6/507, train_loss: 0.2793\n",
      "7/507, train_loss: 0.2170\n",
      "8/507, train_loss: 0.1741\n",
      "9/507, train_loss: 0.0904\n",
      "10/507, train_loss: 0.3584\n",
      "11/507, train_loss: 0.1562\n",
      "12/507, train_loss: 0.0336\n",
      "13/507, train_loss: 0.4026\n",
      "14/507, train_loss: 0.7485\n",
      "15/507, train_loss: 0.1851\n",
      "16/507, train_loss: 0.2117\n",
      "17/507, train_loss: 0.1525\n",
      "18/507, train_loss: 0.3979\n",
      "19/507, train_loss: 0.3943\n",
      "20/507, train_loss: 0.2277\n",
      "21/507, train_loss: 0.0432\n",
      "22/507, train_loss: 0.2520\n",
      "23/507, train_loss: 0.1038\n",
      "24/507, train_loss: 0.5752\n",
      "25/507, train_loss: 0.1553\n",
      "26/507, train_loss: 0.3943\n",
      "27/507, train_loss: 0.5088\n",
      "28/507, train_loss: 0.1501\n",
      "29/507, train_loss: 0.1693\n",
      "30/507, train_loss: 0.3499\n",
      "31/507, train_loss: 0.0337\n",
      "32/507, train_loss: 0.2681\n",
      "33/507, train_loss: 0.3765\n",
      "34/507, train_loss: 0.1853\n",
      "35/507, train_loss: 0.1792\n",
      "36/507, train_loss: 0.4631\n",
      "37/507, train_loss: 0.1102\n",
      "38/507, train_loss: 0.6587\n",
      "39/507, train_loss: 0.3711\n",
      "40/507, train_loss: 0.2744\n",
      "41/507, train_loss: 0.3145\n",
      "42/507, train_loss: 0.2043\n",
      "43/507, train_loss: 0.9878\n",
      "44/507, train_loss: 0.5947\n",
      "45/507, train_loss: 0.1615\n",
      "46/507, train_loss: 0.2805\n",
      "47/507, train_loss: 0.7695\n",
      "48/507, train_loss: 0.1763\n",
      "49/507, train_loss: 0.2241\n",
      "50/507, train_loss: 0.6016\n",
      "51/507, train_loss: 0.3696\n",
      "52/507, train_loss: 0.1711\n",
      "53/507, train_loss: 0.7275\n",
      "54/507, train_loss: 0.5566\n",
      "55/507, train_loss: 0.5400\n",
      "56/507, train_loss: 0.1998\n",
      "57/507, train_loss: 0.1479\n",
      "58/507, train_loss: 0.4912\n",
      "59/507, train_loss: 0.2361\n",
      "60/507, train_loss: 0.4917\n",
      "61/507, train_loss: 0.2196\n",
      "62/507, train_loss: 0.1801\n",
      "63/507, train_loss: 0.2148\n",
      "64/507, train_loss: 0.2473\n",
      "65/507, train_loss: 0.0741\n",
      "66/507, train_loss: 0.2764\n",
      "67/507, train_loss: 0.1742\n",
      "68/507, train_loss: 0.1917\n",
      "69/507, train_loss: 0.1366\n",
      "70/507, train_loss: 0.9189\n",
      "71/507, train_loss: 0.2603\n",
      "72/507, train_loss: 0.3262\n",
      "73/507, train_loss: 0.1552\n",
      "74/507, train_loss: 0.2905\n",
      "75/507, train_loss: 0.1211\n",
      "76/507, train_loss: 0.1276\n",
      "77/507, train_loss: 0.2242\n",
      "78/507, train_loss: 0.4214\n",
      "79/507, train_loss: 0.1873\n",
      "80/507, train_loss: 0.2214\n",
      "81/507, train_loss: 0.1117\n",
      "82/507, train_loss: 0.1641\n",
      "83/507, train_loss: 0.4016\n",
      "84/507, train_loss: 0.0535\n",
      "85/507, train_loss: 0.1373\n",
      "86/507, train_loss: 0.1348\n",
      "87/507, train_loss: 0.1626\n",
      "88/507, train_loss: 0.1628\n",
      "89/507, train_loss: 0.3994\n",
      "90/507, train_loss: 0.1536\n",
      "91/507, train_loss: 0.6089\n",
      "92/507, train_loss: 0.2729\n",
      "93/507, train_loss: 0.3022\n",
      "94/507, train_loss: 0.2471\n",
      "95/507, train_loss: 0.9526\n",
      "96/507, train_loss: 0.2290\n",
      "97/507, train_loss: 0.1351\n",
      "98/507, train_loss: 0.2024\n",
      "99/507, train_loss: 0.1794\n",
      "100/507, train_loss: 0.3804\n",
      "101/507, train_loss: 0.2180\n",
      "102/507, train_loss: 0.3970\n",
      "103/507, train_loss: 0.4189\n",
      "104/507, train_loss: 0.9766\n",
      "105/507, train_loss: 0.7188\n",
      "106/507, train_loss: 0.0394\n",
      "107/507, train_loss: 0.2115\n",
      "108/507, train_loss: 0.6172\n",
      "109/507, train_loss: 0.1477\n",
      "110/507, train_loss: 0.1954\n",
      "111/507, train_loss: 0.2954\n",
      "112/507, train_loss: 0.1512\n",
      "113/507, train_loss: 0.5127\n",
      "114/507, train_loss: 0.6060\n",
      "115/507, train_loss: 0.1049\n",
      "116/507, train_loss: 0.4902\n",
      "117/507, train_loss: 0.3389\n",
      "118/507, train_loss: 0.0847\n",
      "119/507, train_loss: 0.1455\n",
      "120/507, train_loss: 0.2103\n",
      "121/507, train_loss: 0.2935\n",
      "122/507, train_loss: 0.3760\n",
      "123/507, train_loss: 0.4365\n",
      "124/507, train_loss: 0.2744\n",
      "125/507, train_loss: 0.3225\n",
      "126/507, train_loss: 0.2910\n",
      "127/507, train_loss: 0.2800\n",
      "128/507, train_loss: 0.2666\n",
      "129/507, train_loss: 0.3149\n",
      "130/507, train_loss: 0.2129\n",
      "131/507, train_loss: 0.7705\n",
      "132/507, train_loss: 0.1294\n",
      "133/507, train_loss: 0.3672\n",
      "134/507, train_loss: 0.8237\n",
      "135/507, train_loss: 0.3533\n",
      "136/507, train_loss: 0.6602\n",
      "137/507, train_loss: 0.0493\n",
      "138/507, train_loss: 0.0510\n",
      "139/507, train_loss: 0.4382\n",
      "140/507, train_loss: 0.4648\n",
      "141/507, train_loss: 0.2233\n",
      "142/507, train_loss: 0.2230\n",
      "143/507, train_loss: 0.3655\n",
      "144/507, train_loss: 0.4866\n",
      "145/507, train_loss: 0.1675\n",
      "146/507, train_loss: 0.6201\n",
      "147/507, train_loss: 0.7466\n",
      "148/507, train_loss: 0.4851\n",
      "149/507, train_loss: 0.3057\n",
      "150/507, train_loss: 0.2651\n",
      "151/507, train_loss: 0.5327\n",
      "152/507, train_loss: 0.4797\n",
      "153/507, train_loss: 0.2070\n",
      "154/507, train_loss: 0.0569\n",
      "155/507, train_loss: 0.5908\n",
      "156/507, train_loss: 0.2295\n",
      "157/507, train_loss: 0.2407\n",
      "158/507, train_loss: 0.4373\n",
      "159/507, train_loss: 0.4395\n",
      "160/507, train_loss: 0.3347\n",
      "161/507, train_loss: 0.2161\n",
      "162/507, train_loss: 0.2935\n",
      "163/507, train_loss: 0.0599\n",
      "164/507, train_loss: 0.3452\n",
      "165/507, train_loss: 0.3650\n",
      "166/507, train_loss: 0.2196\n",
      "167/507, train_loss: 0.1985\n",
      "168/507, train_loss: 0.7285\n",
      "169/507, train_loss: 0.3618\n",
      "170/507, train_loss: 0.1562\n",
      "171/507, train_loss: 0.4429\n",
      "172/507, train_loss: 0.4907\n",
      "173/507, train_loss: 0.1565\n",
      "174/507, train_loss: 0.3662\n",
      "175/507, train_loss: 0.0335\n",
      "176/507, train_loss: 0.2798\n",
      "177/507, train_loss: 0.2544\n",
      "178/507, train_loss: 0.1997\n",
      "179/507, train_loss: 0.0833\n",
      "180/507, train_loss: 0.0275\n",
      "181/507, train_loss: 0.3667\n",
      "182/507, train_loss: 0.2278\n",
      "183/507, train_loss: 0.1332\n",
      "184/507, train_loss: 0.5615\n",
      "185/507, train_loss: 0.2708\n",
      "186/507, train_loss: 0.4668\n",
      "187/507, train_loss: 0.0353\n",
      "188/507, train_loss: 0.3169\n",
      "189/507, train_loss: 0.3865\n",
      "190/507, train_loss: 0.3030\n",
      "191/507, train_loss: 0.2341\n",
      "192/507, train_loss: 0.6416\n",
      "193/507, train_loss: 0.2258\n",
      "194/507, train_loss: 0.1635\n",
      "195/507, train_loss: 0.2085\n",
      "196/507, train_loss: 0.1582\n",
      "197/507, train_loss: 0.2710\n",
      "198/507, train_loss: 0.2678\n",
      "199/507, train_loss: 0.1428\n",
      "200/507, train_loss: 0.4426\n",
      "201/507, train_loss: 0.3843\n",
      "202/507, train_loss: 0.0423\n",
      "203/507, train_loss: 0.1722\n",
      "204/507, train_loss: 0.3105\n",
      "205/507, train_loss: 0.0249\n",
      "206/507, train_loss: 0.2539\n",
      "207/507, train_loss: 0.5186\n",
      "208/507, train_loss: 0.5146\n",
      "209/507, train_loss: 0.6782\n",
      "210/507, train_loss: 0.1978\n",
      "211/507, train_loss: 0.5488\n",
      "212/507, train_loss: 0.3516\n",
      "213/507, train_loss: 0.3301\n",
      "214/507, train_loss: 0.3555\n",
      "215/507, train_loss: 0.3535\n",
      "216/507, train_loss: 0.0486\n",
      "217/507, train_loss: 0.5073\n",
      "218/507, train_loss: 0.1599\n",
      "219/507, train_loss: 0.4138\n",
      "220/507, train_loss: 0.3818\n",
      "221/507, train_loss: 0.2327\n",
      "222/507, train_loss: 0.2468\n",
      "223/507, train_loss: 0.2498\n",
      "224/507, train_loss: 0.4873\n",
      "225/507, train_loss: 0.1140\n",
      "226/507, train_loss: 0.2603\n",
      "227/507, train_loss: 0.6323\n",
      "228/507, train_loss: 0.3806\n",
      "229/507, train_loss: 0.4368\n",
      "230/507, train_loss: 0.2163\n",
      "231/507, train_loss: 0.2842\n",
      "232/507, train_loss: 0.3718\n",
      "233/507, train_loss: 0.2130\n",
      "234/507, train_loss: 0.2747\n",
      "235/507, train_loss: 0.0303\n",
      "236/507, train_loss: 0.4502\n",
      "237/507, train_loss: 0.1193\n",
      "238/507, train_loss: 0.7319\n",
      "239/507, train_loss: 0.2520\n",
      "240/507, train_loss: 0.3660\n",
      "241/507, train_loss: 0.1367\n",
      "242/507, train_loss: 0.0637\n",
      "243/507, train_loss: 0.5586\n",
      "244/507, train_loss: 0.7349\n",
      "245/507, train_loss: 0.3213\n",
      "246/507, train_loss: 0.2003\n",
      "247/507, train_loss: 0.4272\n",
      "248/507, train_loss: 0.8574\n",
      "249/507, train_loss: 0.2947\n",
      "250/507, train_loss: 0.0579\n",
      "251/507, train_loss: 0.1731\n",
      "252/507, train_loss: 0.3022\n",
      "253/507, train_loss: 0.2250\n",
      "254/507, train_loss: 0.1313\n",
      "255/507, train_loss: 0.1600\n",
      "256/507, train_loss: 0.7314\n",
      "257/507, train_loss: 0.2371\n",
      "258/507, train_loss: 0.6123\n",
      "259/507, train_loss: 0.3198\n",
      "260/507, train_loss: 0.5273\n",
      "261/507, train_loss: 0.0873\n",
      "262/507, train_loss: 0.4102\n",
      "263/507, train_loss: 0.3967\n",
      "264/507, train_loss: 0.3467\n",
      "265/507, train_loss: 0.3022\n",
      "266/507, train_loss: 0.1353\n",
      "267/507, train_loss: 0.5645\n",
      "268/507, train_loss: 0.2002\n",
      "269/507, train_loss: 0.5498\n",
      "270/507, train_loss: 0.0937\n",
      "271/507, train_loss: 0.3184\n",
      "272/507, train_loss: 0.1843\n",
      "273/507, train_loss: 0.5288\n",
      "274/507, train_loss: 0.1831\n",
      "275/507, train_loss: 0.6904\n",
      "276/507, train_loss: 0.1862\n",
      "277/507, train_loss: 0.3877\n",
      "278/507, train_loss: 0.1625\n",
      "279/507, train_loss: 0.2253\n",
      "280/507, train_loss: 0.3887\n",
      "281/507, train_loss: 0.3997\n",
      "282/507, train_loss: 0.6787\n",
      "283/507, train_loss: 0.2510\n",
      "284/507, train_loss: 0.2515\n",
      "285/507, train_loss: 0.0459\n",
      "286/507, train_loss: 0.7554\n",
      "287/507, train_loss: 0.2415\n",
      "288/507, train_loss: 0.0412\n",
      "289/507, train_loss: 0.2349\n",
      "290/507, train_loss: 0.2198\n",
      "291/507, train_loss: 0.2498\n",
      "292/507, train_loss: 0.4651\n",
      "293/507, train_loss: 0.7075\n",
      "294/507, train_loss: 0.1880\n",
      "295/507, train_loss: 0.8994\n",
      "296/507, train_loss: 0.2297\n",
      "297/507, train_loss: 0.1698\n",
      "298/507, train_loss: 0.1382\n",
      "299/507, train_loss: 0.3728\n",
      "300/507, train_loss: 0.0813\n",
      "301/507, train_loss: 0.1569\n",
      "302/507, train_loss: 0.1449\n",
      "303/507, train_loss: 0.2808\n",
      "304/507, train_loss: 0.1616\n",
      "305/507, train_loss: 0.4097\n",
      "306/507, train_loss: 0.3091\n",
      "307/507, train_loss: 0.3079\n",
      "308/507, train_loss: 0.4219\n",
      "309/507, train_loss: 0.4504\n",
      "310/507, train_loss: 0.1085\n",
      "311/507, train_loss: 0.5601\n",
      "312/507, train_loss: 0.3906\n",
      "313/507, train_loss: 0.3662\n",
      "314/507, train_loss: 0.5698\n",
      "315/507, train_loss: 0.2197\n",
      "316/507, train_loss: 0.7598\n",
      "317/507, train_loss: 0.6948\n",
      "318/507, train_loss: 0.1616\n",
      "319/507, train_loss: 0.3464\n",
      "320/507, train_loss: 0.7637\n",
      "321/507, train_loss: 0.3022\n",
      "322/507, train_loss: 0.2015\n",
      "323/507, train_loss: 0.1970\n",
      "324/507, train_loss: 0.1296\n",
      "325/507, train_loss: 0.1578\n",
      "326/507, train_loss: 0.8931\n",
      "327/507, train_loss: 0.0652\n",
      "328/507, train_loss: 0.2520\n",
      "329/507, train_loss: 0.2330\n",
      "330/507, train_loss: 0.1931\n",
      "331/507, train_loss: 0.3823\n",
      "332/507, train_loss: 0.2104\n",
      "333/507, train_loss: 0.0511\n",
      "334/507, train_loss: 0.3311\n",
      "335/507, train_loss: 0.1975\n",
      "336/507, train_loss: 0.1981\n",
      "337/507, train_loss: 0.1589\n",
      "338/507, train_loss: 0.1841\n",
      "339/507, train_loss: 0.3325\n",
      "340/507, train_loss: 0.0728\n",
      "341/507, train_loss: 0.1821\n",
      "342/507, train_loss: 0.1101\n",
      "343/507, train_loss: 0.2400\n",
      "344/507, train_loss: 0.2742\n",
      "345/507, train_loss: 1.0059\n",
      "346/507, train_loss: 0.9473\n",
      "347/507, train_loss: 0.3870\n",
      "348/507, train_loss: 0.1995\n",
      "349/507, train_loss: 0.5010\n",
      "350/507, train_loss: 0.0843\n",
      "351/507, train_loss: 0.9741\n",
      "352/507, train_loss: 0.2209\n",
      "353/507, train_loss: 0.2183\n",
      "354/507, train_loss: 0.1070\n",
      "355/507, train_loss: 0.3325\n",
      "356/507, train_loss: 0.5527\n",
      "357/507, train_loss: 0.2155\n",
      "358/507, train_loss: 0.2891\n",
      "359/507, train_loss: 0.3594\n",
      "360/507, train_loss: 0.2473\n",
      "361/507, train_loss: 0.1943\n",
      "362/507, train_loss: 0.2896\n",
      "363/507, train_loss: 0.2302\n",
      "364/507, train_loss: 0.1055\n",
      "365/507, train_loss: 0.3284\n",
      "366/507, train_loss: 0.2075\n",
      "367/507, train_loss: 0.1191\n",
      "368/507, train_loss: 0.3931\n",
      "369/507, train_loss: 0.3936\n",
      "370/507, train_loss: 0.5381\n",
      "371/507, train_loss: 0.2651\n",
      "372/507, train_loss: 0.3848\n",
      "373/507, train_loss: 0.1708\n",
      "374/507, train_loss: 0.5522\n",
      "375/507, train_loss: 0.6406\n",
      "376/507, train_loss: 0.3860\n",
      "377/507, train_loss: 1.0078\n",
      "378/507, train_loss: 0.1226\n",
      "379/507, train_loss: 0.3289\n",
      "380/507, train_loss: 0.1685\n",
      "381/507, train_loss: 0.1680\n",
      "382/507, train_loss: 1.0518\n",
      "383/507, train_loss: 0.1284\n",
      "384/507, train_loss: 0.2803\n",
      "385/507, train_loss: 0.2025\n",
      "386/507, train_loss: 0.2646\n",
      "387/507, train_loss: 0.4746\n",
      "388/507, train_loss: 0.1884\n",
      "389/507, train_loss: 0.3030\n",
      "390/507, train_loss: 0.3042\n",
      "391/507, train_loss: 0.2830\n",
      "392/507, train_loss: 0.5122\n",
      "393/507, train_loss: 0.8438\n",
      "394/507, train_loss: 0.2705\n",
      "395/507, train_loss: 0.2380\n",
      "396/507, train_loss: 0.4287\n",
      "397/507, train_loss: 0.1830\n",
      "398/507, train_loss: 0.2205\n",
      "399/507, train_loss: 0.3918\n",
      "400/507, train_loss: 0.2068\n",
      "401/507, train_loss: 0.0438\n",
      "402/507, train_loss: 0.3574\n",
      "403/507, train_loss: 0.0402\n",
      "404/507, train_loss: 0.6904\n",
      "405/507, train_loss: 0.1709\n",
      "406/507, train_loss: 0.3828\n",
      "407/507, train_loss: 0.4148\n",
      "408/507, train_loss: 0.6387\n",
      "409/507, train_loss: 0.4072\n",
      "410/507, train_loss: 0.2739\n",
      "411/507, train_loss: 0.1078\n",
      "412/507, train_loss: 0.1958\n",
      "413/507, train_loss: 0.6084\n",
      "414/507, train_loss: 0.1702\n",
      "415/507, train_loss: 0.4624\n",
      "416/507, train_loss: 0.1610\n",
      "417/507, train_loss: 0.2888\n",
      "418/507, train_loss: 0.1892\n",
      "419/507, train_loss: 0.5732\n",
      "420/507, train_loss: 0.2288\n",
      "421/507, train_loss: 0.5088\n",
      "422/507, train_loss: 0.1748\n",
      "423/507, train_loss: 0.4365\n",
      "424/507, train_loss: 0.4116\n",
      "425/507, train_loss: 0.3818\n",
      "426/507, train_loss: 0.3276\n",
      "427/507, train_loss: 0.2920\n",
      "428/507, train_loss: 0.4900\n",
      "429/507, train_loss: 0.6997\n",
      "430/507, train_loss: 0.1531\n",
      "431/507, train_loss: 0.1483\n",
      "432/507, train_loss: 0.2015\n",
      "433/507, train_loss: 0.4087\n",
      "434/507, train_loss: 0.7754\n",
      "435/507, train_loss: 0.0279\n",
      "436/507, train_loss: 0.1847\n",
      "437/507, train_loss: 0.0966\n",
      "438/507, train_loss: 0.6157\n",
      "439/507, train_loss: 0.1650\n",
      "440/507, train_loss: 0.0362\n",
      "441/507, train_loss: 0.2278\n",
      "442/507, train_loss: 0.2683\n",
      "443/507, train_loss: 0.0240\n",
      "444/507, train_loss: 0.1600\n",
      "445/507, train_loss: 0.4766\n",
      "446/507, train_loss: 0.2678\n",
      "447/507, train_loss: 0.2258\n",
      "448/507, train_loss: 0.7207\n",
      "449/507, train_loss: 0.3618\n",
      "450/507, train_loss: 0.2260\n",
      "451/507, train_loss: 0.6777\n",
      "452/507, train_loss: 0.3462\n",
      "453/507, train_loss: 0.3643\n",
      "454/507, train_loss: 0.2915\n",
      "455/507, train_loss: 0.1453\n",
      "456/507, train_loss: 0.1667\n",
      "457/507, train_loss: 0.3115\n",
      "458/507, train_loss: 0.4636\n",
      "459/507, train_loss: 0.2759\n",
      "460/507, train_loss: 0.7407\n",
      "461/507, train_loss: 0.8228\n",
      "462/507, train_loss: 0.3188\n",
      "463/507, train_loss: 0.2079\n",
      "464/507, train_loss: 0.1580\n",
      "465/507, train_loss: 0.3765\n",
      "466/507, train_loss: 0.1765\n",
      "467/507, train_loss: 0.4946\n",
      "468/507, train_loss: 0.6157\n",
      "469/507, train_loss: 0.4229\n",
      "470/507, train_loss: 0.2014\n",
      "471/507, train_loss: 0.1244\n",
      "472/507, train_loss: 0.5903\n",
      "473/507, train_loss: 0.1823\n",
      "474/507, train_loss: 0.1816\n",
      "475/507, train_loss: 0.6245\n",
      "476/507, train_loss: 0.3091\n",
      "477/507, train_loss: 0.2756\n",
      "478/507, train_loss: 0.1461\n",
      "479/507, train_loss: 0.2764\n",
      "480/507, train_loss: 0.2390\n",
      "481/507, train_loss: 0.2632\n",
      "482/507, train_loss: 0.3279\n",
      "483/507, train_loss: 0.5898\n",
      "484/507, train_loss: 0.1895\n",
      "485/507, train_loss: 0.6709\n",
      "486/507, train_loss: 0.2854\n",
      "487/507, train_loss: 0.3340\n",
      "488/507, train_loss: 0.1057\n",
      "489/507, train_loss: 0.4353\n",
      "490/507, train_loss: 0.2175\n",
      "491/507, train_loss: 0.2598\n",
      "492/507, train_loss: 0.2246\n",
      "493/507, train_loss: 0.2007\n",
      "494/507, train_loss: 0.7358\n",
      "495/507, train_loss: 0.1777\n",
      "496/507, train_loss: 0.1970\n",
      "497/507, train_loss: 0.2825\n",
      "498/507, train_loss: 0.2861\n",
      "499/507, train_loss: 0.7173\n",
      "500/507, train_loss: 0.4590\n",
      "501/507, train_loss: 0.1332\n",
      "502/507, train_loss: 0.3374\n",
      "503/507, train_loss: 0.4329\n",
      "504/507, train_loss: 0.2272\n",
      "505/507, train_loss: 0.5146\n",
      "506/507, train_loss: 0.3594\n",
      "507/507, train_loss: 0.1583\n",
      "epoch 6 average loss: 0.3251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 15:45:13 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 15:45:16 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 7/20\n",
      "1/507, train_loss: 0.8271\n",
      "2/507, train_loss: 0.1741\n",
      "3/507, train_loss: 0.1993\n",
      "4/507, train_loss: 0.1818\n",
      "5/507, train_loss: 0.3108\n",
      "6/507, train_loss: 0.2668\n",
      "7/507, train_loss: 0.2830\n",
      "8/507, train_loss: 0.1771\n",
      "9/507, train_loss: 0.0503\n",
      "10/507, train_loss: 0.1031\n",
      "11/507, train_loss: 0.6187\n",
      "12/507, train_loss: 0.3037\n",
      "13/507, train_loss: 0.1899\n",
      "14/507, train_loss: 0.2783\n",
      "15/507, train_loss: 0.1575\n",
      "16/507, train_loss: 0.6816\n",
      "17/507, train_loss: 0.3923\n",
      "18/507, train_loss: 0.7310\n",
      "19/507, train_loss: 0.5068\n",
      "20/507, train_loss: 0.1670\n",
      "21/507, train_loss: 0.2284\n",
      "22/507, train_loss: 0.2537\n",
      "23/507, train_loss: 0.3008\n",
      "24/507, train_loss: 0.1418\n",
      "25/507, train_loss: 0.4229\n",
      "26/507, train_loss: 0.3560\n",
      "27/507, train_loss: 0.0405\n",
      "28/507, train_loss: 0.1812\n",
      "29/507, train_loss: 0.1660\n",
      "30/507, train_loss: 0.8037\n",
      "31/507, train_loss: 0.4229\n",
      "32/507, train_loss: 0.1289\n",
      "33/507, train_loss: 0.5176\n",
      "34/507, train_loss: 0.7329\n",
      "35/507, train_loss: 0.3833\n",
      "36/507, train_loss: 0.2007\n",
      "37/507, train_loss: 0.1075\n",
      "38/507, train_loss: 0.2313\n",
      "39/507, train_loss: 0.4380\n",
      "40/507, train_loss: 0.1924\n",
      "41/507, train_loss: 0.4321\n",
      "42/507, train_loss: 0.1780\n",
      "43/507, train_loss: 0.1418\n",
      "44/507, train_loss: 0.2788\n",
      "45/507, train_loss: 0.1604\n",
      "46/507, train_loss: 0.1230\n",
      "47/507, train_loss: 0.4250\n",
      "48/507, train_loss: 0.2007\n",
      "49/507, train_loss: 0.8877\n",
      "50/507, train_loss: 0.9414\n",
      "51/507, train_loss: 0.3938\n",
      "52/507, train_loss: 0.1366\n",
      "53/507, train_loss: 0.5439\n",
      "54/507, train_loss: 0.2952\n",
      "55/507, train_loss: 0.1921\n",
      "56/507, train_loss: 0.6519\n",
      "57/507, train_loss: 0.2156\n",
      "58/507, train_loss: 0.1056\n",
      "59/507, train_loss: 0.3645\n",
      "60/507, train_loss: 0.2362\n",
      "61/507, train_loss: 0.4421\n",
      "62/507, train_loss: 0.2739\n",
      "63/507, train_loss: 0.2183\n",
      "64/507, train_loss: 0.1257\n",
      "65/507, train_loss: 0.2505\n",
      "66/507, train_loss: 0.1270\n",
      "67/507, train_loss: 0.2075\n",
      "68/507, train_loss: 0.5508\n",
      "69/507, train_loss: 0.1348\n",
      "70/507, train_loss: 0.1470\n",
      "71/507, train_loss: 0.5898\n",
      "72/507, train_loss: 0.1752\n",
      "73/507, train_loss: 0.1829\n",
      "74/507, train_loss: 0.1931\n",
      "75/507, train_loss: 0.3145\n",
      "76/507, train_loss: 0.9272\n",
      "77/507, train_loss: 0.1458\n",
      "78/507, train_loss: 0.1769\n",
      "79/507, train_loss: 0.1328\n",
      "80/507, train_loss: 0.0399\n",
      "81/507, train_loss: 0.1855\n",
      "82/507, train_loss: 0.1433\n",
      "83/507, train_loss: 0.1304\n",
      "84/507, train_loss: 0.4126\n",
      "85/507, train_loss: 0.1843\n",
      "86/507, train_loss: 0.0350\n",
      "87/507, train_loss: 0.2676\n",
      "88/507, train_loss: 1.2617\n",
      "89/507, train_loss: 0.3000\n",
      "90/507, train_loss: 0.6572\n",
      "91/507, train_loss: 0.1765\n",
      "92/507, train_loss: 0.1018\n",
      "93/507, train_loss: 0.2141\n",
      "94/507, train_loss: 0.8486\n",
      "95/507, train_loss: 0.0969\n",
      "96/507, train_loss: 0.1639\n",
      "97/507, train_loss: 0.0306\n",
      "98/507, train_loss: 0.2159\n",
      "99/507, train_loss: 0.1465\n",
      "100/507, train_loss: 0.1345\n",
      "101/507, train_loss: 0.2041\n",
      "102/507, train_loss: 0.3027\n",
      "103/507, train_loss: 0.3618\n",
      "104/507, train_loss: 0.4614\n",
      "105/507, train_loss: 0.3284\n",
      "106/507, train_loss: 0.6187\n",
      "107/507, train_loss: 0.2349\n",
      "108/507, train_loss: 0.2637\n",
      "109/507, train_loss: 0.2212\n",
      "110/507, train_loss: 0.1207\n",
      "111/507, train_loss: 0.0401\n",
      "112/507, train_loss: 0.8203\n",
      "113/507, train_loss: 0.2786\n",
      "114/507, train_loss: 0.1139\n",
      "115/507, train_loss: 0.2168\n",
      "116/507, train_loss: 0.1847\n",
      "117/507, train_loss: 0.2617\n",
      "118/507, train_loss: 0.2418\n",
      "119/507, train_loss: 0.2423\n",
      "120/507, train_loss: 0.4570\n",
      "121/507, train_loss: 0.3877\n",
      "122/507, train_loss: 0.0815\n",
      "123/507, train_loss: 0.1171\n",
      "124/507, train_loss: 0.1904\n",
      "125/507, train_loss: 0.0946\n",
      "126/507, train_loss: 0.1172\n",
      "127/507, train_loss: 0.1335\n",
      "128/507, train_loss: 0.2927\n",
      "129/507, train_loss: 0.0350\n",
      "130/507, train_loss: 0.2134\n",
      "131/507, train_loss: 0.6123\n",
      "132/507, train_loss: 0.3484\n",
      "133/507, train_loss: 0.5283\n",
      "134/507, train_loss: 0.2102\n",
      "135/507, train_loss: 0.4353\n",
      "136/507, train_loss: 0.2050\n",
      "137/507, train_loss: 0.9287\n",
      "138/507, train_loss: 0.2803\n",
      "139/507, train_loss: 0.1879\n",
      "140/507, train_loss: 0.1536\n",
      "141/507, train_loss: 0.3308\n",
      "142/507, train_loss: 0.1273\n",
      "143/507, train_loss: 0.2169\n",
      "144/507, train_loss: 0.2452\n",
      "145/507, train_loss: 0.1853\n",
      "146/507, train_loss: 0.4995\n",
      "147/507, train_loss: 0.1207\n",
      "148/507, train_loss: 0.2773\n",
      "149/507, train_loss: 0.1606\n",
      "150/507, train_loss: 0.1007\n",
      "151/507, train_loss: 0.5620\n",
      "152/507, train_loss: 0.7773\n",
      "153/507, train_loss: 0.4568\n",
      "154/507, train_loss: 0.0912\n",
      "155/507, train_loss: 0.1350\n",
      "156/507, train_loss: 0.2281\n",
      "157/507, train_loss: 0.2886\n",
      "158/507, train_loss: 0.3303\n",
      "159/507, train_loss: 0.6104\n",
      "160/507, train_loss: 0.1437\n",
      "161/507, train_loss: 0.1755\n",
      "162/507, train_loss: 0.1613\n",
      "163/507, train_loss: 0.2515\n",
      "164/507, train_loss: 0.3611\n",
      "165/507, train_loss: 0.3528\n",
      "166/507, train_loss: 0.5576\n",
      "167/507, train_loss: 0.6470\n",
      "168/507, train_loss: 0.1793\n",
      "169/507, train_loss: 0.1748\n",
      "170/507, train_loss: 0.1572\n",
      "171/507, train_loss: 0.1821\n",
      "172/507, train_loss: 0.0257\n",
      "173/507, train_loss: 0.6211\n",
      "174/507, train_loss: 0.3809\n",
      "175/507, train_loss: 0.7734\n",
      "176/507, train_loss: 0.1416\n",
      "177/507, train_loss: 0.2812\n",
      "178/507, train_loss: 0.1895\n",
      "179/507, train_loss: 0.2527\n",
      "180/507, train_loss: 0.6162\n",
      "181/507, train_loss: 0.3499\n",
      "182/507, train_loss: 0.2446\n",
      "183/507, train_loss: 0.2000\n",
      "184/507, train_loss: 0.3027\n",
      "185/507, train_loss: 0.4849\n",
      "186/507, train_loss: 0.2336\n",
      "187/507, train_loss: 0.1169\n",
      "188/507, train_loss: 0.2649\n",
      "189/507, train_loss: 0.9976\n",
      "190/507, train_loss: 0.1030\n",
      "191/507, train_loss: 0.3418\n",
      "192/507, train_loss: 0.2529\n",
      "193/507, train_loss: 0.0588\n",
      "194/507, train_loss: 0.3984\n",
      "195/507, train_loss: 0.1777\n",
      "196/507, train_loss: 0.5654\n",
      "197/507, train_loss: 0.2747\n",
      "198/507, train_loss: 0.1244\n",
      "199/507, train_loss: 0.3906\n",
      "200/507, train_loss: 0.1680\n",
      "201/507, train_loss: 0.3342\n",
      "202/507, train_loss: 0.2241\n",
      "203/507, train_loss: 0.0416\n",
      "204/507, train_loss: 0.1993\n",
      "205/507, train_loss: 0.2454\n",
      "206/507, train_loss: 0.4990\n",
      "207/507, train_loss: 0.9209\n",
      "208/507, train_loss: 0.2576\n",
      "209/507, train_loss: 0.7124\n",
      "210/507, train_loss: 0.4641\n",
      "211/507, train_loss: 0.2285\n",
      "212/507, train_loss: 0.0721\n",
      "213/507, train_loss: 0.0443\n",
      "214/507, train_loss: 0.1743\n",
      "215/507, train_loss: 0.1105\n",
      "216/507, train_loss: 1.0176\n",
      "217/507, train_loss: 0.1680\n",
      "218/507, train_loss: 0.1908\n",
      "219/507, train_loss: 0.1697\n",
      "220/507, train_loss: 0.3330\n",
      "221/507, train_loss: 0.0927\n",
      "222/507, train_loss: 0.2449\n",
      "223/507, train_loss: 0.3547\n",
      "224/507, train_loss: 0.4727\n",
      "225/507, train_loss: 0.3691\n",
      "226/507, train_loss: 0.2126\n",
      "227/507, train_loss: 0.3691\n",
      "228/507, train_loss: 0.2671\n",
      "229/507, train_loss: 0.1383\n",
      "230/507, train_loss: 0.7065\n",
      "231/507, train_loss: 0.2520\n",
      "232/507, train_loss: 0.2476\n",
      "233/507, train_loss: 0.7197\n",
      "234/507, train_loss: 0.7451\n",
      "235/507, train_loss: 0.4329\n",
      "236/507, train_loss: 0.2625\n",
      "237/507, train_loss: 0.0979\n",
      "238/507, train_loss: 0.1667\n",
      "239/507, train_loss: 0.1493\n",
      "240/507, train_loss: 0.1963\n",
      "241/507, train_loss: 0.0646\n",
      "242/507, train_loss: 0.2170\n",
      "243/507, train_loss: 0.4727\n",
      "244/507, train_loss: 0.3342\n",
      "245/507, train_loss: 0.6797\n",
      "246/507, train_loss: 0.0967\n",
      "247/507, train_loss: 0.4243\n",
      "248/507, train_loss: 0.2101\n",
      "249/507, train_loss: 0.1116\n",
      "250/507, train_loss: 0.2947\n",
      "251/507, train_loss: 0.2742\n",
      "252/507, train_loss: 0.2869\n",
      "253/507, train_loss: 0.2930\n",
      "254/507, train_loss: 0.1815\n",
      "255/507, train_loss: 0.4233\n",
      "256/507, train_loss: 0.0950\n",
      "257/507, train_loss: 0.3613\n",
      "258/507, train_loss: 0.3252\n",
      "259/507, train_loss: 0.1492\n",
      "260/507, train_loss: 0.5142\n",
      "261/507, train_loss: 0.3149\n",
      "262/507, train_loss: 0.1682\n",
      "263/507, train_loss: 0.2927\n",
      "264/507, train_loss: 0.1946\n",
      "265/507, train_loss: 0.4829\n",
      "266/507, train_loss: 0.2428\n",
      "267/507, train_loss: 0.2549\n",
      "268/507, train_loss: 0.7207\n",
      "269/507, train_loss: 0.1646\n",
      "270/507, train_loss: 0.2510\n",
      "271/507, train_loss: 0.2478\n",
      "272/507, train_loss: 0.1897\n",
      "273/507, train_loss: 0.1620\n",
      "274/507, train_loss: 0.1141\n",
      "275/507, train_loss: 0.2345\n",
      "276/507, train_loss: 0.5337\n",
      "277/507, train_loss: 0.3101\n",
      "278/507, train_loss: 0.0597\n",
      "279/507, train_loss: 0.6465\n",
      "280/507, train_loss: 0.2263\n",
      "281/507, train_loss: 0.6162\n",
      "282/507, train_loss: 0.1667\n",
      "283/507, train_loss: 0.2374\n",
      "284/507, train_loss: 0.1653\n",
      "285/507, train_loss: 0.4971\n",
      "286/507, train_loss: 0.1489\n",
      "287/507, train_loss: 0.1252\n",
      "288/507, train_loss: 0.1578\n",
      "289/507, train_loss: 0.4553\n",
      "290/507, train_loss: 0.1860\n",
      "291/507, train_loss: 0.1633\n",
      "292/507, train_loss: 0.2018\n",
      "293/507, train_loss: 0.1228\n",
      "294/507, train_loss: 0.1202\n",
      "295/507, train_loss: 0.3018\n",
      "296/507, train_loss: 0.2947\n",
      "297/507, train_loss: 0.3989\n",
      "298/507, train_loss: 0.1343\n",
      "299/507, train_loss: 0.2336\n",
      "300/507, train_loss: 0.1802\n",
      "301/507, train_loss: 0.2783\n",
      "302/507, train_loss: 0.2917\n",
      "303/507, train_loss: 0.1724\n",
      "304/507, train_loss: 0.0962\n",
      "305/507, train_loss: 0.0289\n",
      "306/507, train_loss: 0.1748\n",
      "307/507, train_loss: 0.1775\n",
      "308/507, train_loss: 0.1055\n",
      "309/507, train_loss: 0.3188\n",
      "310/507, train_loss: 0.1553\n",
      "311/507, train_loss: 0.0627\n",
      "312/507, train_loss: 0.2150\n",
      "313/507, train_loss: 0.2365\n",
      "314/507, train_loss: 0.2349\n",
      "315/507, train_loss: 0.2734\n",
      "316/507, train_loss: 0.0341\n",
      "317/507, train_loss: 0.3401\n",
      "318/507, train_loss: 0.5352\n",
      "319/507, train_loss: 0.1246\n",
      "320/507, train_loss: 0.1691\n",
      "321/507, train_loss: 0.6787\n",
      "322/507, train_loss: 0.5615\n",
      "323/507, train_loss: 0.4319\n",
      "324/507, train_loss: 0.2329\n",
      "325/507, train_loss: 0.2732\n",
      "326/507, train_loss: 0.1790\n",
      "327/507, train_loss: 0.3691\n",
      "328/507, train_loss: 0.1497\n",
      "329/507, train_loss: 0.1619\n",
      "330/507, train_loss: 0.9004\n",
      "331/507, train_loss: 0.0219\n",
      "332/507, train_loss: 0.2089\n",
      "333/507, train_loss: 0.8340\n",
      "334/507, train_loss: 0.4819\n",
      "335/507, train_loss: 0.2467\n",
      "336/507, train_loss: 0.1995\n",
      "337/507, train_loss: 0.2544\n",
      "338/507, train_loss: 0.2803\n",
      "339/507, train_loss: 0.1266\n",
      "340/507, train_loss: 0.2454\n",
      "341/507, train_loss: 0.1799\n",
      "342/507, train_loss: 0.1097\n",
      "343/507, train_loss: 0.2413\n",
      "344/507, train_loss: 0.1550\n",
      "345/507, train_loss: 0.3411\n",
      "346/507, train_loss: 0.3696\n",
      "347/507, train_loss: 0.3843\n",
      "348/507, train_loss: 0.1272\n",
      "349/507, train_loss: 0.2452\n",
      "350/507, train_loss: 0.1963\n",
      "351/507, train_loss: 0.1027\n",
      "352/507, train_loss: 0.5469\n",
      "353/507, train_loss: 0.7290\n",
      "354/507, train_loss: 0.2012\n",
      "355/507, train_loss: 0.7227\n",
      "356/507, train_loss: 0.0169\n",
      "357/507, train_loss: 0.5557\n",
      "358/507, train_loss: 0.3296\n",
      "359/507, train_loss: 0.2097\n",
      "360/507, train_loss: 0.7617\n",
      "361/507, train_loss: 0.4600\n",
      "362/507, train_loss: 0.1914\n",
      "363/507, train_loss: 0.4160\n",
      "364/507, train_loss: 0.4011\n",
      "365/507, train_loss: 0.1763\n",
      "366/507, train_loss: 0.3271\n",
      "367/507, train_loss: 0.2808\n",
      "368/507, train_loss: 0.5791\n",
      "369/507, train_loss: 0.4299\n",
      "370/507, train_loss: 0.3906\n",
      "371/507, train_loss: 0.6465\n",
      "372/507, train_loss: 0.4460\n",
      "373/507, train_loss: 0.2112\n",
      "374/507, train_loss: 0.1033\n",
      "375/507, train_loss: 0.2852\n",
      "376/507, train_loss: 0.1433\n",
      "377/507, train_loss: 0.5088\n",
      "378/507, train_loss: 0.2100\n",
      "379/507, train_loss: 0.4800\n",
      "380/507, train_loss: 0.2131\n",
      "381/507, train_loss: 0.2803\n",
      "382/507, train_loss: 0.7632\n",
      "383/507, train_loss: 0.5415\n",
      "384/507, train_loss: 0.6123\n",
      "385/507, train_loss: 0.3994\n",
      "386/507, train_loss: 0.3413\n",
      "387/507, train_loss: 0.1215\n",
      "388/507, train_loss: 0.7144\n",
      "389/507, train_loss: 0.7651\n",
      "390/507, train_loss: 0.1083\n",
      "391/507, train_loss: 0.1786\n",
      "392/507, train_loss: 0.3804\n",
      "393/507, train_loss: 0.4033\n",
      "394/507, train_loss: 0.0529\n",
      "395/507, train_loss: 0.0349\n",
      "396/507, train_loss: 0.1781\n",
      "397/507, train_loss: 0.1165\n",
      "398/507, train_loss: 0.2249\n",
      "399/507, train_loss: 0.6753\n",
      "400/507, train_loss: 0.3501\n",
      "401/507, train_loss: 0.1061\n",
      "402/507, train_loss: 0.7368\n",
      "403/507, train_loss: 0.1376\n",
      "404/507, train_loss: 0.2832\n",
      "405/507, train_loss: 0.2170\n",
      "406/507, train_loss: 0.6646\n",
      "407/507, train_loss: 0.1704\n",
      "408/507, train_loss: 0.3413\n",
      "409/507, train_loss: 0.7490\n",
      "410/507, train_loss: 0.4902\n",
      "411/507, train_loss: 0.1904\n",
      "412/507, train_loss: 0.1934\n",
      "413/507, train_loss: 0.3655\n",
      "414/507, train_loss: 0.0291\n",
      "415/507, train_loss: 0.0718\n",
      "416/507, train_loss: 0.1522\n",
      "417/507, train_loss: 0.6919\n",
      "418/507, train_loss: 0.1827\n",
      "419/507, train_loss: 0.5049\n",
      "420/507, train_loss: 0.2742\n",
      "421/507, train_loss: 0.4951\n",
      "422/507, train_loss: 0.1288\n",
      "423/507, train_loss: 0.1985\n",
      "424/507, train_loss: 0.1932\n",
      "425/507, train_loss: 0.5098\n",
      "426/507, train_loss: 0.1340\n",
      "427/507, train_loss: 0.3877\n",
      "428/507, train_loss: 0.1643\n",
      "429/507, train_loss: 0.3145\n",
      "430/507, train_loss: 0.7109\n",
      "431/507, train_loss: 0.5889\n",
      "432/507, train_loss: 0.2627\n",
      "433/507, train_loss: 0.1377\n",
      "434/507, train_loss: 0.2871\n",
      "435/507, train_loss: 0.5176\n",
      "436/507, train_loss: 0.2410\n",
      "437/507, train_loss: 0.1221\n",
      "438/507, train_loss: 0.5615\n",
      "439/507, train_loss: 0.1211\n",
      "440/507, train_loss: 0.2195\n",
      "441/507, train_loss: 0.0330\n",
      "442/507, train_loss: 0.2491\n",
      "443/507, train_loss: 0.6724\n",
      "444/507, train_loss: 0.0406\n",
      "445/507, train_loss: 0.3926\n",
      "446/507, train_loss: 0.2489\n",
      "447/507, train_loss: 0.7607\n",
      "448/507, train_loss: 0.1833\n",
      "449/507, train_loss: 0.3857\n",
      "450/507, train_loss: 0.2080\n",
      "451/507, train_loss: 0.3608\n",
      "452/507, train_loss: 0.1804\n",
      "453/507, train_loss: 0.2100\n",
      "454/507, train_loss: 0.1980\n",
      "455/507, train_loss: 0.2341\n",
      "456/507, train_loss: 0.1084\n",
      "457/507, train_loss: 0.1733\n",
      "458/507, train_loss: 0.3096\n",
      "459/507, train_loss: 0.1912\n",
      "460/507, train_loss: 0.0657\n",
      "461/507, train_loss: 0.5908\n",
      "462/507, train_loss: 0.1067\n",
      "463/507, train_loss: 0.2725\n",
      "464/507, train_loss: 0.2507\n",
      "465/507, train_loss: 0.2876\n",
      "466/507, train_loss: 0.1299\n",
      "467/507, train_loss: 0.3169\n",
      "468/507, train_loss: 0.0914\n",
      "469/507, train_loss: 0.1381\n",
      "470/507, train_loss: 0.0519\n",
      "471/507, train_loss: 0.1975\n",
      "472/507, train_loss: 0.2529\n",
      "473/507, train_loss: 0.1616\n",
      "474/507, train_loss: 0.0963\n",
      "475/507, train_loss: 0.4507\n",
      "476/507, train_loss: 0.0729\n",
      "477/507, train_loss: 0.1117\n",
      "478/507, train_loss: 0.1852\n",
      "479/507, train_loss: 0.1302\n",
      "480/507, train_loss: 0.3037\n",
      "481/507, train_loss: 0.1226\n",
      "482/507, train_loss: 0.1108\n",
      "483/507, train_loss: 0.2825\n",
      "484/507, train_loss: 0.5620\n",
      "485/507, train_loss: 0.1682\n",
      "486/507, train_loss: 0.2472\n",
      "487/507, train_loss: 0.1062\n",
      "488/507, train_loss: 0.1694\n",
      "489/507, train_loss: 0.0977\n",
      "490/507, train_loss: 0.3840\n",
      "491/507, train_loss: 0.0216\n",
      "492/507, train_loss: 0.1765\n",
      "493/507, train_loss: 0.2086\n",
      "494/507, train_loss: 0.2046\n",
      "495/507, train_loss: 0.2180\n",
      "496/507, train_loss: 0.2568\n",
      "497/507, train_loss: 0.1652\n",
      "498/507, train_loss: 0.1985\n",
      "499/507, train_loss: 0.3547\n",
      "500/507, train_loss: 0.2258\n",
      "501/507, train_loss: 0.4734\n",
      "502/507, train_loss: 0.3345\n",
      "503/507, train_loss: 0.2871\n",
      "504/507, train_loss: 0.3289\n",
      "505/507, train_loss: 0.2192\n",
      "506/507, train_loss: 0.0998\n",
      "507/507, train_loss: 0.5703\n",
      "epoch 7 average loss: 0.2970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 17:46:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 17:46:20 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 8/20\n",
      "1/507, train_loss: 0.1582\n",
      "2/507, train_loss: 0.2922\n",
      "3/507, train_loss: 0.2695\n",
      "4/507, train_loss: 0.2007\n",
      "5/507, train_loss: 0.2227\n",
      "6/507, train_loss: 0.1338\n",
      "7/507, train_loss: 0.3833\n",
      "8/507, train_loss: 0.4414\n",
      "9/507, train_loss: 0.0651\n",
      "10/507, train_loss: 0.2158\n",
      "11/507, train_loss: 0.0998\n",
      "12/507, train_loss: 0.2429\n",
      "13/507, train_loss: 0.2966\n",
      "14/507, train_loss: 0.2747\n",
      "15/507, train_loss: 0.1943\n",
      "16/507, train_loss: 0.0303\n",
      "17/507, train_loss: 0.2269\n",
      "18/507, train_loss: 0.4619\n",
      "19/507, train_loss: 0.2383\n",
      "20/507, train_loss: 0.2147\n",
      "21/507, train_loss: 0.4675\n",
      "22/507, train_loss: 0.1237\n",
      "23/507, train_loss: 0.3015\n",
      "24/507, train_loss: 0.1412\n",
      "25/507, train_loss: 0.5913\n",
      "26/507, train_loss: 0.0277\n",
      "27/507, train_loss: 0.8774\n",
      "28/507, train_loss: 0.1780\n",
      "29/507, train_loss: 0.0735\n",
      "30/507, train_loss: 0.2480\n",
      "31/507, train_loss: 0.4143\n",
      "32/507, train_loss: 0.7466\n",
      "33/507, train_loss: 0.2524\n",
      "34/507, train_loss: 0.1185\n",
      "35/507, train_loss: 0.2471\n",
      "36/507, train_loss: 0.1927\n",
      "37/507, train_loss: 0.1031\n",
      "38/507, train_loss: 0.4021\n",
      "39/507, train_loss: 0.1029\n",
      "40/507, train_loss: 0.3901\n",
      "41/507, train_loss: 0.1173\n",
      "42/507, train_loss: 0.8311\n",
      "43/507, train_loss: 0.1729\n",
      "44/507, train_loss: 0.1161\n",
      "45/507, train_loss: 0.0423\n",
      "46/507, train_loss: 0.0763\n",
      "47/507, train_loss: 0.5977\n",
      "48/507, train_loss: 0.3491\n",
      "49/507, train_loss: 0.2693\n",
      "50/507, train_loss: 0.2788\n",
      "51/507, train_loss: 0.1104\n",
      "52/507, train_loss: 0.2067\n",
      "53/507, train_loss: 0.2233\n",
      "54/507, train_loss: 0.1726\n",
      "55/507, train_loss: 0.3669\n",
      "56/507, train_loss: 0.2917\n",
      "57/507, train_loss: 0.3394\n",
      "58/507, train_loss: 0.2002\n",
      "59/507, train_loss: 0.1649\n",
      "60/507, train_loss: 0.2715\n",
      "61/507, train_loss: 0.3518\n",
      "62/507, train_loss: 0.3816\n",
      "63/507, train_loss: 0.1423\n",
      "64/507, train_loss: 0.1738\n",
      "65/507, train_loss: 0.2107\n",
      "66/507, train_loss: 0.3408\n",
      "67/507, train_loss: 0.2081\n",
      "68/507, train_loss: 0.0200\n",
      "69/507, train_loss: 0.8198\n",
      "70/507, train_loss: 0.3000\n",
      "71/507, train_loss: 0.1575\n",
      "72/507, train_loss: 0.1592\n",
      "73/507, train_loss: 0.3760\n",
      "74/507, train_loss: 0.4111\n",
      "75/507, train_loss: 0.6934\n",
      "76/507, train_loss: 0.4009\n",
      "77/507, train_loss: 0.4773\n",
      "78/507, train_loss: 0.3337\n",
      "79/507, train_loss: 0.3003\n",
      "80/507, train_loss: 0.1533\n",
      "81/507, train_loss: 0.2123\n",
      "82/507, train_loss: 0.0706\n",
      "83/507, train_loss: 0.6543\n",
      "84/507, train_loss: 0.3135\n",
      "85/507, train_loss: 0.1349\n",
      "86/507, train_loss: 0.3503\n",
      "87/507, train_loss: 0.1508\n",
      "88/507, train_loss: 0.2781\n",
      "89/507, train_loss: 0.1808\n",
      "90/507, train_loss: 0.4883\n",
      "91/507, train_loss: 0.1722\n",
      "92/507, train_loss: 0.1560\n",
      "93/507, train_loss: 0.7617\n",
      "94/507, train_loss: 0.0498\n",
      "95/507, train_loss: 0.3496\n",
      "96/507, train_loss: 0.7295\n",
      "97/507, train_loss: 0.2690\n",
      "98/507, train_loss: 0.3799\n",
      "99/507, train_loss: 0.8804\n",
      "100/507, train_loss: 0.1882\n",
      "101/507, train_loss: 0.2830\n",
      "102/507, train_loss: 0.2064\n",
      "103/507, train_loss: 0.0472\n",
      "104/507, train_loss: 0.2944\n",
      "105/507, train_loss: 0.3125\n",
      "106/507, train_loss: 0.1527\n",
      "107/507, train_loss: 0.1349\n",
      "108/507, train_loss: 0.4353\n",
      "109/507, train_loss: 0.2494\n",
      "110/507, train_loss: 0.0657\n",
      "111/507, train_loss: 0.3474\n",
      "112/507, train_loss: 0.2610\n",
      "113/507, train_loss: 0.2312\n",
      "114/507, train_loss: 0.6299\n",
      "115/507, train_loss: 0.2537\n",
      "116/507, train_loss: 0.4175\n",
      "117/507, train_loss: 0.5771\n",
      "118/507, train_loss: 0.2529\n",
      "119/507, train_loss: 0.0738\n",
      "120/507, train_loss: 0.3188\n",
      "121/507, train_loss: 0.7598\n",
      "122/507, train_loss: 0.6265\n",
      "123/507, train_loss: 0.1996\n",
      "124/507, train_loss: 0.1522\n",
      "125/507, train_loss: 0.1565\n",
      "126/507, train_loss: 0.0400\n",
      "127/507, train_loss: 0.2617\n",
      "128/507, train_loss: 0.2216\n",
      "129/507, train_loss: 0.1777\n",
      "130/507, train_loss: 0.0281\n",
      "131/507, train_loss: 0.7207\n",
      "132/507, train_loss: 0.9697\n",
      "133/507, train_loss: 0.1575\n",
      "134/507, train_loss: 0.1868\n",
      "135/507, train_loss: 0.5479\n",
      "136/507, train_loss: 0.1918\n",
      "137/507, train_loss: 0.1963\n",
      "138/507, train_loss: 0.1273\n",
      "139/507, train_loss: 0.2893\n",
      "140/507, train_loss: 0.1636\n",
      "141/507, train_loss: 0.2191\n",
      "142/507, train_loss: 0.1405\n",
      "143/507, train_loss: 0.5068\n",
      "144/507, train_loss: 0.1350\n",
      "145/507, train_loss: 0.5938\n",
      "146/507, train_loss: 0.1489\n",
      "147/507, train_loss: 0.0632\n",
      "148/507, train_loss: 0.1658\n",
      "149/507, train_loss: 0.1416\n",
      "150/507, train_loss: 0.1951\n",
      "151/507, train_loss: 0.2181\n",
      "152/507, train_loss: 0.3062\n",
      "153/507, train_loss: 0.9795\n",
      "154/507, train_loss: 0.2314\n",
      "155/507, train_loss: 0.6870\n",
      "156/507, train_loss: 0.1225\n",
      "157/507, train_loss: 0.1882\n",
      "158/507, train_loss: 0.2394\n",
      "159/507, train_loss: 0.2211\n",
      "160/507, train_loss: 0.4016\n",
      "161/507, train_loss: 0.4260\n",
      "162/507, train_loss: 0.1765\n",
      "163/507, train_loss: 0.2537\n",
      "164/507, train_loss: 0.1338\n",
      "165/507, train_loss: 0.1406\n",
      "166/507, train_loss: 0.3325\n",
      "167/507, train_loss: 0.1483\n",
      "168/507, train_loss: 0.1011\n",
      "169/507, train_loss: 0.2305\n",
      "170/507, train_loss: 0.2306\n",
      "171/507, train_loss: 0.2749\n",
      "172/507, train_loss: 0.1615\n",
      "173/507, train_loss: 0.3857\n",
      "174/507, train_loss: 0.1827\n",
      "175/507, train_loss: 0.1487\n",
      "176/507, train_loss: 0.5928\n",
      "177/507, train_loss: 0.3386\n",
      "178/507, train_loss: 0.2025\n",
      "179/507, train_loss: 0.2222\n",
      "180/507, train_loss: 0.0239\n",
      "181/507, train_loss: 0.4963\n",
      "182/507, train_loss: 0.2068\n",
      "183/507, train_loss: 0.1664\n",
      "184/507, train_loss: 0.2585\n",
      "185/507, train_loss: 0.1472\n",
      "186/507, train_loss: 0.1400\n",
      "187/507, train_loss: 0.1898\n",
      "188/507, train_loss: 0.3975\n",
      "189/507, train_loss: 0.1338\n",
      "190/507, train_loss: 0.1176\n",
      "191/507, train_loss: 0.9819\n",
      "192/507, train_loss: 0.3127\n",
      "193/507, train_loss: 0.1366\n",
      "194/507, train_loss: 0.1758\n",
      "195/507, train_loss: 0.4839\n",
      "196/507, train_loss: 0.3254\n",
      "197/507, train_loss: 0.2192\n",
      "198/507, train_loss: 0.7300\n",
      "199/507, train_loss: 0.1923\n",
      "200/507, train_loss: 0.1919\n",
      "201/507, train_loss: 0.0400\n",
      "202/507, train_loss: 0.4241\n",
      "203/507, train_loss: 0.2571\n",
      "204/507, train_loss: 0.5156\n",
      "205/507, train_loss: 0.2136\n",
      "206/507, train_loss: 0.2164\n",
      "207/507, train_loss: 0.0979\n",
      "208/507, train_loss: 0.5908\n",
      "209/507, train_loss: 0.4109\n",
      "210/507, train_loss: 0.3525\n",
      "211/507, train_loss: 0.3813\n",
      "212/507, train_loss: 0.0423\n",
      "213/507, train_loss: 0.5171\n",
      "214/507, train_loss: 0.3438\n",
      "215/507, train_loss: 1.0078\n",
      "216/507, train_loss: 0.0884\n",
      "217/507, train_loss: 0.2559\n",
      "218/507, train_loss: 0.2554\n",
      "219/507, train_loss: 0.2345\n",
      "220/507, train_loss: 0.0836\n",
      "221/507, train_loss: 0.4417\n",
      "222/507, train_loss: 0.2169\n",
      "223/507, train_loss: 0.3870\n",
      "224/507, train_loss: 0.2125\n",
      "225/507, train_loss: 0.0310\n",
      "226/507, train_loss: 0.4055\n",
      "227/507, train_loss: 0.5420\n",
      "228/507, train_loss: 0.5171\n",
      "229/507, train_loss: 0.5942\n",
      "230/507, train_loss: 0.3987\n",
      "231/507, train_loss: 0.1510\n",
      "232/507, train_loss: 0.2410\n",
      "233/507, train_loss: 0.9976\n",
      "234/507, train_loss: 0.1780\n",
      "235/507, train_loss: 0.3831\n",
      "236/507, train_loss: 0.1252\n",
      "237/507, train_loss: 0.2505\n",
      "238/507, train_loss: 0.1763\n",
      "239/507, train_loss: 0.0328\n",
      "240/507, train_loss: 0.2979\n",
      "241/507, train_loss: 0.7783\n",
      "242/507, train_loss: 0.1411\n",
      "243/507, train_loss: 0.0824\n",
      "244/507, train_loss: 0.7085\n",
      "245/507, train_loss: 0.2686\n",
      "246/507, train_loss: 0.2942\n",
      "247/507, train_loss: 0.2646\n",
      "248/507, train_loss: 0.0292\n",
      "249/507, train_loss: 0.2539\n",
      "250/507, train_loss: 0.2898\n",
      "251/507, train_loss: 0.3215\n",
      "252/507, train_loss: 0.1714\n",
      "253/507, train_loss: 0.2026\n",
      "254/507, train_loss: 0.1775\n",
      "255/507, train_loss: 0.1826\n",
      "256/507, train_loss: 0.3027\n",
      "257/507, train_loss: 0.1620\n",
      "258/507, train_loss: 0.1220\n",
      "259/507, train_loss: 0.6528\n",
      "260/507, train_loss: 0.6011\n",
      "261/507, train_loss: 0.1688\n",
      "262/507, train_loss: 0.2102\n",
      "263/507, train_loss: 0.1128\n",
      "264/507, train_loss: 0.2302\n",
      "265/507, train_loss: 0.1509\n",
      "266/507, train_loss: 0.2217\n",
      "267/507, train_loss: 0.4668\n",
      "268/507, train_loss: 0.1190\n",
      "269/507, train_loss: 0.1017\n",
      "270/507, train_loss: 0.2805\n",
      "271/507, train_loss: 0.4038\n",
      "272/507, train_loss: 0.1936\n",
      "273/507, train_loss: 0.0299\n",
      "274/507, train_loss: 0.1641\n",
      "275/507, train_loss: 0.3367\n",
      "276/507, train_loss: 0.4326\n",
      "277/507, train_loss: 0.4976\n",
      "278/507, train_loss: 0.1984\n",
      "279/507, train_loss: 0.5854\n",
      "280/507, train_loss: 0.5327\n",
      "281/507, train_loss: 0.2341\n",
      "282/507, train_loss: 0.1659\n",
      "283/507, train_loss: 0.4453\n",
      "284/507, train_loss: 0.3557\n",
      "285/507, train_loss: 0.4316\n",
      "286/507, train_loss: 0.3579\n",
      "287/507, train_loss: 0.2424\n",
      "288/507, train_loss: 0.1938\n",
      "289/507, train_loss: 0.4434\n",
      "290/507, train_loss: 0.1819\n",
      "291/507, train_loss: 0.1550\n",
      "292/507, train_loss: 0.0257\n",
      "293/507, train_loss: 0.1226\n",
      "294/507, train_loss: 0.1249\n",
      "295/507, train_loss: 0.1644\n",
      "296/507, train_loss: 0.1884\n",
      "297/507, train_loss: 0.1018\n",
      "298/507, train_loss: 0.4097\n",
      "299/507, train_loss: 0.3372\n",
      "300/507, train_loss: 0.2300\n",
      "301/507, train_loss: 0.2932\n",
      "302/507, train_loss: 0.3748\n",
      "303/507, train_loss: 0.2859\n",
      "304/507, train_loss: 0.0930\n",
      "305/507, train_loss: 0.0804\n",
      "306/507, train_loss: 0.1501\n",
      "307/507, train_loss: 0.1819\n",
      "308/507, train_loss: 0.1802\n",
      "309/507, train_loss: 0.2485\n",
      "310/507, train_loss: 0.5054\n",
      "311/507, train_loss: 0.1594\n",
      "312/507, train_loss: 0.2205\n",
      "313/507, train_loss: 0.6074\n",
      "314/507, train_loss: 0.2383\n",
      "315/507, train_loss: 0.4517\n",
      "316/507, train_loss: 0.3677\n",
      "317/507, train_loss: 0.0930\n",
      "318/507, train_loss: 0.2441\n",
      "319/507, train_loss: 0.0547\n",
      "320/507, train_loss: 0.2915\n",
      "321/507, train_loss: 0.2012\n",
      "322/507, train_loss: 0.3662\n",
      "323/507, train_loss: 0.1019\n",
      "324/507, train_loss: 0.5815\n",
      "325/507, train_loss: 0.5776\n",
      "326/507, train_loss: 0.1725\n",
      "327/507, train_loss: 0.1323\n",
      "328/507, train_loss: 0.2944\n",
      "329/507, train_loss: 0.1843\n",
      "330/507, train_loss: 0.1272\n",
      "331/507, train_loss: 0.1240\n",
      "332/507, train_loss: 0.2275\n",
      "333/507, train_loss: 0.2148\n",
      "334/507, train_loss: 1.0049\n",
      "335/507, train_loss: 0.5049\n",
      "336/507, train_loss: 0.1892\n",
      "337/507, train_loss: 0.3450\n",
      "338/507, train_loss: 0.1514\n",
      "339/507, train_loss: 0.8145\n",
      "340/507, train_loss: 0.2441\n",
      "341/507, train_loss: 0.1044\n",
      "342/507, train_loss: 0.2898\n",
      "343/507, train_loss: 0.4182\n",
      "344/507, train_loss: 0.2891\n",
      "345/507, train_loss: 0.1676\n",
      "346/507, train_loss: 0.2944\n",
      "347/507, train_loss: 0.2362\n",
      "348/507, train_loss: 0.4443\n",
      "349/507, train_loss: 0.1224\n",
      "350/507, train_loss: 0.0509\n",
      "351/507, train_loss: 0.1794\n",
      "352/507, train_loss: 0.0850\n",
      "353/507, train_loss: 0.2180\n",
      "354/507, train_loss: 0.2180\n",
      "355/507, train_loss: 0.1094\n",
      "356/507, train_loss: 0.5835\n",
      "357/507, train_loss: 0.3225\n",
      "358/507, train_loss: 0.0210\n",
      "359/507, train_loss: 0.1282\n",
      "360/507, train_loss: 0.0344\n",
      "361/507, train_loss: 0.2119\n",
      "362/507, train_loss: 0.5020\n",
      "363/507, train_loss: 0.1951\n",
      "364/507, train_loss: 0.0792\n",
      "365/507, train_loss: 0.1381\n",
      "366/507, train_loss: 0.4871\n",
      "367/507, train_loss: 0.1331\n",
      "368/507, train_loss: 0.3835\n",
      "369/507, train_loss: 1.0439\n",
      "370/507, train_loss: 0.2642\n",
      "371/507, train_loss: 0.9126\n",
      "372/507, train_loss: 0.1035\n",
      "373/507, train_loss: 0.1448\n",
      "374/507, train_loss: 0.5508\n",
      "375/507, train_loss: 0.1721\n",
      "376/507, train_loss: 0.6855\n",
      "377/507, train_loss: 0.1401\n",
      "378/507, train_loss: 0.0327\n",
      "379/507, train_loss: 0.4321\n",
      "380/507, train_loss: 0.3279\n",
      "381/507, train_loss: 0.3359\n",
      "382/507, train_loss: 0.0690\n",
      "383/507, train_loss: 0.0406\n",
      "384/507, train_loss: 0.1606\n",
      "385/507, train_loss: 0.1171\n",
      "386/507, train_loss: 0.3457\n",
      "387/507, train_loss: 0.0559\n",
      "388/507, train_loss: 0.2051\n",
      "389/507, train_loss: 0.1621\n",
      "390/507, train_loss: 0.0342\n",
      "391/507, train_loss: 0.2256\n",
      "392/507, train_loss: 0.1884\n",
      "393/507, train_loss: 0.5664\n",
      "394/507, train_loss: 0.2705\n",
      "395/507, train_loss: 0.1328\n",
      "396/507, train_loss: 0.1636\n",
      "397/507, train_loss: 0.3315\n",
      "398/507, train_loss: 0.2581\n",
      "399/507, train_loss: 0.3296\n",
      "400/507, train_loss: 0.1495\n",
      "401/507, train_loss: 0.3069\n",
      "402/507, train_loss: 0.2372\n",
      "403/507, train_loss: 0.5811\n",
      "404/507, train_loss: 0.1334\n",
      "405/507, train_loss: 0.4062\n",
      "406/507, train_loss: 0.2499\n",
      "407/507, train_loss: 0.1223\n",
      "408/507, train_loss: 0.7261\n",
      "409/507, train_loss: 0.2859\n",
      "410/507, train_loss: 0.5557\n",
      "411/507, train_loss: 0.4146\n",
      "412/507, train_loss: 0.2074\n",
      "413/507, train_loss: 0.1360\n",
      "414/507, train_loss: 0.0732\n",
      "415/507, train_loss: 0.0367\n",
      "416/507, train_loss: 0.1968\n",
      "417/507, train_loss: 0.2341\n",
      "418/507, train_loss: 0.4246\n",
      "419/507, train_loss: 0.1885\n",
      "420/507, train_loss: 0.4175\n",
      "421/507, train_loss: 0.1949\n",
      "422/507, train_loss: 0.4612\n",
      "423/507, train_loss: 0.1586\n",
      "424/507, train_loss: 0.1534\n",
      "425/507, train_loss: 0.2515\n",
      "426/507, train_loss: 0.5938\n",
      "427/507, train_loss: 0.3318\n",
      "428/507, train_loss: 0.0836\n",
      "429/507, train_loss: 0.2184\n",
      "430/507, train_loss: 0.4883\n",
      "431/507, train_loss: 0.3359\n",
      "432/507, train_loss: 0.1903\n",
      "433/507, train_loss: 0.1687\n",
      "434/507, train_loss: 0.5234\n",
      "435/507, train_loss: 0.1241\n",
      "436/507, train_loss: 0.1759\n",
      "437/507, train_loss: 0.2700\n",
      "438/507, train_loss: 0.0658\n",
      "439/507, train_loss: 0.6782\n",
      "440/507, train_loss: 0.2944\n",
      "441/507, train_loss: 0.2133\n",
      "442/507, train_loss: 0.1654\n",
      "443/507, train_loss: 0.2168\n",
      "444/507, train_loss: 0.1216\n",
      "445/507, train_loss: 0.3530\n",
      "446/507, train_loss: 0.2112\n",
      "447/507, train_loss: 0.3398\n",
      "448/507, train_loss: 0.1371\n",
      "449/507, train_loss: 0.0833\n",
      "450/507, train_loss: 0.2034\n",
      "451/507, train_loss: 0.1975\n",
      "452/507, train_loss: 0.3347\n",
      "453/507, train_loss: 0.1421\n",
      "454/507, train_loss: 0.1658\n",
      "455/507, train_loss: 0.2047\n",
      "456/507, train_loss: 0.3420\n",
      "457/507, train_loss: 0.1873\n",
      "458/507, train_loss: 0.0757\n",
      "459/507, train_loss: 0.1272\n",
      "460/507, train_loss: 0.4409\n",
      "461/507, train_loss: 0.3208\n",
      "462/507, train_loss: 0.1554\n",
      "463/507, train_loss: 0.1510\n",
      "464/507, train_loss: 0.2206\n",
      "465/507, train_loss: 0.1255\n",
      "466/507, train_loss: 0.0493\n",
      "467/507, train_loss: 0.2725\n",
      "468/507, train_loss: 0.3647\n",
      "469/507, train_loss: 0.5518\n",
      "470/507, train_loss: 0.1000\n",
      "471/507, train_loss: 1.1758\n",
      "472/507, train_loss: 0.2437\n",
      "473/507, train_loss: 0.2820\n",
      "474/507, train_loss: 0.4207\n",
      "475/507, train_loss: 0.1499\n",
      "476/507, train_loss: 0.3792\n",
      "477/507, train_loss: 0.1041\n",
      "478/507, train_loss: 0.2693\n",
      "479/507, train_loss: 0.1539\n",
      "480/507, train_loss: 0.2097\n",
      "481/507, train_loss: 1.0215\n",
      "482/507, train_loss: 0.2075\n",
      "483/507, train_loss: 0.2441\n",
      "484/507, train_loss: 0.2520\n",
      "485/507, train_loss: 0.2429\n",
      "486/507, train_loss: 0.3296\n",
      "487/507, train_loss: 0.2617\n",
      "488/507, train_loss: 0.2656\n",
      "489/507, train_loss: 0.1573\n",
      "490/507, train_loss: 0.1553\n",
      "491/507, train_loss: 0.1465\n",
      "492/507, train_loss: 0.0991\n",
      "493/507, train_loss: 0.8047\n",
      "494/507, train_loss: 0.4697\n",
      "495/507, train_loss: 0.5376\n",
      "496/507, train_loss: 0.1597\n",
      "497/507, train_loss: 0.1711\n",
      "498/507, train_loss: 0.4824\n",
      "499/507, train_loss: 0.1669\n",
      "500/507, train_loss: 0.3149\n",
      "501/507, train_loss: 0.1249\n",
      "502/507, train_loss: 0.1278\n",
      "503/507, train_loss: 0.2307\n",
      "504/507, train_loss: 0.2419\n",
      "505/507, train_loss: 0.1765\n",
      "506/507, train_loss: 0.0682\n",
      "507/507, train_loss: 0.2107\n",
      "epoch 8 average loss: 0.2822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/08/29 19:48:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/08/29 19:48:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 9/20\n",
      "1/507, train_loss: 0.1887\n",
      "2/507, train_loss: 0.4707\n",
      "3/507, train_loss: 0.2815\n",
      "4/507, train_loss: 0.5269\n",
      "5/507, train_loss: 0.1871\n",
      "6/507, train_loss: 0.2939\n",
      "7/507, train_loss: 0.1510\n",
      "8/507, train_loss: 0.5708\n",
      "9/507, train_loss: 0.7051\n",
      "10/507, train_loss: 0.1073\n",
      "11/507, train_loss: 0.4731\n",
      "12/507, train_loss: 0.4299\n",
      "13/507, train_loss: 0.1542\n",
      "14/507, train_loss: 0.2051\n",
      "15/507, train_loss: 0.4504\n",
      "16/507, train_loss: 0.9619\n",
      "17/507, train_loss: 0.0283\n",
      "18/507, train_loss: 0.1659\n",
      "19/507, train_loss: 0.2043\n",
      "20/507, train_loss: 0.3733\n",
      "21/507, train_loss: 0.2118\n",
      "22/507, train_loss: 0.1415\n",
      "23/507, train_loss: 0.1934\n",
      "24/507, train_loss: 0.5962\n",
      "25/507, train_loss: 0.1553\n",
      "26/507, train_loss: 0.1190\n",
      "27/507, train_loss: 0.1741\n",
      "28/507, train_loss: 0.0994\n",
      "29/507, train_loss: 0.2341\n",
      "30/507, train_loss: 0.1860\n",
      "31/507, train_loss: 0.5938\n",
      "32/507, train_loss: 0.2712\n",
      "33/507, train_loss: 0.1522\n",
      "34/507, train_loss: 0.6240\n",
      "35/507, train_loss: 0.1071\n",
      "36/507, train_loss: 0.1414\n",
      "37/507, train_loss: 0.3928\n",
      "38/507, train_loss: 0.2013\n",
      "39/507, train_loss: 0.1338\n",
      "40/507, train_loss: 0.1866\n",
      "41/507, train_loss: 0.2194\n",
      "42/507, train_loss: 0.1216\n",
      "43/507, train_loss: 0.2629\n",
      "44/507, train_loss: 0.3501\n",
      "45/507, train_loss: 0.1465\n",
      "46/507, train_loss: 0.1117\n",
      "47/507, train_loss: 0.0149\n",
      "48/507, train_loss: 0.2673\n",
      "49/507, train_loss: 0.0166\n",
      "50/507, train_loss: 0.0119\n",
      "51/507, train_loss: 0.0123\n",
      "52/507, train_loss: 0.6572\n",
      "53/507, train_loss: 0.4963\n",
      "54/507, train_loss: 0.2932\n",
      "55/507, train_loss: 0.2195\n",
      "56/507, train_loss: 0.2114\n",
      "57/507, train_loss: 0.8818\n",
      "58/507, train_loss: 0.1992\n",
      "59/507, train_loss: 0.1307\n",
      "60/507, train_loss: 0.0382\n",
      "61/507, train_loss: 0.3936\n",
      "62/507, train_loss: 0.2690\n",
      "63/507, train_loss: 0.3203\n",
      "64/507, train_loss: 0.1626\n",
      "65/507, train_loss: 0.2683\n",
      "66/507, train_loss: 0.3743\n",
      "67/507, train_loss: 0.3901\n",
      "68/507, train_loss: 0.3645\n",
      "69/507, train_loss: 0.2192\n",
      "70/507, train_loss: 0.1587\n",
      "71/507, train_loss: 0.3115\n",
      "72/507, train_loss: 0.1199\n",
      "73/507, train_loss: 0.2805\n",
      "74/507, train_loss: 0.1277\n",
      "75/507, train_loss: 0.2180\n",
      "76/507, train_loss: 0.1631\n",
      "77/507, train_loss: 0.0515\n",
      "78/507, train_loss: 0.1195\n",
      "79/507, train_loss: 0.0255\n",
      "80/507, train_loss: 0.0875\n",
      "81/507, train_loss: 0.0546\n",
      "82/507, train_loss: 0.6802\n",
      "83/507, train_loss: 0.7744\n",
      "84/507, train_loss: 0.0820\n",
      "85/507, train_loss: 0.4136\n",
      "86/507, train_loss: 0.3076\n",
      "87/507, train_loss: 0.2465\n",
      "88/507, train_loss: 0.3579\n",
      "89/507, train_loss: 0.2155\n",
      "90/507, train_loss: 0.1425\n",
      "91/507, train_loss: 0.3149\n",
      "92/507, train_loss: 0.2314\n",
      "93/507, train_loss: 0.1360\n",
      "94/507, train_loss: 0.1802\n",
      "95/507, train_loss: 0.2474\n",
      "96/507, train_loss: 0.4622\n",
      "97/507, train_loss: 0.2939\n",
      "98/507, train_loss: 0.0172\n",
      "99/507, train_loss: 0.2091\n",
      "100/507, train_loss: 0.0384\n",
      "101/507, train_loss: 0.1165\n",
      "102/507, train_loss: 0.3899\n",
      "103/507, train_loss: 0.3650\n",
      "104/507, train_loss: 0.1882\n",
      "105/507, train_loss: 0.0164\n",
      "106/507, train_loss: 0.1663\n",
      "107/507, train_loss: 0.0795\n",
      "108/507, train_loss: 0.0731\n",
      "109/507, train_loss: 0.0203\n",
      "110/507, train_loss: 0.2808\n",
      "111/507, train_loss: 0.0144\n",
      "112/507, train_loss: 0.6357\n",
      "113/507, train_loss: 0.0262\n",
      "114/507, train_loss: 0.2329\n",
      "115/507, train_loss: 0.1448\n",
      "116/507, train_loss: 0.1503\n",
      "117/507, train_loss: 0.0704\n",
      "118/507, train_loss: 0.6172\n",
      "119/507, train_loss: 0.2632\n",
      "120/507, train_loss: 0.7202\n",
      "121/507, train_loss: 0.2102\n",
      "122/507, train_loss: 0.1656\n",
      "123/507, train_loss: 0.1508\n",
      "124/507, train_loss: 0.0974\n",
      "125/507, train_loss: 0.0271\n",
      "126/507, train_loss: 0.1512\n",
      "127/507, train_loss: 0.1913\n",
      "128/507, train_loss: 0.1431\n",
      "129/507, train_loss: 0.0174\n",
      "130/507, train_loss: 0.1708\n",
      "131/507, train_loss: 0.2939\n",
      "132/507, train_loss: 0.2004\n",
      "133/507, train_loss: 0.1587\n",
      "134/507, train_loss: 0.2078\n",
      "135/507, train_loss: 0.2063\n",
      "136/507, train_loss: 0.4106\n",
      "137/507, train_loss: 0.2239\n",
      "138/507, train_loss: 0.6924\n",
      "139/507, train_loss: 0.3159\n",
      "140/507, train_loss: 0.2467\n",
      "141/507, train_loss: 0.1230\n",
      "142/507, train_loss: 0.3899\n",
      "143/507, train_loss: 0.2893\n",
      "144/507, train_loss: 0.6030\n",
      "145/507, train_loss: 0.4148\n",
      "146/507, train_loss: 0.2009\n",
      "147/507, train_loss: 0.1034\n",
      "148/507, train_loss: 0.2194\n",
      "149/507, train_loss: 0.3228\n",
      "150/507, train_loss: 0.2170\n",
      "151/507, train_loss: 0.0880\n",
      "152/507, train_loss: 0.2600\n",
      "153/507, train_loss: 0.1707\n",
      "154/507, train_loss: 0.2617\n",
      "155/507, train_loss: 0.1768\n",
      "156/507, train_loss: 0.1028\n",
      "157/507, train_loss: 0.5386\n",
      "158/507, train_loss: 0.1428\n",
      "159/507, train_loss: 0.2607\n",
      "160/507, train_loss: 0.6875\n",
      "161/507, train_loss: 0.1418\n",
      "162/507, train_loss: 0.1661\n",
      "163/507, train_loss: 0.2002\n",
      "164/507, train_loss: 0.2651\n",
      "165/507, train_loss: 0.1376\n",
      "166/507, train_loss: 0.2510\n",
      "167/507, train_loss: 0.2920\n",
      "168/507, train_loss: 0.1322\n",
      "169/507, train_loss: 0.1974\n",
      "170/507, train_loss: 0.2097\n",
      "171/507, train_loss: 0.2076\n",
      "172/507, train_loss: 0.1621\n",
      "173/507, train_loss: 0.2363\n",
      "174/507, train_loss: 0.2135\n",
      "175/507, train_loss: 0.0889\n",
      "176/507, train_loss: 0.0983\n",
      "177/507, train_loss: 0.0350\n",
      "178/507, train_loss: 0.5132\n",
      "179/507, train_loss: 0.6235\n",
      "180/507, train_loss: 0.8584\n",
      "181/507, train_loss: 0.1963\n",
      "182/507, train_loss: 0.2527\n",
      "183/507, train_loss: 0.1516\n",
      "184/507, train_loss: 0.2015\n",
      "185/507, train_loss: 0.1533\n",
      "186/507, train_loss: 0.2700\n",
      "187/507, train_loss: 0.1547\n",
      "188/507, train_loss: 0.1858\n",
      "189/507, train_loss: 0.1368\n",
      "190/507, train_loss: 0.6763\n",
      "191/507, train_loss: 0.6821\n",
      "192/507, train_loss: 0.1726\n",
      "193/507, train_loss: 0.5601\n",
      "194/507, train_loss: 0.1924\n",
      "195/507, train_loss: 0.2549\n",
      "196/507, train_loss: 0.3662\n",
      "197/507, train_loss: 0.0756\n",
      "198/507, train_loss: 0.5601\n",
      "199/507, train_loss: 0.1301\n",
      "200/507, train_loss: 0.6221\n",
      "201/507, train_loss: 0.1506\n",
      "202/507, train_loss: 0.5728\n",
      "203/507, train_loss: 0.3547\n",
      "204/507, train_loss: 0.1802\n",
      "205/507, train_loss: 0.4236\n",
      "206/507, train_loss: 0.2233\n",
      "207/507, train_loss: 0.1581\n",
      "208/507, train_loss: 0.5806\n",
      "209/507, train_loss: 0.1709\n",
      "210/507, train_loss: 0.2407\n",
      "211/507, train_loss: 0.1478\n",
      "212/507, train_loss: 0.3289\n",
      "213/507, train_loss: 0.3003\n",
      "214/507, train_loss: 0.6836\n",
      "215/507, train_loss: 0.5977\n",
      "216/507, train_loss: 0.1985\n",
      "217/507, train_loss: 0.3176\n",
      "218/507, train_loss: 0.1553\n",
      "219/507, train_loss: 0.4792\n",
      "220/507, train_loss: 0.1531\n",
      "221/507, train_loss: 0.1526\n",
      "222/507, train_loss: 0.1761\n",
      "223/507, train_loss: 0.2539\n",
      "224/507, train_loss: 0.1641\n",
      "225/507, train_loss: 0.3740\n",
      "226/507, train_loss: 0.2632\n",
      "227/507, train_loss: 0.0458\n",
      "228/507, train_loss: 0.3181\n",
      "229/507, train_loss: 0.2083\n",
      "230/507, train_loss: 0.1671\n",
      "231/507, train_loss: 0.4951\n",
      "232/507, train_loss: 0.1506\n",
      "233/507, train_loss: 0.4778\n",
      "234/507, train_loss: 0.0972\n",
      "235/507, train_loss: 0.3413\n",
      "236/507, train_loss: 0.4238\n",
      "237/507, train_loss: 0.4692\n",
      "238/507, train_loss: 0.0413\n",
      "239/507, train_loss: 0.3083\n",
      "240/507, train_loss: 0.5195\n",
      "241/507, train_loss: 0.5337\n",
      "242/507, train_loss: 0.1663\n",
      "243/507, train_loss: 0.5830\n",
      "244/507, train_loss: 0.0891\n",
      "245/507, train_loss: 0.2825\n",
      "246/507, train_loss: 0.2271\n",
      "247/507, train_loss: 0.1190\n",
      "248/507, train_loss: 0.2451\n",
      "249/507, train_loss: 0.1196\n",
      "250/507, train_loss: 0.2134\n",
      "251/507, train_loss: 0.1068\n",
      "252/507, train_loss: 0.1974\n",
      "253/507, train_loss: 0.4395\n",
      "254/507, train_loss: 0.1562\n",
      "255/507, train_loss: 0.2283\n",
      "256/507, train_loss: 0.1113\n",
      "257/507, train_loss: 0.2515\n",
      "258/507, train_loss: 0.6792\n",
      "259/507, train_loss: 0.6401\n",
      "260/507, train_loss: 0.1740\n",
      "261/507, train_loss: 0.2457\n",
      "262/507, train_loss: 0.1085\n",
      "263/507, train_loss: 0.0336\n",
      "264/507, train_loss: 0.2195\n",
      "265/507, train_loss: 0.1171\n",
      "266/507, train_loss: 0.1536\n",
      "267/507, train_loss: 0.2874\n",
      "268/507, train_loss: 0.1416\n",
      "269/507, train_loss: 0.1514\n",
      "270/507, train_loss: 0.1257\n",
      "271/507, train_loss: 0.1509\n",
      "272/507, train_loss: 0.1902\n",
      "273/507, train_loss: 0.2004\n",
      "274/507, train_loss: 0.1379\n",
      "275/507, train_loss: 0.0883\n",
      "276/507, train_loss: 0.2399\n",
      "277/507, train_loss: 0.4351\n",
      "278/507, train_loss: 0.0883\n",
      "279/507, train_loss: 0.0155\n",
      "280/507, train_loss: 0.1365\n",
      "281/507, train_loss: 0.0212\n",
      "282/507, train_loss: 0.2607\n",
      "283/507, train_loss: 0.2949\n",
      "284/507, train_loss: 0.2184\n",
      "285/507, train_loss: 0.2827\n",
      "286/507, train_loss: 0.1152\n",
      "287/507, train_loss: 0.2024\n",
      "288/507, train_loss: 0.6982\n",
      "289/507, train_loss: 0.3276\n",
      "290/507, train_loss: 0.7646\n",
      "291/507, train_loss: 0.1497\n",
      "292/507, train_loss: 0.4294\n",
      "293/507, train_loss: 0.0988\n",
      "294/507, train_loss: 0.1433\n",
      "295/507, train_loss: 0.2949\n",
      "296/507, train_loss: 0.1985\n",
      "297/507, train_loss: 0.7031\n",
      "298/507, train_loss: 0.2427\n",
      "299/507, train_loss: 0.1511\n",
      "300/507, train_loss: 0.1412\n",
      "301/507, train_loss: 0.4922\n",
      "302/507, train_loss: 0.1819\n",
      "303/507, train_loss: 0.7637\n",
      "304/507, train_loss: 0.0463\n",
      "305/507, train_loss: 0.2231\n",
      "306/507, train_loss: 0.0610\n",
      "307/507, train_loss: 0.0819\n",
      "308/507, train_loss: 0.2017\n",
      "309/507, train_loss: 0.1311\n",
      "310/507, train_loss: 0.3613\n",
      "311/507, train_loss: 0.1105\n",
      "312/507, train_loss: 0.5410\n",
      "313/507, train_loss: 0.2708\n",
      "314/507, train_loss: 0.2439\n",
      "315/507, train_loss: 0.1016\n",
      "316/507, train_loss: 0.7612\n",
      "317/507, train_loss: 0.0516\n",
      "318/507, train_loss: 0.3042\n",
      "319/507, train_loss: 0.1025\n",
      "320/507, train_loss: 0.3201\n",
      "321/507, train_loss: 0.3467\n",
      "322/507, train_loss: 0.3633\n",
      "323/507, train_loss: 0.1771\n",
      "324/507, train_loss: 0.2334\n",
      "325/507, train_loss: 0.6851\n",
      "326/507, train_loss: 0.2668\n",
      "327/507, train_loss: 0.1892\n",
      "328/507, train_loss: 0.0284\n",
      "329/507, train_loss: 0.2339\n",
      "330/507, train_loss: 0.5957\n",
      "331/507, train_loss: 0.1499\n",
      "332/507, train_loss: 0.6357\n",
      "333/507, train_loss: 0.2246\n",
      "334/507, train_loss: 0.2411\n",
      "335/507, train_loss: 0.3584\n",
      "336/507, train_loss: 0.6284\n",
      "337/507, train_loss: 0.2168\n",
      "338/507, train_loss: 0.3020\n",
      "339/507, train_loss: 0.3428\n",
      "340/507, train_loss: 0.2432\n",
      "341/507, train_loss: 0.2666\n",
      "342/507, train_loss: 0.1888\n",
      "343/507, train_loss: 0.1625\n",
      "344/507, train_loss: 0.0590\n",
      "345/507, train_loss: 0.1219\n",
      "346/507, train_loss: 0.1198\n",
      "347/507, train_loss: 0.0798\n",
      "348/507, train_loss: 0.1541\n",
      "349/507, train_loss: 0.2837\n",
      "350/507, train_loss: 0.0549\n",
      "351/507, train_loss: 0.0861\n",
      "352/507, train_loss: 0.1321\n",
      "353/507, train_loss: 0.1196\n",
      "354/507, train_loss: 0.1266\n",
      "355/507, train_loss: 0.0836\n",
      "356/507, train_loss: 0.0661\n"
     ]
    }
   ],
   "source": [
    "epoch_len = len_train_ds // train_loader.batch_size\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    mlflow.log_param(\"gt_box_mode\", gt_box_mode)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"patch_size\", patch_size)\n",
    "    mlflow.log_param(\"data_list_file_path\", data_list_file_path)\n",
    "    mlflow.log_param(\"data_base_dir\", data_base_dir)\n",
    "    mlflow.log_param(\"amp\", amp)\n",
    "    \n",
    "    mlflow.log_param(\"n_input_channels\", n_input_channels)\n",
    "    mlflow.log_param(\"spatial_dims\", spatial_dims)\n",
    "    mlflow.log_param(\"balanced_sampler_pos_fraction\", balanced_sampler_pos_fraction)\n",
    "    mlflow.log_param(\"score_thresh\", score_thresh)\n",
    "    mlflow.log_param(\"nms_thresh\", nms_thresh)\n",
    "    \n",
    "    mlflow.log_param(\"initial_lr\", lr)\n",
    "    mlflow.log_param(\"val_interval\", val_interval)\n",
    "    mlflow.log_param(\"max_epochs\", max_epochs)\n",
    "    mlflow.log_param(\"w_cls\", w_cls)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        detector.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_cls_loss = 0\n",
    "        epoch_box_reg_loss = 0\n",
    "        step = 0\n",
    "        scheduler_warmup.step()\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = [\n",
    "                batch_data_ii[\"image\"].to(device) for batch_data_i in batch_data for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "            targets = [\n",
    "                dict(\n",
    "                    label=batch_data_ii[\"label\"].to(device),\n",
    "                    box=batch_data_ii[\"box\"].to(device)\n",
    "                )\n",
    "                for batch_data_i in batch_data\n",
    "                for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "\n",
    "            for param in detector.network.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            if amp and (scaler is not None):\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = detector(inputs, targets)\n",
    "                    loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = detector(inputs, targets)\n",
    "                loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # saving into mlflow\n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_cls_loss += outputs[detector.cls_key].detach().item()\n",
    "            epoch_box_reg_loss += outputs[detector.box_reg_key].detach().item()\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            mlflow.log_metric(\"train_loss\", loss.detach().item(), epoch_len * epoch + step)\n",
    "\n",
    "        del inputs, batch_data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_cls_loss /= step\n",
    "        epoch_box_reg_loss /= step\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        mlflow.log_metric(\"avg_train_loss\", epoch_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_cls_loss\", epoch_cls_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_box_reg_loss\", epoch_box_reg_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"train_lr\", optimizer.param_groups[0][\"lr\"], epoch + 1)\n",
    "\n",
    "        # saving last trained model\n",
    "        mlflow.pytorch.log_model(detector.network, \"model\")\n",
    "\n",
    "        # validation for model selection\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            detector.eval()\n",
    "            val_outputs_all = []\n",
    "            val_targets_all = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    # if all val_data_i[\"image\"] smaller than val_patch_size, no need to use inferer\n",
    "                    # otherwise, need inferer to handle large input images.\n",
    "                    use_inferer = not all(\n",
    "                        [val_data_i[\"image\"][0, ...].numel() < np.prod(val_patch_size) for val_data_i in val_data]\n",
    "                    )\n",
    "                    val_inputs = [val_data_i.pop(\"image\").to(device) for val_data_i in val_data]\n",
    "\n",
    "                    if amp:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "                    else:\n",
    "                        val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "\n",
    "                    # save outputs for evaluation\n",
    "                    val_outputs_all += val_outputs\n",
    "                    val_targets_all += val_data\n",
    "\n",
    "            # visualize an inference image and boxes \n",
    "            draw_img = visualize_one_xy_slice_in_3d_image(\n",
    "                gt_boxes=val_data[0][\"box\"].cpu().detach().numpy(),\n",
    "                image=val_inputs[0][0, ...].cpu().detach().numpy(),\n",
    "                pred_boxes=val_outputs[0][detector.target_box_key].cpu().detach().numpy(),\n",
    "            )\n",
    "            # mlflow.log_image(draw_img.transpose([2, 1, 0]), \"val_img_xy.png\")\n",
    "            mlflow.log_image(draw_img, \"val_img_xy.png\")\n",
    "\n",
    "            # compute metrics\n",
    "            del val_inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            results_metric = matching_batch(\n",
    "                iou_fn=box_utils.box_iou,\n",
    "                iou_thresholds=coco_metric.iou_thresholds,\n",
    "                pred_boxes=[\n",
    "                    val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_scores=[\n",
    "                    val_data_i[detector.pred_score_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                gt_boxes=[val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_targets_all],\n",
    "                gt_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_targets_all\n",
    "                ]\n",
    "            )\n",
    "            val_epoch_metric_dict = coco_metric(results_metric)[0]\n",
    "            print(val_epoch_metric_dict)\n",
    "\n",
    "            # write metrics\n",
    "            for k in val_epoch_metric_dict.keys():\n",
    "                mlflow.log_metric(\"val_\" + k, val_epoch_metric_dict[k], epoch + 1)\n",
    "            val_epoch_metric = val_epoch_metric_dict.values()\n",
    "            val_epoch_metric = sum(val_epoch_metric) / len(val_epoch_metric)\n",
    "            mlflow.log_metric(\"val_metric\", val_epoch_metric, epoch + 1)\n",
    "\n",
    "            # save best trained model\n",
    "            if val_epoch_metric > best_val_epoch_metric:\n",
    "                best_val_epoch_metric = val_epoch_metric\n",
    "                best_val_epoch = epoch + 1\n",
    "                mlflow.pytorch.log_model(detector.network, \"best_model\")\n",
    "            print(\n",
    "                \"current epoch: {} current metric: {:.4f} \"\n",
    "                \"best metric: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, val_epoch_metric, best_val_epoch_metric, best_val_epoch\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(f\"train completed, best_metric: {best_val_epoch_metric:.4f} \" f\"at epoch: {best_val_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79dfad-c3c3-4e70-9f34-cee54b016a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
