{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6948feac-77f3-43ab-91e3-5e2a79c3dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'Eddy'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'Usp1#'\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = 'LUNA16 + HC + MSD - without lung seg'\n",
    "description = \"Mixed dataset training without lung segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8931abf-4b0b-4731-adf9-7c5b2840c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import gc\n",
    "\n",
    "from visualize_image import visualize_one_xy_slice_in_3d_image\n",
    "from loading_dataset import load_data\n",
    "from model import load_model\n",
    "import numpy as np\n",
    "from monai.data import box_utils\n",
    "from monai.apps.detection.metrics.matching import matching_batch\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from monai.apps.detection.metrics.coco import COCOMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2c25b3-b7bc-4abc-856a-f2a4d88259bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box_mode = 'cccwhd'\n",
    "batch_size = 10\n",
    "patch_size = [96,96,40]\n",
    "data_list_file_path = '/data/output/mixed_data/mixed_train_val0.json'\n",
    "data_base_dir = ''\n",
    "# data_base_dir = '/data/HC_Images_resample/'\n",
    "# data_base_dir = '/data/MSD_Images_resample/'\n",
    "# data_base_dir = '/data/LUNA16_Images_resample/'\n",
    "amp=True\n",
    "\n",
    "returned_layers = [1,2]\n",
    "base_anchor_shapes = [[6,8,4],[8,6,5],[10,10,6]]\n",
    "conv1_t_stride = [2,2,1]\n",
    "n_input_channels = 1\n",
    "spatial_dims = 3\n",
    "fg_labels = [0]\n",
    "verbose = False\n",
    "balanced_sampler_pos_fraction = 0.3\n",
    "score_thresh = 0.02\n",
    "nms_thresh = 0.22\n",
    "val_patch_size = [256,256,104]\n",
    "\n",
    "lr = 1e-2\n",
    "val_interval = 5\n",
    "coco_metric = COCOMetric(classes=[\"nodule\"], iou_list=[0.1], max_detection=[100])\n",
    "best_val_epoch_metric = 0.0\n",
    "best_val_epoch = -1\n",
    "max_epochs = 100\n",
    "w_cls = 1.0\n",
    "\n",
    "compute_dtype = torch.float32\n",
    "if amp:\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff675dc4-c1a7-4f49-8c72-938dca13e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.2.0\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 2.0.1+cu118\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: c33f1ba588ee00229a309000e888f9817b4f1934\n",
      "MONAI __file__: /usr/local/lib/python3.8/dist-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: 0.21.0\n",
      "Pillow version: 10.0.1\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.2+cu118\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.6\n",
      "pandas version: 2.0.3\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.7.1\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "\n",
    "monai.config.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ad2cc-31a4-4f4d-a131-256b6cfd9e61",
   "metadata": {},
   "source": [
    "### loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c9bf6dd-f147-4974-8b8b-54204a268866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, len_train_ds = load_data(\n",
    "    gt_box_mode, patch_size, batch_size, amp, data_list_file_path, data_base_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e514-7ab6-45dd-bd31-2f7bf68ff2d6",
   "metadata": {},
   "source": [
    "### loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bf21214-041f-401e-a2f7-f0072929bf90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector, device = load_model(\n",
    "    returned_layers, base_anchor_shapes, conv1_t_stride, n_input_channels,\n",
    "    spatial_dims, fg_labels, verbose, balanced_sampler_pos_fraction,\n",
    "    score_thresh, nms_thresh, val_patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7938680-00c0-4d79-bb31-4320bac9e332",
   "metadata": {},
   "source": [
    "### Initialize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7489f0c5-5ffc-4005-994d-8d74c85cae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    detector.network.parameters(),\n",
    "    lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=3e-5,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "after_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=150, gamma=0.1\n",
    ")\n",
    "scheduler_warmup = GradualWarmupScheduler(\n",
    "    optimizer, multiplier=1, total_epoch=10, after_scheduler=after_scheduler\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler() if amp else None\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ca37e-e17d-4e84-b9bc-4029ba3dcaf2",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5beae8c-e323-4dd2-84b6-eeebf82bf10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n",
      "1/231, train_loss: 1.0234\n",
      "2/231, train_loss: 1.1934\n",
      "3/231, train_loss: 1.4141\n",
      "4/231, train_loss: 0.9980\n",
      "5/231, train_loss: 1.1514\n",
      "6/231, train_loss: 1.1514\n",
      "7/231, train_loss: 1.1699\n",
      "8/231, train_loss: 1.2549\n",
      "9/231, train_loss: 0.8560\n",
      "10/231, train_loss: 0.9814\n",
      "11/231, train_loss: 1.0449\n",
      "12/231, train_loss: 0.9199\n",
      "13/231, train_loss: 1.0371\n",
      "14/231, train_loss: 0.9590\n",
      "15/231, train_loss: 0.8281\n",
      "16/231, train_loss: 0.7842\n",
      "17/231, train_loss: 0.8418\n",
      "18/231, train_loss: 0.7690\n",
      "19/231, train_loss: 0.7285\n",
      "20/231, train_loss: 0.7588\n",
      "21/231, train_loss: 0.6987\n",
      "22/231, train_loss: 0.6094\n",
      "23/231, train_loss: 0.9180\n",
      "24/231, train_loss: 0.8354\n",
      "25/231, train_loss: 1.0674\n",
      "26/231, train_loss: 0.8232\n",
      "27/231, train_loss: 0.7646\n",
      "28/231, train_loss: 0.6836\n",
      "29/231, train_loss: 0.7461\n",
      "30/231, train_loss: 0.7505\n",
      "31/231, train_loss: 0.7363\n",
      "32/231, train_loss: 0.7544\n",
      "33/231, train_loss: 0.8721\n",
      "34/231, train_loss: 0.8130\n",
      "35/231, train_loss: 0.8198\n",
      "36/231, train_loss: 0.7759\n",
      "37/231, train_loss: 0.7061\n",
      "38/231, train_loss: 0.8623\n",
      "39/231, train_loss: 0.6348\n",
      "40/231, train_loss: 0.7754\n",
      "41/231, train_loss: 0.7510\n",
      "42/231, train_loss: 0.7305\n",
      "43/231, train_loss: 0.6392\n",
      "44/231, train_loss: 0.6489\n",
      "45/231, train_loss: 0.6240\n",
      "46/231, train_loss: 0.5205\n",
      "47/231, train_loss: 0.7236\n",
      "48/231, train_loss: 0.7754\n",
      "49/231, train_loss: 0.7261\n",
      "50/231, train_loss: 0.5928\n",
      "51/231, train_loss: 0.7002\n",
      "52/231, train_loss: 0.5488\n",
      "53/231, train_loss: 0.8174\n",
      "54/231, train_loss: 0.6040\n",
      "55/231, train_loss: 0.6104\n",
      "56/231, train_loss: 0.7856\n",
      "57/231, train_loss: 0.6748\n",
      "58/231, train_loss: 0.6631\n",
      "59/231, train_loss: 0.7114\n",
      "60/231, train_loss: 0.5049\n",
      "61/231, train_loss: 0.9209\n",
      "62/231, train_loss: 0.7139\n",
      "63/231, train_loss: 0.7397\n",
      "64/231, train_loss: 0.6973\n",
      "65/231, train_loss: 0.6602\n",
      "66/231, train_loss: 0.4756\n",
      "67/231, train_loss: 0.8369\n",
      "68/231, train_loss: 0.7500\n",
      "69/231, train_loss: 0.6846\n",
      "70/231, train_loss: 0.4746\n",
      "71/231, train_loss: 0.7593\n",
      "72/231, train_loss: 0.8164\n",
      "73/231, train_loss: 0.6733\n",
      "74/231, train_loss: 0.7871\n",
      "75/231, train_loss: 0.6631\n",
      "76/231, train_loss: 0.7578\n",
      "77/231, train_loss: 0.6641\n",
      "78/231, train_loss: 0.7832\n",
      "79/231, train_loss: 0.6631\n",
      "80/231, train_loss: 0.5527\n",
      "81/231, train_loss: 0.7422\n",
      "82/231, train_loss: 0.4761\n",
      "83/231, train_loss: 0.8135\n",
      "84/231, train_loss: 0.6875\n",
      "85/231, train_loss: 0.6836\n",
      "86/231, train_loss: 0.6816\n",
      "87/231, train_loss: 0.7227\n",
      "88/231, train_loss: 0.6426\n",
      "89/231, train_loss: 0.5527\n",
      "90/231, train_loss: 0.6602\n",
      "91/231, train_loss: 0.5806\n",
      "92/231, train_loss: 0.6733\n",
      "93/231, train_loss: 0.7603\n",
      "94/231, train_loss: 0.7031\n",
      "95/231, train_loss: 0.6396\n",
      "96/231, train_loss: 0.5244\n",
      "97/231, train_loss: 0.5581\n",
      "98/231, train_loss: 0.5962\n",
      "99/231, train_loss: 0.5029\n",
      "100/231, train_loss: 0.6411\n",
      "101/231, train_loss: 0.6260\n",
      "102/231, train_loss: 0.5801\n",
      "103/231, train_loss: 0.6025\n",
      "104/231, train_loss: 0.5879\n",
      "105/231, train_loss: 0.6934\n",
      "106/231, train_loss: 0.5601\n",
      "107/231, train_loss: 0.6836\n",
      "108/231, train_loss: 0.7031\n",
      "109/231, train_loss: 0.7461\n",
      "110/231, train_loss: 0.6504\n",
      "111/231, train_loss: 0.6929\n",
      "112/231, train_loss: 0.7080\n",
      "113/231, train_loss: 0.6250\n",
      "114/231, train_loss: 0.6655\n",
      "115/231, train_loss: 0.5908\n",
      "116/231, train_loss: 0.6973\n",
      "117/231, train_loss: 0.4468\n",
      "118/231, train_loss: 0.5527\n",
      "119/231, train_loss: 0.5620\n",
      "120/231, train_loss: 0.7744\n",
      "121/231, train_loss: 0.7217\n",
      "122/231, train_loss: 0.7173\n",
      "123/231, train_loss: 0.5342\n",
      "124/231, train_loss: 0.4695\n",
      "125/231, train_loss: 0.7183\n",
      "126/231, train_loss: 0.7383\n",
      "127/231, train_loss: 0.6387\n",
      "128/231, train_loss: 0.6562\n",
      "129/231, train_loss: 0.7461\n",
      "130/231, train_loss: 0.7720\n",
      "131/231, train_loss: 0.7002\n",
      "132/231, train_loss: 0.4976\n",
      "133/231, train_loss: 0.5708\n",
      "134/231, train_loss: 0.5698\n",
      "135/231, train_loss: 0.6011\n",
      "136/231, train_loss: 0.7036\n",
      "137/231, train_loss: 0.5425\n",
      "138/231, train_loss: 0.5469\n",
      "139/231, train_loss: 0.5742\n",
      "140/231, train_loss: 0.7305\n",
      "141/231, train_loss: 0.5522\n",
      "142/231, train_loss: 0.5664\n",
      "143/231, train_loss: 0.5449\n",
      "144/231, train_loss: 0.7598\n",
      "145/231, train_loss: 0.5879\n",
      "146/231, train_loss: 0.6323\n",
      "147/231, train_loss: 0.6973\n",
      "148/231, train_loss: 0.6055\n",
      "149/231, train_loss: 0.6802\n",
      "150/231, train_loss: 0.6460\n",
      "151/231, train_loss: 0.6445\n",
      "152/231, train_loss: 0.6621\n",
      "153/231, train_loss: 0.7402\n",
      "154/231, train_loss: 0.6230\n",
      "155/231, train_loss: 0.5396\n",
      "156/231, train_loss: 0.6035\n",
      "157/231, train_loss: 0.5923\n",
      "158/231, train_loss: 0.5005\n",
      "159/231, train_loss: 0.6284\n",
      "160/231, train_loss: 0.5820\n",
      "161/231, train_loss: 0.5811\n",
      "162/231, train_loss: 0.6943\n",
      "163/231, train_loss: 0.5938\n",
      "164/231, train_loss: 0.6475\n",
      "165/231, train_loss: 0.5874\n",
      "166/231, train_loss: 0.5415\n",
      "167/231, train_loss: 0.6489\n",
      "168/231, train_loss: 0.7061\n",
      "169/231, train_loss: 0.5830\n",
      "170/231, train_loss: 0.6855\n",
      "171/231, train_loss: 0.4651\n",
      "172/231, train_loss: 0.6162\n",
      "173/231, train_loss: 0.7290\n",
      "174/231, train_loss: 0.6108\n",
      "175/231, train_loss: 0.5947\n",
      "176/231, train_loss: 0.6025\n",
      "177/231, train_loss: 0.6650\n",
      "178/231, train_loss: 0.6445\n",
      "179/231, train_loss: 0.7026\n",
      "180/231, train_loss: 0.5410\n",
      "181/231, train_loss: 0.7256\n",
      "182/231, train_loss: 0.4844\n",
      "183/231, train_loss: 0.6299\n",
      "184/231, train_loss: 0.5850\n",
      "185/231, train_loss: 0.5430\n",
      "186/231, train_loss: 0.6685\n",
      "187/231, train_loss: 0.5327\n",
      "188/231, train_loss: 0.6670\n",
      "189/231, train_loss: 0.5435\n",
      "190/231, train_loss: 0.5308\n",
      "191/231, train_loss: 0.6416\n",
      "192/231, train_loss: 0.7017\n",
      "193/231, train_loss: 0.6357\n",
      "194/231, train_loss: 0.4573\n",
      "195/231, train_loss: 0.5869\n",
      "196/231, train_loss: 0.5464\n",
      "197/231, train_loss: 0.5767\n",
      "198/231, train_loss: 0.5396\n",
      "199/231, train_loss: 0.6582\n",
      "200/231, train_loss: 0.5635\n",
      "201/231, train_loss: 0.7246\n",
      "202/231, train_loss: 0.7534\n",
      "203/231, train_loss: 0.5664\n",
      "204/231, train_loss: 0.6006\n",
      "205/231, train_loss: 0.6382\n",
      "206/231, train_loss: 0.5835\n",
      "207/231, train_loss: 0.6094\n",
      "208/231, train_loss: 0.6997\n",
      "209/231, train_loss: 0.6733\n",
      "210/231, train_loss: 0.6367\n",
      "211/231, train_loss: 0.6543\n",
      "212/231, train_loss: 0.5713\n",
      "213/231, train_loss: 0.6660\n",
      "214/231, train_loss: 0.5928\n",
      "215/231, train_loss: 0.6431\n",
      "216/231, train_loss: 0.5205\n",
      "217/231, train_loss: 0.6880\n",
      "218/231, train_loss: 0.5103\n",
      "219/231, train_loss: 0.7446\n",
      "220/231, train_loss: 0.6401\n",
      "221/231, train_loss: 0.6904\n",
      "222/231, train_loss: 0.5029\n",
      "223/231, train_loss: 0.5000\n",
      "224/231, train_loss: 0.6250\n",
      "225/231, train_loss: 0.6714\n",
      "226/231, train_loss: 0.5884\n",
      "227/231, train_loss: 0.5400\n",
      "228/231, train_loss: 0.5479\n",
      "229/231, train_loss: 0.6030\n",
      "230/231, train_loss: 0.6040\n",
      "231/231, train_loss: 0.7373\n",
      "232/231, train_loss: 0.4241\n",
      "epoch 1 average loss: 0.6796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 14:48:45 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 14:48:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 2/100\n",
      "1/231, train_loss: 0.6489\n",
      "2/231, train_loss: 0.5762\n",
      "3/231, train_loss: 0.7451\n",
      "4/231, train_loss: 0.5869\n",
      "5/231, train_loss: 0.6460\n",
      "6/231, train_loss: 0.6484\n",
      "7/231, train_loss: 0.6001\n",
      "8/231, train_loss: 0.7480\n",
      "9/231, train_loss: 0.5889\n",
      "10/231, train_loss: 0.7949\n",
      "11/231, train_loss: 0.6074\n",
      "12/231, train_loss: 0.6001\n",
      "13/231, train_loss: 0.4810\n",
      "14/231, train_loss: 0.5737\n",
      "15/231, train_loss: 0.6714\n",
      "16/231, train_loss: 0.6133\n",
      "17/231, train_loss: 0.5889\n",
      "18/231, train_loss: 0.7051\n",
      "19/231, train_loss: 0.7466\n",
      "20/231, train_loss: 0.6475\n",
      "21/231, train_loss: 0.5928\n",
      "22/231, train_loss: 0.6304\n",
      "23/231, train_loss: 0.8398\n",
      "24/231, train_loss: 0.6826\n",
      "25/231, train_loss: 0.6572\n",
      "26/231, train_loss: 0.5918\n",
      "27/231, train_loss: 0.5610\n",
      "28/231, train_loss: 0.7119\n",
      "29/231, train_loss: 0.7134\n",
      "30/231, train_loss: 0.5684\n",
      "31/231, train_loss: 0.7549\n",
      "32/231, train_loss: 0.6426\n",
      "33/231, train_loss: 0.6250\n",
      "34/231, train_loss: 0.6631\n",
      "35/231, train_loss: 0.7305\n",
      "36/231, train_loss: 0.6318\n",
      "37/231, train_loss: 0.5352\n",
      "38/231, train_loss: 0.5371\n",
      "39/231, train_loss: 0.6997\n",
      "40/231, train_loss: 0.5981\n",
      "41/231, train_loss: 0.6528\n",
      "42/231, train_loss: 0.6709\n",
      "43/231, train_loss: 0.5337\n",
      "44/231, train_loss: 0.5835\n",
      "45/231, train_loss: 0.7139\n",
      "46/231, train_loss: 0.6797\n",
      "47/231, train_loss: 0.5376\n",
      "48/231, train_loss: 0.6685\n",
      "49/231, train_loss: 0.5439\n",
      "50/231, train_loss: 0.5649\n",
      "51/231, train_loss: 0.7061\n",
      "52/231, train_loss: 0.6113\n",
      "53/231, train_loss: 0.4446\n",
      "54/231, train_loss: 0.5337\n",
      "55/231, train_loss: 0.6855\n",
      "56/231, train_loss: 0.6675\n",
      "57/231, train_loss: 0.6860\n",
      "58/231, train_loss: 0.6162\n",
      "59/231, train_loss: 0.5142\n",
      "60/231, train_loss: 0.5996\n",
      "61/231, train_loss: 0.5098\n",
      "62/231, train_loss: 0.5093\n",
      "63/231, train_loss: 0.5386\n",
      "64/231, train_loss: 0.4553\n",
      "65/231, train_loss: 0.6719\n",
      "66/231, train_loss: 0.6475\n",
      "67/231, train_loss: 0.6309\n",
      "68/231, train_loss: 0.6079\n",
      "69/231, train_loss: 0.6265\n",
      "70/231, train_loss: 0.5391\n",
      "71/231, train_loss: 0.4995\n",
      "72/231, train_loss: 0.6421\n",
      "73/231, train_loss: 0.6855\n",
      "74/231, train_loss: 0.7188\n",
      "75/231, train_loss: 0.6172\n",
      "76/231, train_loss: 0.7109\n",
      "77/231, train_loss: 0.5049\n",
      "78/231, train_loss: 0.5312\n",
      "79/231, train_loss: 0.6187\n",
      "80/231, train_loss: 0.5269\n",
      "81/231, train_loss: 0.6318\n",
      "82/231, train_loss: 0.5229\n",
      "83/231, train_loss: 0.6392\n",
      "84/231, train_loss: 0.5093\n",
      "85/231, train_loss: 0.5674\n",
      "86/231, train_loss: 0.5439\n",
      "87/231, train_loss: 0.4575\n",
      "88/231, train_loss: 0.6475\n",
      "89/231, train_loss: 0.7148\n",
      "90/231, train_loss: 0.4648\n",
      "91/231, train_loss: 0.5278\n",
      "92/231, train_loss: 0.6699\n",
      "93/231, train_loss: 0.5801\n",
      "94/231, train_loss: 0.6357\n",
      "95/231, train_loss: 0.5176\n",
      "96/231, train_loss: 0.6768\n",
      "97/231, train_loss: 0.5518\n",
      "98/231, train_loss: 0.6367\n",
      "99/231, train_loss: 0.5278\n",
      "100/231, train_loss: 0.4277\n",
      "101/231, train_loss: 0.6802\n",
      "102/231, train_loss: 0.6431\n",
      "103/231, train_loss: 0.4854\n",
      "104/231, train_loss: 0.6978\n",
      "105/231, train_loss: 0.5469\n",
      "106/231, train_loss: 0.6396\n",
      "107/231, train_loss: 0.6235\n",
      "108/231, train_loss: 0.5947\n",
      "109/231, train_loss: 0.5498\n",
      "110/231, train_loss: 0.6826\n",
      "111/231, train_loss: 0.6006\n",
      "112/231, train_loss: 0.6016\n",
      "113/231, train_loss: 0.6055\n",
      "114/231, train_loss: 0.5645\n",
      "115/231, train_loss: 0.4912\n",
      "116/231, train_loss: 0.6802\n",
      "117/231, train_loss: 0.4795\n",
      "118/231, train_loss: 0.7095\n",
      "119/231, train_loss: 0.5640\n",
      "120/231, train_loss: 0.6753\n",
      "121/231, train_loss: 0.5693\n",
      "122/231, train_loss: 0.6489\n",
      "123/231, train_loss: 0.6465\n",
      "124/231, train_loss: 0.5938\n",
      "125/231, train_loss: 0.7129\n",
      "126/231, train_loss: 0.5107\n",
      "127/231, train_loss: 0.6582\n",
      "128/231, train_loss: 0.6074\n",
      "129/231, train_loss: 0.6616\n",
      "130/231, train_loss: 0.6055\n",
      "131/231, train_loss: 0.5430\n",
      "132/231, train_loss: 0.5601\n",
      "133/231, train_loss: 0.5508\n",
      "134/231, train_loss: 0.5146\n",
      "135/231, train_loss: 0.5156\n",
      "136/231, train_loss: 0.5205\n",
      "137/231, train_loss: 0.5479\n",
      "138/231, train_loss: 0.4556\n",
      "139/231, train_loss: 0.7202\n",
      "140/231, train_loss: 0.4790\n",
      "141/231, train_loss: 0.5356\n",
      "142/231, train_loss: 0.6758\n",
      "143/231, train_loss: 0.6670\n",
      "144/231, train_loss: 0.4624\n",
      "145/231, train_loss: 0.4824\n",
      "146/231, train_loss: 0.6416\n",
      "147/231, train_loss: 0.4614\n",
      "148/231, train_loss: 0.5498\n",
      "149/231, train_loss: 0.7549\n",
      "150/231, train_loss: 0.3896\n",
      "151/231, train_loss: 0.6387\n",
      "152/231, train_loss: 0.6567\n",
      "153/231, train_loss: 0.5322\n",
      "154/231, train_loss: 0.3511\n",
      "155/231, train_loss: 0.6899\n",
      "156/231, train_loss: 0.5249\n",
      "157/231, train_loss: 0.5400\n",
      "158/231, train_loss: 0.3794\n",
      "159/231, train_loss: 0.7285\n",
      "160/231, train_loss: 0.5312\n",
      "161/231, train_loss: 0.5605\n",
      "162/231, train_loss: 0.4668\n",
      "163/231, train_loss: 0.3896\n",
      "164/231, train_loss: 0.5190\n",
      "165/231, train_loss: 0.5737\n",
      "166/231, train_loss: 0.5386\n",
      "167/231, train_loss: 0.6230\n",
      "168/231, train_loss: 0.5225\n",
      "169/231, train_loss: 0.4526\n",
      "170/231, train_loss: 0.5269\n",
      "171/231, train_loss: 0.5625\n",
      "172/231, train_loss: 0.7500\n",
      "173/231, train_loss: 0.5547\n",
      "174/231, train_loss: 0.5449\n",
      "175/231, train_loss: 0.6177\n",
      "176/231, train_loss: 0.6016\n",
      "177/231, train_loss: 0.5308\n",
      "178/231, train_loss: 0.4646\n",
      "179/231, train_loss: 0.6055\n",
      "180/231, train_loss: 0.4741\n",
      "181/231, train_loss: 0.5308\n",
      "182/231, train_loss: 0.5908\n",
      "183/231, train_loss: 0.7178\n",
      "184/231, train_loss: 0.4954\n",
      "185/231, train_loss: 0.5342\n",
      "186/231, train_loss: 0.5098\n",
      "187/231, train_loss: 0.4355\n",
      "188/231, train_loss: 0.5757\n",
      "189/231, train_loss: 0.5181\n",
      "190/231, train_loss: 0.5830\n",
      "191/231, train_loss: 0.7065\n",
      "192/231, train_loss: 0.4355\n",
      "193/231, train_loss: 0.3755\n",
      "194/231, train_loss: 0.3633\n",
      "195/231, train_loss: 0.4839\n",
      "196/231, train_loss: 0.5386\n",
      "197/231, train_loss: 0.4626\n",
      "198/231, train_loss: 0.5571\n",
      "199/231, train_loss: 0.4846\n",
      "200/231, train_loss: 0.5005\n",
      "201/231, train_loss: 0.5381\n",
      "202/231, train_loss: 0.5381\n",
      "203/231, train_loss: 0.4502\n",
      "204/231, train_loss: 0.4219\n",
      "205/231, train_loss: 0.5161\n",
      "206/231, train_loss: 0.3418\n",
      "207/231, train_loss: 0.4880\n",
      "208/231, train_loss: 0.5776\n",
      "209/231, train_loss: 0.7256\n",
      "210/231, train_loss: 0.3447\n",
      "211/231, train_loss: 0.5806\n",
      "212/231, train_loss: 0.4209\n",
      "213/231, train_loss: 0.5781\n",
      "214/231, train_loss: 0.6919\n",
      "215/231, train_loss: 0.5020\n",
      "216/231, train_loss: 0.5933\n",
      "217/231, train_loss: 0.5225\n",
      "218/231, train_loss: 0.4109\n",
      "219/231, train_loss: 0.5049\n",
      "220/231, train_loss: 0.4797\n",
      "221/231, train_loss: 0.4841\n",
      "222/231, train_loss: 0.3352\n",
      "223/231, train_loss: 0.4084\n",
      "224/231, train_loss: 0.4541\n",
      "225/231, train_loss: 0.4136\n",
      "226/231, train_loss: 0.5161\n",
      "227/231, train_loss: 0.3350\n",
      "228/231, train_loss: 0.3872\n",
      "229/231, train_loss: 0.3660\n",
      "230/231, train_loss: 0.4260\n",
      "231/231, train_loss: 0.4014\n",
      "232/231, train_loss: 0.5693\n",
      "epoch 2 average loss: 0.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 15:30:59 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 15:31:02 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 3/100\n",
      "1/231, train_loss: 0.5259\n",
      "2/231, train_loss: 0.3848\n",
      "3/231, train_loss: 0.5244\n",
      "4/231, train_loss: 0.6265\n",
      "5/231, train_loss: 0.5078\n",
      "6/231, train_loss: 0.3877\n",
      "7/231, train_loss: 0.3584\n",
      "8/231, train_loss: 0.6592\n",
      "9/231, train_loss: 0.7256\n",
      "10/231, train_loss: 0.5498\n",
      "11/231, train_loss: 0.3567\n",
      "12/231, train_loss: 0.4817\n",
      "13/231, train_loss: 0.4854\n",
      "14/231, train_loss: 0.5039\n",
      "15/231, train_loss: 0.6440\n",
      "16/231, train_loss: 0.7632\n",
      "17/231, train_loss: 0.4983\n",
      "18/231, train_loss: 0.4780\n",
      "19/231, train_loss: 0.5391\n",
      "20/231, train_loss: 0.6211\n",
      "21/231, train_loss: 0.4951\n",
      "22/231, train_loss: 0.5688\n",
      "23/231, train_loss: 0.5513\n",
      "24/231, train_loss: 0.5220\n",
      "25/231, train_loss: 0.3477\n",
      "26/231, train_loss: 0.6201\n",
      "27/231, train_loss: 0.4888\n",
      "28/231, train_loss: 0.5693\n",
      "29/231, train_loss: 0.6309\n",
      "30/231, train_loss: 0.5425\n",
      "31/231, train_loss: 0.5259\n",
      "32/231, train_loss: 0.5674\n",
      "33/231, train_loss: 0.6006\n",
      "34/231, train_loss: 0.4761\n",
      "35/231, train_loss: 0.4092\n",
      "36/231, train_loss: 0.3857\n",
      "37/231, train_loss: 0.3647\n",
      "38/231, train_loss: 0.5078\n",
      "39/231, train_loss: 0.5488\n",
      "40/231, train_loss: 0.3257\n",
      "41/231, train_loss: 0.5210\n",
      "42/231, train_loss: 0.6538\n",
      "43/231, train_loss: 0.5059\n",
      "44/231, train_loss: 0.4751\n",
      "45/231, train_loss: 0.5229\n",
      "46/231, train_loss: 0.4370\n",
      "47/231, train_loss: 0.5308\n",
      "48/231, train_loss: 0.4182\n",
      "49/231, train_loss: 0.3486\n",
      "50/231, train_loss: 0.4424\n",
      "51/231, train_loss: 0.4141\n",
      "52/231, train_loss: 0.5244\n",
      "53/231, train_loss: 0.6348\n",
      "54/231, train_loss: 0.4324\n",
      "55/231, train_loss: 0.6172\n",
      "56/231, train_loss: 0.3821\n",
      "57/231, train_loss: 0.4539\n",
      "58/231, train_loss: 0.3691\n",
      "59/231, train_loss: 0.3787\n",
      "60/231, train_loss: 0.5259\n",
      "61/231, train_loss: 0.5361\n",
      "62/231, train_loss: 0.4219\n",
      "63/231, train_loss: 0.4751\n",
      "64/231, train_loss: 0.3789\n",
      "65/231, train_loss: 0.3438\n",
      "66/231, train_loss: 0.4529\n",
      "67/231, train_loss: 0.3921\n",
      "68/231, train_loss: 0.3687\n",
      "69/231, train_loss: 0.6016\n",
      "70/231, train_loss: 0.3420\n",
      "71/231, train_loss: 0.5605\n",
      "72/231, train_loss: 0.3057\n",
      "73/231, train_loss: 0.6016\n",
      "74/231, train_loss: 0.4924\n",
      "75/231, train_loss: 0.3462\n",
      "76/231, train_loss: 0.5728\n",
      "77/231, train_loss: 0.3777\n",
      "78/231, train_loss: 0.5498\n",
      "79/231, train_loss: 0.5898\n",
      "80/231, train_loss: 0.4050\n",
      "81/231, train_loss: 0.3489\n",
      "82/231, train_loss: 0.4062\n",
      "83/231, train_loss: 0.4126\n",
      "84/231, train_loss: 0.4189\n",
      "85/231, train_loss: 0.3718\n",
      "86/231, train_loss: 0.7866\n",
      "87/231, train_loss: 0.3892\n",
      "88/231, train_loss: 0.5654\n",
      "89/231, train_loss: 0.4048\n",
      "90/231, train_loss: 0.8223\n",
      "91/231, train_loss: 0.6738\n",
      "92/231, train_loss: 0.4128\n",
      "93/231, train_loss: 0.5439\n",
      "94/231, train_loss: 0.3955\n",
      "95/231, train_loss: 0.5156\n",
      "96/231, train_loss: 0.3787\n",
      "97/231, train_loss: 0.3979\n",
      "98/231, train_loss: 0.3252\n",
      "99/231, train_loss: 0.2253\n",
      "100/231, train_loss: 0.5283\n",
      "101/231, train_loss: 0.5444\n",
      "102/231, train_loss: 0.5723\n",
      "103/231, train_loss: 0.6294\n",
      "104/231, train_loss: 0.3379\n",
      "105/231, train_loss: 0.3301\n",
      "106/231, train_loss: 0.4297\n",
      "107/231, train_loss: 0.3616\n",
      "108/231, train_loss: 0.3145\n",
      "109/231, train_loss: 0.6162\n",
      "110/231, train_loss: 0.4824\n",
      "111/231, train_loss: 0.5400\n",
      "112/231, train_loss: 0.3154\n",
      "113/231, train_loss: 0.2961\n",
      "114/231, train_loss: 0.4502\n",
      "115/231, train_loss: 0.3699\n",
      "116/231, train_loss: 0.2496\n",
      "117/231, train_loss: 0.2915\n",
      "118/231, train_loss: 0.7842\n",
      "119/231, train_loss: 0.6685\n",
      "120/231, train_loss: 0.6489\n",
      "121/231, train_loss: 0.4573\n",
      "122/231, train_loss: 0.3323\n",
      "123/231, train_loss: 0.2842\n",
      "124/231, train_loss: 0.4004\n",
      "125/231, train_loss: 0.4368\n",
      "126/231, train_loss: 0.6504\n",
      "127/231, train_loss: 0.4448\n",
      "128/231, train_loss: 0.3647\n",
      "129/231, train_loss: 0.5229\n",
      "130/231, train_loss: 0.4795\n",
      "131/231, train_loss: 0.5752\n",
      "132/231, train_loss: 0.4636\n",
      "133/231, train_loss: 0.3950\n",
      "134/231, train_loss: 0.6001\n",
      "135/231, train_loss: 0.4985\n",
      "136/231, train_loss: 0.4966\n",
      "137/231, train_loss: 0.5518\n",
      "138/231, train_loss: 0.4326\n",
      "139/231, train_loss: 0.5151\n",
      "140/231, train_loss: 0.4988\n",
      "141/231, train_loss: 0.2310\n",
      "142/231, train_loss: 0.6045\n",
      "143/231, train_loss: 0.5693\n",
      "144/231, train_loss: 0.3481\n",
      "145/231, train_loss: 0.7041\n",
      "146/231, train_loss: 0.4536\n",
      "147/231, train_loss: 0.2808\n",
      "148/231, train_loss: 0.4727\n",
      "149/231, train_loss: 0.4419\n",
      "150/231, train_loss: 0.5205\n",
      "151/231, train_loss: 0.4282\n",
      "152/231, train_loss: 0.4917\n",
      "153/231, train_loss: 0.3845\n",
      "154/231, train_loss: 0.3088\n",
      "155/231, train_loss: 0.2644\n",
      "156/231, train_loss: 0.3428\n",
      "157/231, train_loss: 0.6831\n",
      "158/231, train_loss: 0.3904\n",
      "159/231, train_loss: 0.6958\n",
      "160/231, train_loss: 0.3350\n",
      "161/231, train_loss: 0.5127\n",
      "162/231, train_loss: 0.4458\n",
      "163/231, train_loss: 0.2908\n",
      "164/231, train_loss: 0.4023\n",
      "165/231, train_loss: 0.3223\n",
      "166/231, train_loss: 0.5503\n",
      "167/231, train_loss: 0.5879\n",
      "168/231, train_loss: 0.4131\n",
      "169/231, train_loss: 0.3540\n",
      "170/231, train_loss: 0.4043\n",
      "171/231, train_loss: 0.4624\n",
      "172/231, train_loss: 0.5518\n",
      "173/231, train_loss: 0.3350\n",
      "174/231, train_loss: 0.3350\n",
      "175/231, train_loss: 0.5044\n",
      "176/231, train_loss: 0.3376\n",
      "177/231, train_loss: 0.3149\n",
      "178/231, train_loss: 0.5610\n",
      "179/231, train_loss: 0.5513\n",
      "180/231, train_loss: 0.4521\n",
      "181/231, train_loss: 0.4927\n",
      "182/231, train_loss: 0.4419\n",
      "183/231, train_loss: 0.5781\n",
      "184/231, train_loss: 0.5464\n",
      "185/231, train_loss: 0.4316\n",
      "186/231, train_loss: 0.5776\n",
      "187/231, train_loss: 0.3511\n",
      "188/231, train_loss: 0.3958\n",
      "189/231, train_loss: 0.2993\n",
      "190/231, train_loss: 0.4514\n",
      "191/231, train_loss: 0.3767\n",
      "192/231, train_loss: 0.2551\n",
      "193/231, train_loss: 0.3342\n",
      "194/231, train_loss: 0.4431\n",
      "195/231, train_loss: 0.5786\n",
      "196/231, train_loss: 0.3447\n",
      "197/231, train_loss: 0.4819\n",
      "198/231, train_loss: 0.5527\n",
      "199/231, train_loss: 0.3279\n",
      "200/231, train_loss: 0.3818\n",
      "201/231, train_loss: 0.6738\n",
      "202/231, train_loss: 0.5991\n",
      "203/231, train_loss: 0.2327\n",
      "204/231, train_loss: 0.3916\n",
      "205/231, train_loss: 0.8135\n",
      "206/231, train_loss: 0.3677\n",
      "207/231, train_loss: 0.3359\n",
      "208/231, train_loss: 0.3542\n",
      "209/231, train_loss: 0.4648\n",
      "210/231, train_loss: 0.3262\n",
      "211/231, train_loss: 0.4751\n",
      "212/231, train_loss: 0.2866\n",
      "213/231, train_loss: 0.6074\n",
      "214/231, train_loss: 0.3701\n",
      "215/231, train_loss: 0.4609\n",
      "216/231, train_loss: 0.4238\n",
      "217/231, train_loss: 0.3699\n",
      "218/231, train_loss: 0.3794\n",
      "219/231, train_loss: 0.2502\n",
      "220/231, train_loss: 0.5620\n",
      "221/231, train_loss: 0.3904\n",
      "222/231, train_loss: 0.3367\n",
      "223/231, train_loss: 0.3293\n",
      "224/231, train_loss: 0.4126\n",
      "225/231, train_loss: 0.4028\n",
      "226/231, train_loss: 0.2837\n",
      "227/231, train_loss: 0.5229\n",
      "228/231, train_loss: 0.2861\n",
      "229/231, train_loss: 0.5918\n",
      "230/231, train_loss: 0.3755\n",
      "231/231, train_loss: 0.4272\n",
      "232/231, train_loss: 0.3213\n",
      "epoch 3 average loss: 0.4614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 16:13:10 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 16:13:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 4/100\n",
      "1/231, train_loss: 0.4053\n",
      "2/231, train_loss: 0.4424\n",
      "3/231, train_loss: 0.3542\n",
      "4/231, train_loss: 0.3418\n",
      "5/231, train_loss: 0.2449\n",
      "6/231, train_loss: 0.3403\n",
      "7/231, train_loss: 0.6235\n",
      "8/231, train_loss: 0.5444\n",
      "9/231, train_loss: 0.3342\n",
      "10/231, train_loss: 0.3801\n",
      "11/231, train_loss: 0.7026\n",
      "12/231, train_loss: 0.4380\n",
      "13/231, train_loss: 0.3635\n",
      "14/231, train_loss: 0.3384\n",
      "15/231, train_loss: 0.6128\n",
      "16/231, train_loss: 0.3252\n",
      "17/231, train_loss: 0.3496\n",
      "18/231, train_loss: 0.6821\n",
      "19/231, train_loss: 0.6489\n",
      "20/231, train_loss: 0.4595\n",
      "21/231, train_loss: 0.4890\n",
      "22/231, train_loss: 0.4370\n",
      "23/231, train_loss: 0.4717\n",
      "24/231, train_loss: 0.2666\n",
      "25/231, train_loss: 0.3142\n",
      "26/231, train_loss: 0.5605\n",
      "27/231, train_loss: 0.2683\n",
      "28/231, train_loss: 0.4661\n",
      "29/231, train_loss: 0.2471\n",
      "30/231, train_loss: 0.3098\n",
      "31/231, train_loss: 0.5073\n",
      "32/231, train_loss: 0.3113\n",
      "33/231, train_loss: 0.5825\n",
      "34/231, train_loss: 0.3674\n",
      "35/231, train_loss: 0.3242\n",
      "36/231, train_loss: 0.3740\n",
      "37/231, train_loss: 0.6196\n",
      "38/231, train_loss: 0.3130\n",
      "39/231, train_loss: 0.3599\n",
      "40/231, train_loss: 0.5547\n",
      "41/231, train_loss: 0.4001\n",
      "42/231, train_loss: 0.4126\n",
      "43/231, train_loss: 0.6113\n",
      "44/231, train_loss: 0.3215\n",
      "45/231, train_loss: 0.3933\n",
      "46/231, train_loss: 0.4985\n",
      "47/231, train_loss: 0.4216\n",
      "48/231, train_loss: 0.6001\n",
      "49/231, train_loss: 0.2795\n",
      "50/231, train_loss: 0.3208\n",
      "51/231, train_loss: 0.3398\n",
      "52/231, train_loss: 0.3406\n",
      "53/231, train_loss: 0.3433\n",
      "54/231, train_loss: 0.2571\n",
      "55/231, train_loss: 0.4116\n",
      "56/231, train_loss: 0.4033\n",
      "57/231, train_loss: 0.2419\n",
      "58/231, train_loss: 0.3191\n",
      "59/231, train_loss: 0.7056\n",
      "60/231, train_loss: 0.3167\n",
      "61/231, train_loss: 0.2603\n",
      "62/231, train_loss: 0.2142\n",
      "63/231, train_loss: 0.3638\n",
      "64/231, train_loss: 0.2283\n",
      "65/231, train_loss: 0.3481\n",
      "66/231, train_loss: 0.5854\n",
      "67/231, train_loss: 0.4756\n",
      "68/231, train_loss: 0.4260\n",
      "69/231, train_loss: 0.5327\n",
      "70/231, train_loss: 0.5703\n",
      "71/231, train_loss: 0.3220\n",
      "72/231, train_loss: 0.3279\n",
      "73/231, train_loss: 0.2502\n",
      "74/231, train_loss: 0.4407\n",
      "75/231, train_loss: 0.4905\n",
      "76/231, train_loss: 0.6133\n",
      "77/231, train_loss: 0.4033\n",
      "78/231, train_loss: 0.2925\n",
      "79/231, train_loss: 0.4954\n",
      "80/231, train_loss: 0.3020\n",
      "81/231, train_loss: 0.3896\n",
      "82/231, train_loss: 0.1924\n",
      "83/231, train_loss: 0.5078\n",
      "84/231, train_loss: 0.2324\n",
      "85/231, train_loss: 0.5190\n",
      "86/231, train_loss: 0.3889\n",
      "87/231, train_loss: 0.2817\n",
      "88/231, train_loss: 0.3560\n",
      "89/231, train_loss: 0.5366\n",
      "90/231, train_loss: 0.4172\n",
      "91/231, train_loss: 0.2832\n",
      "92/231, train_loss: 0.3093\n",
      "93/231, train_loss: 0.3816\n",
      "94/231, train_loss: 0.3530\n",
      "95/231, train_loss: 0.5010\n",
      "96/231, train_loss: 0.6196\n",
      "97/231, train_loss: 0.2201\n",
      "98/231, train_loss: 0.2544\n",
      "99/231, train_loss: 0.5430\n",
      "100/231, train_loss: 0.2499\n",
      "101/231, train_loss: 0.5112\n",
      "102/231, train_loss: 0.2327\n",
      "103/231, train_loss: 0.6143\n",
      "104/231, train_loss: 0.6670\n",
      "105/231, train_loss: 0.3142\n",
      "106/231, train_loss: 0.4373\n",
      "107/231, train_loss: 0.2328\n",
      "108/231, train_loss: 0.7207\n",
      "109/231, train_loss: 0.4695\n",
      "110/231, train_loss: 0.6572\n",
      "111/231, train_loss: 0.3188\n",
      "112/231, train_loss: 0.4121\n",
      "113/231, train_loss: 0.4651\n",
      "114/231, train_loss: 0.3362\n",
      "115/231, train_loss: 0.3206\n",
      "116/231, train_loss: 0.6953\n",
      "117/231, train_loss: 0.3313\n",
      "118/231, train_loss: 0.4417\n",
      "119/231, train_loss: 0.4578\n",
      "120/231, train_loss: 0.2349\n",
      "121/231, train_loss: 0.3789\n",
      "122/231, train_loss: 0.6089\n",
      "123/231, train_loss: 0.4917\n",
      "124/231, train_loss: 0.3484\n",
      "125/231, train_loss: 0.4089\n",
      "126/231, train_loss: 0.5532\n",
      "127/231, train_loss: 0.2874\n",
      "128/231, train_loss: 0.3765\n",
      "129/231, train_loss: 0.2949\n",
      "130/231, train_loss: 0.1816\n",
      "131/231, train_loss: 0.2397\n",
      "132/231, train_loss: 0.3408\n",
      "133/231, train_loss: 0.7461\n",
      "134/231, train_loss: 0.3030\n",
      "135/231, train_loss: 0.3787\n",
      "136/231, train_loss: 0.5532\n",
      "137/231, train_loss: 0.4229\n",
      "138/231, train_loss: 0.3110\n",
      "139/231, train_loss: 0.2896\n",
      "140/231, train_loss: 0.5039\n",
      "141/231, train_loss: 0.3796\n",
      "142/231, train_loss: 0.2896\n",
      "143/231, train_loss: 0.3340\n",
      "144/231, train_loss: 0.2019\n",
      "145/231, train_loss: 0.2727\n",
      "146/231, train_loss: 0.3877\n",
      "147/231, train_loss: 0.3887\n",
      "148/231, train_loss: 0.3052\n",
      "149/231, train_loss: 0.2026\n",
      "150/231, train_loss: 0.3931\n",
      "151/231, train_loss: 0.3894\n",
      "152/231, train_loss: 0.4814\n",
      "153/231, train_loss: 0.8799\n",
      "154/231, train_loss: 0.2781\n",
      "155/231, train_loss: 0.4380\n",
      "156/231, train_loss: 0.2935\n",
      "157/231, train_loss: 0.4553\n",
      "158/231, train_loss: 0.2666\n",
      "159/231, train_loss: 0.4497\n",
      "160/231, train_loss: 0.2676\n",
      "161/231, train_loss: 0.3799\n",
      "162/231, train_loss: 0.3904\n",
      "163/231, train_loss: 0.5010\n",
      "164/231, train_loss: 0.2607\n",
      "165/231, train_loss: 0.3052\n",
      "166/231, train_loss: 0.2510\n",
      "167/231, train_loss: 0.4253\n",
      "168/231, train_loss: 0.5435\n",
      "169/231, train_loss: 0.1681\n",
      "170/231, train_loss: 0.3750\n",
      "171/231, train_loss: 0.4729\n",
      "172/231, train_loss: 0.2539\n",
      "173/231, train_loss: 0.4341\n",
      "174/231, train_loss: 0.3499\n",
      "175/231, train_loss: 0.3452\n",
      "176/231, train_loss: 0.5469\n",
      "177/231, train_loss: 0.2644\n",
      "178/231, train_loss: 0.2852\n",
      "179/231, train_loss: 0.5308\n",
      "180/231, train_loss: 0.2341\n",
      "181/231, train_loss: 0.1848\n",
      "182/231, train_loss: 0.2656\n",
      "183/231, train_loss: 0.2539\n",
      "184/231, train_loss: 0.4451\n",
      "185/231, train_loss: 0.4744\n",
      "186/231, train_loss: 0.4851\n",
      "187/231, train_loss: 0.5078\n",
      "188/231, train_loss: 0.2856\n",
      "189/231, train_loss: 0.3484\n",
      "190/231, train_loss: 0.2169\n",
      "191/231, train_loss: 0.3877\n",
      "192/231, train_loss: 0.7778\n",
      "193/231, train_loss: 0.3442\n",
      "194/231, train_loss: 0.3975\n",
      "195/231, train_loss: 0.2637\n",
      "196/231, train_loss: 0.4231\n",
      "197/231, train_loss: 0.2468\n",
      "198/231, train_loss: 0.3977\n",
      "199/231, train_loss: 0.4346\n",
      "200/231, train_loss: 0.2399\n",
      "201/231, train_loss: 0.1730\n",
      "202/231, train_loss: 0.3735\n",
      "203/231, train_loss: 0.3508\n",
      "204/231, train_loss: 0.2761\n",
      "205/231, train_loss: 0.3821\n",
      "206/231, train_loss: 0.3828\n",
      "207/231, train_loss: 0.3240\n",
      "208/231, train_loss: 0.2607\n",
      "209/231, train_loss: 0.2261\n",
      "210/231, train_loss: 0.4646\n",
      "211/231, train_loss: 0.2332\n",
      "212/231, train_loss: 0.3293\n",
      "213/231, train_loss: 0.4846\n",
      "214/231, train_loss: 0.2607\n",
      "215/231, train_loss: 0.3364\n",
      "216/231, train_loss: 0.2236\n",
      "217/231, train_loss: 0.2128\n",
      "218/231, train_loss: 0.3806\n",
      "219/231, train_loss: 0.5801\n",
      "220/231, train_loss: 0.5127\n",
      "221/231, train_loss: 0.2986\n",
      "222/231, train_loss: 0.5625\n",
      "223/231, train_loss: 0.3040\n",
      "224/231, train_loss: 0.3208\n",
      "225/231, train_loss: 0.1895\n",
      "226/231, train_loss: 0.4724\n",
      "227/231, train_loss: 0.3442\n",
      "228/231, train_loss: 0.3164\n",
      "229/231, train_loss: 0.2042\n",
      "230/231, train_loss: 0.2634\n",
      "231/231, train_loss: 0.2976\n",
      "232/231, train_loss: 0.2083\n",
      "epoch 4 average loss: 0.3879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 16:55:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 16:55:08 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 5/100\n",
      "1/231, train_loss: 0.4089\n",
      "2/231, train_loss: 0.2522\n",
      "3/231, train_loss: 0.3401\n",
      "4/231, train_loss: 0.3628\n",
      "5/231, train_loss: 0.4131\n",
      "6/231, train_loss: 0.2800\n",
      "7/231, train_loss: 0.3989\n",
      "8/231, train_loss: 0.5996\n",
      "9/231, train_loss: 0.4465\n",
      "10/231, train_loss: 0.2305\n",
      "11/231, train_loss: 0.2175\n",
      "12/231, train_loss: 0.5479\n",
      "13/231, train_loss: 0.2498\n",
      "14/231, train_loss: 0.3269\n",
      "15/231, train_loss: 0.2566\n",
      "16/231, train_loss: 0.4346\n",
      "17/231, train_loss: 0.3281\n",
      "18/231, train_loss: 0.2881\n",
      "19/231, train_loss: 0.3657\n",
      "20/231, train_loss: 0.2822\n",
      "21/231, train_loss: 0.4004\n",
      "22/231, train_loss: 0.3647\n",
      "23/231, train_loss: 0.2625\n",
      "24/231, train_loss: 0.5669\n",
      "25/231, train_loss: 0.2910\n",
      "26/231, train_loss: 0.3945\n",
      "27/231, train_loss: 0.5366\n",
      "28/231, train_loss: 0.6216\n",
      "29/231, train_loss: 0.4004\n",
      "30/231, train_loss: 0.4253\n",
      "31/231, train_loss: 0.5234\n",
      "32/231, train_loss: 0.4238\n",
      "33/231, train_loss: 0.4033\n",
      "34/231, train_loss: 0.3274\n",
      "35/231, train_loss: 0.2368\n",
      "36/231, train_loss: 0.4307\n",
      "37/231, train_loss: 0.2581\n",
      "38/231, train_loss: 0.3325\n",
      "39/231, train_loss: 0.2189\n",
      "40/231, train_loss: 0.3901\n",
      "41/231, train_loss: 0.3433\n",
      "42/231, train_loss: 0.2147\n",
      "43/231, train_loss: 0.5957\n",
      "44/231, train_loss: 0.3167\n",
      "45/231, train_loss: 0.1935\n",
      "46/231, train_loss: 0.2432\n",
      "47/231, train_loss: 0.3445\n",
      "48/231, train_loss: 0.2402\n",
      "49/231, train_loss: 0.3909\n",
      "50/231, train_loss: 0.3025\n",
      "51/231, train_loss: 0.4214\n",
      "52/231, train_loss: 0.2456\n",
      "53/231, train_loss: 0.3420\n",
      "54/231, train_loss: 0.3799\n",
      "55/231, train_loss: 0.2976\n",
      "56/231, train_loss: 0.3174\n",
      "57/231, train_loss: 0.5127\n",
      "58/231, train_loss: 0.4443\n",
      "59/231, train_loss: 0.4592\n",
      "60/231, train_loss: 0.2959\n",
      "61/231, train_loss: 0.5820\n",
      "62/231, train_loss: 0.3044\n",
      "63/231, train_loss: 0.2803\n",
      "64/231, train_loss: 0.4287\n",
      "65/231, train_loss: 0.4360\n",
      "66/231, train_loss: 0.3481\n",
      "67/231, train_loss: 0.4189\n",
      "68/231, train_loss: 0.2690\n",
      "69/231, train_loss: 0.2856\n",
      "70/231, train_loss: 0.3794\n",
      "71/231, train_loss: 0.3044\n",
      "72/231, train_loss: 0.4937\n",
      "73/231, train_loss: 0.2817\n",
      "74/231, train_loss: 0.3403\n",
      "75/231, train_loss: 0.4648\n",
      "76/231, train_loss: 0.7910\n",
      "77/231, train_loss: 0.4194\n",
      "78/231, train_loss: 0.1748\n",
      "79/231, train_loss: 0.2600\n",
      "80/231, train_loss: 0.4661\n",
      "81/231, train_loss: 0.3884\n",
      "82/231, train_loss: 0.2876\n",
      "83/231, train_loss: 0.6250\n",
      "84/231, train_loss: 0.5054\n",
      "85/231, train_loss: 0.2852\n",
      "86/231, train_loss: 0.4294\n",
      "87/231, train_loss: 0.5254\n",
      "88/231, train_loss: 0.3086\n",
      "89/231, train_loss: 0.4233\n",
      "90/231, train_loss: 0.4844\n",
      "91/231, train_loss: 0.3809\n",
      "92/231, train_loss: 0.3394\n",
      "93/231, train_loss: 0.2876\n",
      "94/231, train_loss: 0.2510\n",
      "95/231, train_loss: 0.3599\n",
      "96/231, train_loss: 0.2068\n",
      "97/231, train_loss: 0.3706\n",
      "98/231, train_loss: 0.5259\n",
      "99/231, train_loss: 0.2788\n",
      "100/231, train_loss: 0.3721\n",
      "101/231, train_loss: 0.3296\n",
      "102/231, train_loss: 0.2153\n",
      "103/231, train_loss: 0.2397\n",
      "104/231, train_loss: 0.3367\n",
      "105/231, train_loss: 0.4150\n",
      "106/231, train_loss: 0.3022\n",
      "107/231, train_loss: 0.2612\n",
      "108/231, train_loss: 0.3855\n",
      "109/231, train_loss: 0.2444\n",
      "110/231, train_loss: 0.2561\n",
      "111/231, train_loss: 0.3689\n",
      "112/231, train_loss: 0.4097\n",
      "113/231, train_loss: 0.4045\n",
      "114/231, train_loss: 0.2075\n",
      "115/231, train_loss: 0.1755\n",
      "116/231, train_loss: 0.4373\n",
      "117/231, train_loss: 0.3093\n",
      "118/231, train_loss: 0.2175\n",
      "119/231, train_loss: 0.1937\n",
      "120/231, train_loss: 0.1851\n",
      "121/231, train_loss: 0.3232\n",
      "122/231, train_loss: 0.2324\n",
      "123/231, train_loss: 0.3354\n",
      "124/231, train_loss: 0.3093\n",
      "125/231, train_loss: 0.2808\n",
      "126/231, train_loss: 0.3418\n",
      "127/231, train_loss: 0.4619\n",
      "128/231, train_loss: 0.1544\n",
      "129/231, train_loss: 0.6064\n",
      "130/231, train_loss: 0.2098\n",
      "131/231, train_loss: 0.4409\n",
      "132/231, train_loss: 0.1929\n",
      "133/231, train_loss: 0.2878\n",
      "134/231, train_loss: 0.2551\n",
      "135/231, train_loss: 0.3306\n",
      "136/231, train_loss: 0.1581\n",
      "137/231, train_loss: 0.2854\n",
      "138/231, train_loss: 0.2622\n",
      "139/231, train_loss: 0.3557\n",
      "140/231, train_loss: 0.5488\n",
      "141/231, train_loss: 0.1626\n",
      "142/231, train_loss: 0.3994\n",
      "143/231, train_loss: 0.3569\n",
      "144/231, train_loss: 0.3979\n",
      "145/231, train_loss: 0.4182\n",
      "146/231, train_loss: 0.3535\n",
      "147/231, train_loss: 0.3696\n",
      "148/231, train_loss: 0.1792\n",
      "149/231, train_loss: 0.3877\n",
      "150/231, train_loss: 0.5527\n",
      "151/231, train_loss: 0.5308\n",
      "152/231, train_loss: 0.3264\n",
      "153/231, train_loss: 0.4805\n",
      "154/231, train_loss: 0.3064\n",
      "155/231, train_loss: 0.3477\n",
      "156/231, train_loss: 0.4297\n",
      "157/231, train_loss: 0.2878\n",
      "158/231, train_loss: 0.2925\n",
      "159/231, train_loss: 0.3994\n",
      "160/231, train_loss: 0.4995\n",
      "161/231, train_loss: 0.2676\n",
      "162/231, train_loss: 0.3813\n",
      "163/231, train_loss: 0.3037\n",
      "164/231, train_loss: 0.2510\n",
      "165/231, train_loss: 0.2598\n",
      "166/231, train_loss: 0.1823\n",
      "167/231, train_loss: 0.2688\n",
      "168/231, train_loss: 0.4648\n",
      "169/231, train_loss: 0.2449\n",
      "170/231, train_loss: 0.3257\n",
      "171/231, train_loss: 0.3096\n",
      "172/231, train_loss: 0.2690\n",
      "173/231, train_loss: 0.1927\n",
      "174/231, train_loss: 0.4570\n",
      "175/231, train_loss: 0.1384\n",
      "176/231, train_loss: 0.1709\n",
      "177/231, train_loss: 0.3496\n",
      "178/231, train_loss: 0.3159\n",
      "179/231, train_loss: 0.5244\n",
      "180/231, train_loss: 0.2499\n",
      "181/231, train_loss: 0.2988\n",
      "182/231, train_loss: 0.2371\n",
      "183/231, train_loss: 0.2131\n",
      "184/231, train_loss: 0.2766\n",
      "185/231, train_loss: 0.3706\n",
      "186/231, train_loss: 0.2100\n",
      "187/231, train_loss: 0.4204\n",
      "188/231, train_loss: 0.4514\n",
      "189/231, train_loss: 0.2769\n",
      "190/231, train_loss: 0.3894\n",
      "191/231, train_loss: 0.2751\n",
      "192/231, train_loss: 0.3438\n",
      "193/231, train_loss: 0.3579\n",
      "194/231, train_loss: 0.3506\n",
      "195/231, train_loss: 0.4023\n",
      "196/231, train_loss: 0.2498\n",
      "197/231, train_loss: 0.4844\n",
      "198/231, train_loss: 0.3564\n",
      "199/231, train_loss: 0.4597\n",
      "200/231, train_loss: 0.3809\n",
      "201/231, train_loss: 0.4148\n",
      "202/231, train_loss: 0.4297\n",
      "203/231, train_loss: 0.2688\n",
      "204/231, train_loss: 0.4807\n",
      "205/231, train_loss: 0.2047\n",
      "206/231, train_loss: 0.4363\n",
      "207/231, train_loss: 0.2471\n",
      "208/231, train_loss: 0.7046\n",
      "209/231, train_loss: 0.3428\n",
      "210/231, train_loss: 0.4167\n",
      "211/231, train_loss: 0.4365\n",
      "212/231, train_loss: 0.2886\n",
      "213/231, train_loss: 0.4119\n",
      "214/231, train_loss: 0.2012\n",
      "215/231, train_loss: 0.5381\n",
      "216/231, train_loss: 0.3860\n",
      "217/231, train_loss: 0.3469\n",
      "218/231, train_loss: 0.2241\n",
      "219/231, train_loss: 0.4109\n",
      "220/231, train_loss: 0.3267\n",
      "221/231, train_loss: 0.2017\n",
      "222/231, train_loss: 0.3047\n",
      "223/231, train_loss: 0.3013\n",
      "224/231, train_loss: 0.2164\n",
      "225/231, train_loss: 0.2832\n",
      "226/231, train_loss: 0.1709\n",
      "227/231, train_loss: 0.3115\n",
      "228/231, train_loss: 0.2861\n",
      "229/231, train_loss: 0.3345\n",
      "230/231, train_loss: 0.4888\n",
      "231/231, train_loss: 0.2764\n",
      "232/231, train_loss: 0.1841\n",
      "epoch 5 average loss: 0.3468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 17:36:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 17:36:30 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.4887226201057008, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.4887226201057008, 'AP_IoU_0.10_MaxDet_100': 0.535436831654446, 'nodule_AP_IoU_0.10_MaxDet_100': 0.535436831654446, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.814814825852712, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.814814825852712, 'AR_IoU_0.10_MaxDet_100': 0.8717948794364929, 'nodule_AR_IoU_0.10_MaxDet_100': 0.8717948794364929}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 17:48:46 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 17:48:50 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 5 current metric: 0.6777 best metric: 0.6777 at epoch 5\n",
      "----------\n",
      "epoch 6/100\n",
      "1/231, train_loss: 0.4360\n",
      "2/231, train_loss: 0.2056\n",
      "3/231, train_loss: 0.4331\n",
      "4/231, train_loss: 0.1940\n",
      "5/231, train_loss: 0.2034\n",
      "6/231, train_loss: 0.1692\n",
      "7/231, train_loss: 0.6294\n",
      "8/231, train_loss: 0.1841\n",
      "9/231, train_loss: 0.2035\n",
      "10/231, train_loss: 0.2737\n",
      "11/231, train_loss: 0.3464\n",
      "12/231, train_loss: 0.2280\n",
      "13/231, train_loss: 0.5625\n",
      "14/231, train_loss: 0.3320\n",
      "15/231, train_loss: 0.3418\n",
      "16/231, train_loss: 0.4287\n",
      "17/231, train_loss: 0.2312\n",
      "18/231, train_loss: 0.1710\n",
      "19/231, train_loss: 0.2030\n",
      "20/231, train_loss: 0.1460\n",
      "21/231, train_loss: 0.3599\n",
      "22/231, train_loss: 0.4746\n",
      "23/231, train_loss: 0.2754\n",
      "24/231, train_loss: 0.2561\n",
      "25/231, train_loss: 0.2290\n",
      "26/231, train_loss: 0.5264\n",
      "27/231, train_loss: 0.3999\n",
      "28/231, train_loss: 0.4067\n",
      "29/231, train_loss: 0.2404\n",
      "30/231, train_loss: 0.3540\n",
      "31/231, train_loss: 0.3760\n",
      "32/231, train_loss: 0.2310\n",
      "33/231, train_loss: 0.2500\n",
      "34/231, train_loss: 0.3140\n",
      "35/231, train_loss: 0.2114\n",
      "36/231, train_loss: 0.2225\n",
      "37/231, train_loss: 0.2908\n",
      "38/231, train_loss: 0.2715\n",
      "39/231, train_loss: 0.4197\n",
      "40/231, train_loss: 0.3647\n",
      "41/231, train_loss: 0.5391\n",
      "42/231, train_loss: 0.1824\n",
      "43/231, train_loss: 0.7095\n",
      "44/231, train_loss: 0.4622\n",
      "45/231, train_loss: 0.2114\n",
      "46/231, train_loss: 0.2345\n",
      "47/231, train_loss: 0.3486\n",
      "48/231, train_loss: 0.4170\n",
      "49/231, train_loss: 0.2974\n",
      "50/231, train_loss: 0.3284\n",
      "51/231, train_loss: 0.4531\n",
      "52/231, train_loss: 0.3477\n",
      "53/231, train_loss: 0.2937\n",
      "54/231, train_loss: 0.2634\n",
      "55/231, train_loss: 0.4460\n",
      "56/231, train_loss: 0.3037\n",
      "57/231, train_loss: 0.2620\n",
      "58/231, train_loss: 0.6367\n",
      "59/231, train_loss: 0.3059\n",
      "60/231, train_loss: 0.2491\n",
      "61/231, train_loss: 0.3726\n",
      "62/231, train_loss: 0.2014\n",
      "63/231, train_loss: 0.2698\n",
      "64/231, train_loss: 0.2920\n",
      "65/231, train_loss: 0.4932\n",
      "66/231, train_loss: 0.3281\n",
      "67/231, train_loss: 0.2391\n",
      "68/231, train_loss: 0.2585\n",
      "69/231, train_loss: 0.3984\n",
      "70/231, train_loss: 0.1809\n",
      "71/231, train_loss: 0.5679\n",
      "72/231, train_loss: 0.2913\n",
      "73/231, train_loss: 0.2805\n",
      "74/231, train_loss: 0.2457\n",
      "75/231, train_loss: 0.2402\n",
      "76/231, train_loss: 0.3750\n",
      "77/231, train_loss: 0.4133\n",
      "78/231, train_loss: 0.3245\n",
      "79/231, train_loss: 0.1157\n",
      "80/231, train_loss: 0.3550\n",
      "81/231, train_loss: 0.4966\n",
      "82/231, train_loss: 0.1799\n",
      "83/231, train_loss: 0.3101\n",
      "84/231, train_loss: 0.2069\n",
      "85/231, train_loss: 0.1655\n",
      "86/231, train_loss: 0.4092\n",
      "87/231, train_loss: 0.2644\n",
      "88/231, train_loss: 0.3303\n",
      "89/231, train_loss: 0.2146\n",
      "90/231, train_loss: 0.1843\n",
      "91/231, train_loss: 0.2191\n",
      "92/231, train_loss: 0.1846\n",
      "93/231, train_loss: 0.1914\n",
      "94/231, train_loss: 0.1505\n",
      "95/231, train_loss: 0.3347\n",
      "96/231, train_loss: 0.3228\n",
      "97/231, train_loss: 0.1040\n",
      "98/231, train_loss: 0.1874\n",
      "99/231, train_loss: 0.3242\n",
      "100/231, train_loss: 0.1530\n",
      "101/231, train_loss: 0.4287\n",
      "102/231, train_loss: 0.2737\n",
      "103/231, train_loss: 0.3320\n",
      "104/231, train_loss: 0.3411\n",
      "105/231, train_loss: 0.2126\n",
      "106/231, train_loss: 0.1982\n",
      "107/231, train_loss: 0.1321\n",
      "108/231, train_loss: 0.5342\n",
      "109/231, train_loss: 0.2073\n",
      "110/231, train_loss: 0.2122\n",
      "111/231, train_loss: 0.4958\n",
      "112/231, train_loss: 0.3608\n",
      "113/231, train_loss: 0.3577\n",
      "114/231, train_loss: 0.2271\n",
      "115/231, train_loss: 0.6338\n",
      "116/231, train_loss: 0.4778\n",
      "117/231, train_loss: 0.2371\n",
      "118/231, train_loss: 0.4358\n",
      "119/231, train_loss: 0.3125\n",
      "120/231, train_loss: 0.4651\n",
      "121/231, train_loss: 0.1749\n",
      "122/231, train_loss: 0.2036\n",
      "123/231, train_loss: 0.1980\n",
      "124/231, train_loss: 0.2747\n",
      "125/231, train_loss: 0.4214\n",
      "126/231, train_loss: 0.1694\n",
      "127/231, train_loss: 0.1597\n",
      "128/231, train_loss: 0.1844\n",
      "129/231, train_loss: 0.2191\n",
      "130/231, train_loss: 0.2891\n",
      "131/231, train_loss: 0.2292\n",
      "132/231, train_loss: 0.5225\n",
      "133/231, train_loss: 0.4001\n",
      "134/231, train_loss: 0.3403\n",
      "135/231, train_loss: 0.2983\n",
      "136/231, train_loss: 0.2993\n",
      "137/231, train_loss: 0.4092\n",
      "138/231, train_loss: 0.1602\n",
      "139/231, train_loss: 0.5566\n",
      "140/231, train_loss: 0.5522\n",
      "141/231, train_loss: 0.2188\n",
      "142/231, train_loss: 0.2427\n",
      "143/231, train_loss: 0.1377\n",
      "144/231, train_loss: 0.3074\n",
      "145/231, train_loss: 0.2949\n",
      "146/231, train_loss: 0.2397\n",
      "147/231, train_loss: 0.2131\n",
      "148/231, train_loss: 0.3076\n",
      "149/231, train_loss: 0.2292\n",
      "150/231, train_loss: 0.4082\n",
      "151/231, train_loss: 0.2268\n",
      "152/231, train_loss: 0.1663\n",
      "153/231, train_loss: 0.5732\n",
      "154/231, train_loss: 0.3269\n",
      "155/231, train_loss: 0.2069\n",
      "156/231, train_loss: 0.3005\n",
      "157/231, train_loss: 0.1396\n",
      "158/231, train_loss: 0.3040\n",
      "159/231, train_loss: 0.3933\n",
      "160/231, train_loss: 0.1855\n",
      "161/231, train_loss: 0.3044\n",
      "162/231, train_loss: 0.2473\n",
      "163/231, train_loss: 0.3352\n",
      "164/231, train_loss: 0.3179\n",
      "165/231, train_loss: 0.4004\n",
      "166/231, train_loss: 0.4612\n",
      "167/231, train_loss: 0.2175\n",
      "168/231, train_loss: 0.1871\n",
      "169/231, train_loss: 0.2010\n",
      "170/231, train_loss: 0.2959\n",
      "171/231, train_loss: 0.5371\n",
      "172/231, train_loss: 0.4768\n",
      "173/231, train_loss: 0.3638\n",
      "174/231, train_loss: 0.2522\n",
      "175/231, train_loss: 0.1738\n",
      "176/231, train_loss: 0.2236\n",
      "177/231, train_loss: 0.2131\n",
      "178/231, train_loss: 0.3345\n",
      "179/231, train_loss: 0.2079\n",
      "180/231, train_loss: 0.3452\n",
      "181/231, train_loss: 0.1340\n",
      "182/231, train_loss: 0.2445\n",
      "183/231, train_loss: 0.3501\n",
      "184/231, train_loss: 0.2144\n",
      "185/231, train_loss: 0.2798\n",
      "186/231, train_loss: 0.2043\n",
      "187/231, train_loss: 0.4812\n",
      "188/231, train_loss: 0.2693\n",
      "189/231, train_loss: 0.1338\n",
      "190/231, train_loss: 0.5483\n",
      "191/231, train_loss: 0.2036\n",
      "192/231, train_loss: 0.4646\n",
      "193/231, train_loss: 0.1859\n",
      "194/231, train_loss: 0.2046\n",
      "195/231, train_loss: 0.2476\n",
      "196/231, train_loss: 0.3735\n",
      "197/231, train_loss: 0.4224\n",
      "198/231, train_loss: 0.7275\n",
      "199/231, train_loss: 0.4460\n",
      "200/231, train_loss: 0.2739\n",
      "201/231, train_loss: 0.2998\n",
      "202/231, train_loss: 0.2979\n",
      "203/231, train_loss: 0.3582\n",
      "204/231, train_loss: 0.2664\n",
      "205/231, train_loss: 0.1829\n",
      "206/231, train_loss: 0.1450\n",
      "207/231, train_loss: 0.1592\n",
      "208/231, train_loss: 0.2208\n",
      "209/231, train_loss: 0.1588\n",
      "210/231, train_loss: 0.2487\n",
      "211/231, train_loss: 0.5498\n",
      "212/231, train_loss: 0.4998\n",
      "213/231, train_loss: 0.3643\n",
      "214/231, train_loss: 0.2788\n",
      "215/231, train_loss: 0.3359\n",
      "216/231, train_loss: 0.2542\n",
      "217/231, train_loss: 0.2866\n",
      "218/231, train_loss: 0.4170\n",
      "219/231, train_loss: 0.5166\n",
      "220/231, train_loss: 0.3818\n",
      "221/231, train_loss: 0.2788\n",
      "222/231, train_loss: 0.1897\n",
      "223/231, train_loss: 0.7319\n",
      "224/231, train_loss: 0.4238\n",
      "225/231, train_loss: 0.3057\n",
      "226/231, train_loss: 0.2150\n",
      "227/231, train_loss: 0.4429\n",
      "228/231, train_loss: 0.1992\n",
      "229/231, train_loss: 0.1855\n",
      "230/231, train_loss: 0.1686\n",
      "231/231, train_loss: 0.2822\n",
      "232/231, train_loss: 0.6616\n",
      "epoch 6 average loss: 0.3105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 18:36:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 18:36:46 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 7/100\n",
      "1/231, train_loss: 0.2343\n",
      "2/231, train_loss: 0.2920\n",
      "3/231, train_loss: 0.1604\n",
      "4/231, train_loss: 0.3694\n",
      "5/231, train_loss: 0.3479\n",
      "6/231, train_loss: 0.3667\n",
      "7/231, train_loss: 0.3188\n",
      "8/231, train_loss: 0.2007\n",
      "9/231, train_loss: 0.2191\n",
      "10/231, train_loss: 0.4536\n",
      "11/231, train_loss: 0.2529\n",
      "12/231, train_loss: 0.4861\n",
      "13/231, train_loss: 0.2190\n",
      "14/231, train_loss: 0.3535\n",
      "15/231, train_loss: 0.2432\n",
      "16/231, train_loss: 0.2148\n",
      "17/231, train_loss: 0.3823\n",
      "18/231, train_loss: 0.3247\n",
      "19/231, train_loss: 0.3367\n",
      "20/231, train_loss: 0.2456\n",
      "21/231, train_loss: 0.4114\n",
      "22/231, train_loss: 0.2959\n",
      "23/231, train_loss: 0.3008\n",
      "24/231, train_loss: 0.4075\n",
      "25/231, train_loss: 0.3335\n",
      "26/231, train_loss: 0.2269\n",
      "27/231, train_loss: 0.2847\n",
      "28/231, train_loss: 0.2512\n",
      "29/231, train_loss: 0.1960\n",
      "30/231, train_loss: 0.6323\n",
      "31/231, train_loss: 0.5527\n",
      "32/231, train_loss: 0.3481\n",
      "33/231, train_loss: 0.4219\n",
      "34/231, train_loss: 0.2839\n",
      "35/231, train_loss: 0.3662\n",
      "36/231, train_loss: 0.1672\n",
      "37/231, train_loss: 0.1653\n",
      "38/231, train_loss: 0.2285\n",
      "39/231, train_loss: 0.2269\n",
      "40/231, train_loss: 0.5703\n",
      "41/231, train_loss: 0.2732\n",
      "42/231, train_loss: 0.3130\n",
      "43/231, train_loss: 0.1259\n",
      "44/231, train_loss: 0.5312\n",
      "45/231, train_loss: 0.4692\n",
      "46/231, train_loss: 0.1768\n",
      "47/231, train_loss: 0.2944\n",
      "48/231, train_loss: 0.1995\n",
      "49/231, train_loss: 0.2117\n",
      "50/231, train_loss: 0.1940\n",
      "51/231, train_loss: 0.2449\n",
      "52/231, train_loss: 0.2487\n",
      "53/231, train_loss: 0.3301\n",
      "54/231, train_loss: 0.3191\n",
      "55/231, train_loss: 0.2683\n",
      "56/231, train_loss: 0.5518\n",
      "57/231, train_loss: 0.2839\n",
      "58/231, train_loss: 0.1652\n",
      "59/231, train_loss: 0.3420\n",
      "60/231, train_loss: 0.5156\n",
      "61/231, train_loss: 0.2871\n",
      "62/231, train_loss: 0.2317\n",
      "63/231, train_loss: 0.6914\n",
      "64/231, train_loss: 0.2812\n",
      "65/231, train_loss: 0.3076\n",
      "66/231, train_loss: 0.4058\n",
      "67/231, train_loss: 0.4043\n",
      "68/231, train_loss: 0.3015\n",
      "69/231, train_loss: 0.2539\n",
      "70/231, train_loss: 0.1676\n",
      "71/231, train_loss: 0.2126\n",
      "72/231, train_loss: 0.2322\n",
      "73/231, train_loss: 0.3203\n",
      "74/231, train_loss: 0.4761\n",
      "75/231, train_loss: 0.1597\n",
      "76/231, train_loss: 0.2358\n",
      "77/231, train_loss: 0.2922\n",
      "78/231, train_loss: 0.2632\n",
      "79/231, train_loss: 0.1730\n",
      "80/231, train_loss: 0.1858\n",
      "81/231, train_loss: 0.2134\n",
      "82/231, train_loss: 0.2303\n",
      "83/231, train_loss: 0.1517\n",
      "84/231, train_loss: 0.1917\n",
      "85/231, train_loss: 0.3596\n",
      "86/231, train_loss: 0.2532\n",
      "87/231, train_loss: 0.2842\n",
      "88/231, train_loss: 0.6338\n",
      "89/231, train_loss: 0.4946\n",
      "90/231, train_loss: 0.2986\n",
      "91/231, train_loss: 0.3904\n",
      "92/231, train_loss: 0.2141\n",
      "93/231, train_loss: 0.2539\n",
      "94/231, train_loss: 0.3755\n",
      "95/231, train_loss: 0.2949\n",
      "96/231, train_loss: 0.1832\n",
      "97/231, train_loss: 0.1448\n",
      "98/231, train_loss: 0.2710\n",
      "99/231, train_loss: 0.2137\n",
      "100/231, train_loss: 0.3694\n",
      "101/231, train_loss: 0.3293\n",
      "102/231, train_loss: 0.3772\n",
      "103/231, train_loss: 0.2118\n",
      "104/231, train_loss: 0.2817\n",
      "105/231, train_loss: 0.2078\n",
      "106/231, train_loss: 0.4468\n",
      "107/231, train_loss: 0.2449\n",
      "108/231, train_loss: 0.2180\n",
      "109/231, train_loss: 0.2415\n",
      "110/231, train_loss: 0.3401\n",
      "111/231, train_loss: 0.4478\n",
      "112/231, train_loss: 0.3743\n",
      "113/231, train_loss: 0.1583\n",
      "114/231, train_loss: 0.3909\n",
      "115/231, train_loss: 0.3572\n",
      "116/231, train_loss: 0.5264\n",
      "117/231, train_loss: 0.6226\n",
      "118/231, train_loss: 0.1619\n",
      "119/231, train_loss: 0.3970\n",
      "120/231, train_loss: 0.2793\n",
      "121/231, train_loss: 0.5112\n",
      "122/231, train_loss: 0.3667\n",
      "123/231, train_loss: 0.3813\n",
      "124/231, train_loss: 0.2881\n",
      "125/231, train_loss: 0.2268\n",
      "126/231, train_loss: 0.2085\n",
      "127/231, train_loss: 0.4111\n",
      "128/231, train_loss: 0.3586\n",
      "129/231, train_loss: 0.3552\n",
      "130/231, train_loss: 0.2532\n",
      "131/231, train_loss: 0.1851\n",
      "132/231, train_loss: 0.1982\n",
      "133/231, train_loss: 0.1831\n",
      "134/231, train_loss: 0.4597\n",
      "135/231, train_loss: 0.3044\n",
      "136/231, train_loss: 0.2600\n",
      "137/231, train_loss: 0.1581\n",
      "138/231, train_loss: 0.2350\n",
      "139/231, train_loss: 0.4116\n",
      "140/231, train_loss: 0.1707\n",
      "141/231, train_loss: 0.4880\n",
      "142/231, train_loss: 0.3235\n",
      "143/231, train_loss: 0.5161\n",
      "144/231, train_loss: 0.1760\n",
      "145/231, train_loss: 0.1887\n",
      "146/231, train_loss: 0.2400\n",
      "147/231, train_loss: 0.2402\n",
      "148/231, train_loss: 0.1799\n",
      "149/231, train_loss: 0.1782\n",
      "150/231, train_loss: 0.3298\n",
      "151/231, train_loss: 0.6045\n",
      "152/231, train_loss: 0.2585\n",
      "153/231, train_loss: 0.2358\n",
      "154/231, train_loss: 0.5200\n",
      "155/231, train_loss: 0.3145\n",
      "156/231, train_loss: 0.2764\n",
      "157/231, train_loss: 0.4597\n",
      "158/231, train_loss: 0.2954\n",
      "159/231, train_loss: 0.4795\n",
      "160/231, train_loss: 0.2705\n",
      "161/231, train_loss: 0.2505\n",
      "162/231, train_loss: 0.1962\n",
      "163/231, train_loss: 0.1436\n",
      "164/231, train_loss: 0.3064\n",
      "165/231, train_loss: 0.2769\n",
      "166/231, train_loss: 0.2007\n",
      "167/231, train_loss: 0.2189\n",
      "168/231, train_loss: 0.2676\n",
      "169/231, train_loss: 0.2573\n",
      "170/231, train_loss: 0.2607\n",
      "171/231, train_loss: 0.2642\n",
      "172/231, train_loss: 0.4214\n",
      "173/231, train_loss: 0.1814\n",
      "174/231, train_loss: 0.2939\n",
      "175/231, train_loss: 0.4702\n",
      "176/231, train_loss: 0.2163\n",
      "177/231, train_loss: 0.1509\n",
      "178/231, train_loss: 0.3545\n",
      "179/231, train_loss: 0.2554\n",
      "180/231, train_loss: 0.4204\n",
      "181/231, train_loss: 0.4636\n",
      "182/231, train_loss: 0.3806\n",
      "183/231, train_loss: 0.5732\n",
      "184/231, train_loss: 0.2852\n",
      "185/231, train_loss: 0.2808\n",
      "186/231, train_loss: 0.2314\n",
      "187/231, train_loss: 0.3345\n",
      "188/231, train_loss: 0.4783\n",
      "189/231, train_loss: 0.3464\n",
      "190/231, train_loss: 0.2046\n",
      "191/231, train_loss: 0.5278\n",
      "192/231, train_loss: 0.4106\n",
      "193/231, train_loss: 0.3513\n",
      "194/231, train_loss: 0.2332\n",
      "195/231, train_loss: 0.1609\n",
      "196/231, train_loss: 0.1279\n",
      "197/231, train_loss: 0.4841\n",
      "198/231, train_loss: 0.2427\n",
      "199/231, train_loss: 0.2610\n",
      "200/231, train_loss: 0.3271\n",
      "201/231, train_loss: 0.4524\n",
      "202/231, train_loss: 0.2725\n",
      "203/231, train_loss: 0.3394\n",
      "204/231, train_loss: 0.2362\n",
      "205/231, train_loss: 0.1694\n",
      "206/231, train_loss: 0.2576\n",
      "207/231, train_loss: 0.2549\n",
      "208/231, train_loss: 0.3022\n",
      "209/231, train_loss: 0.4683\n",
      "210/231, train_loss: 0.1484\n",
      "211/231, train_loss: 0.1565\n",
      "212/231, train_loss: 0.2297\n",
      "213/231, train_loss: 0.3438\n",
      "214/231, train_loss: 0.5493\n",
      "215/231, train_loss: 0.4692\n",
      "216/231, train_loss: 0.1681\n",
      "217/231, train_loss: 0.2070\n",
      "218/231, train_loss: 0.2725\n",
      "219/231, train_loss: 0.3250\n",
      "220/231, train_loss: 0.1720\n",
      "221/231, train_loss: 0.2766\n",
      "222/231, train_loss: 0.3386\n",
      "223/231, train_loss: 0.3086\n",
      "224/231, train_loss: 0.2244\n",
      "225/231, train_loss: 0.3357\n",
      "226/231, train_loss: 0.3875\n",
      "227/231, train_loss: 0.2627\n",
      "228/231, train_loss: 0.3799\n",
      "229/231, train_loss: 0.1534\n",
      "230/231, train_loss: 0.1825\n",
      "231/231, train_loss: 0.2479\n",
      "232/231, train_loss: 0.1757\n",
      "epoch 7 average loss: 0.3050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 19:24:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 19:24:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 8/100\n",
      "1/231, train_loss: 0.1458\n",
      "2/231, train_loss: 0.1857\n",
      "3/231, train_loss: 0.2754\n",
      "4/231, train_loss: 0.2551\n",
      "5/231, train_loss: 0.1936\n",
      "6/231, train_loss: 0.4370\n",
      "7/231, train_loss: 0.3943\n",
      "8/231, train_loss: 0.4900\n",
      "9/231, train_loss: 0.3408\n",
      "10/231, train_loss: 0.2333\n",
      "11/231, train_loss: 0.2637\n",
      "12/231, train_loss: 0.1958\n",
      "13/231, train_loss: 0.2078\n",
      "14/231, train_loss: 0.1570\n",
      "15/231, train_loss: 0.4854\n",
      "16/231, train_loss: 0.4319\n",
      "17/231, train_loss: 0.2971\n",
      "18/231, train_loss: 0.3682\n",
      "19/231, train_loss: 0.2156\n",
      "20/231, train_loss: 0.2505\n",
      "21/231, train_loss: 0.3704\n",
      "22/231, train_loss: 0.3389\n",
      "23/231, train_loss: 0.1517\n",
      "24/231, train_loss: 0.3779\n",
      "25/231, train_loss: 0.3896\n",
      "26/231, train_loss: 0.2104\n",
      "27/231, train_loss: 0.3794\n",
      "28/231, train_loss: 0.2213\n",
      "29/231, train_loss: 0.2321\n",
      "30/231, train_loss: 0.2625\n",
      "31/231, train_loss: 0.1727\n",
      "32/231, train_loss: 0.5933\n",
      "33/231, train_loss: 0.2576\n",
      "34/231, train_loss: 0.3220\n",
      "35/231, train_loss: 0.4070\n",
      "36/231, train_loss: 0.3838\n",
      "37/231, train_loss: 0.3062\n",
      "38/231, train_loss: 0.2896\n",
      "39/231, train_loss: 0.2416\n",
      "40/231, train_loss: 0.4226\n",
      "41/231, train_loss: 0.3662\n",
      "42/231, train_loss: 0.1196\n",
      "43/231, train_loss: 0.3232\n",
      "44/231, train_loss: 0.1790\n",
      "45/231, train_loss: 0.1089\n",
      "46/231, train_loss: 0.1787\n",
      "47/231, train_loss: 0.1591\n",
      "48/231, train_loss: 0.2106\n",
      "49/231, train_loss: 0.6494\n",
      "50/231, train_loss: 0.6299\n",
      "51/231, train_loss: 0.1572\n",
      "52/231, train_loss: 0.1373\n",
      "53/231, train_loss: 0.1895\n",
      "54/231, train_loss: 0.4023\n",
      "55/231, train_loss: 0.1884\n",
      "56/231, train_loss: 0.3228\n",
      "57/231, train_loss: 0.2622\n",
      "58/231, train_loss: 0.5234\n",
      "59/231, train_loss: 0.2888\n",
      "60/231, train_loss: 0.3770\n",
      "61/231, train_loss: 0.1895\n",
      "62/231, train_loss: 0.3684\n",
      "63/231, train_loss: 0.2556\n",
      "64/231, train_loss: 0.4526\n",
      "65/231, train_loss: 0.1987\n",
      "66/231, train_loss: 0.3276\n",
      "67/231, train_loss: 0.3877\n",
      "68/231, train_loss: 0.1818\n",
      "69/231, train_loss: 0.3413\n",
      "70/231, train_loss: 0.2412\n",
      "71/231, train_loss: 0.5000\n",
      "72/231, train_loss: 0.3950\n",
      "73/231, train_loss: 0.2981\n",
      "74/231, train_loss: 0.2634\n",
      "75/231, train_loss: 0.4355\n",
      "76/231, train_loss: 0.2339\n",
      "77/231, train_loss: 0.3735\n",
      "78/231, train_loss: 0.1708\n",
      "79/231, train_loss: 0.3386\n",
      "80/231, train_loss: 0.1509\n",
      "81/231, train_loss: 0.2139\n",
      "82/231, train_loss: 0.2397\n",
      "83/231, train_loss: 0.3364\n",
      "84/231, train_loss: 0.3198\n",
      "85/231, train_loss: 0.2910\n",
      "86/231, train_loss: 0.3091\n",
      "87/231, train_loss: 0.2125\n",
      "88/231, train_loss: 0.3042\n",
      "89/231, train_loss: 0.2600\n",
      "90/231, train_loss: 0.4189\n",
      "91/231, train_loss: 0.2786\n",
      "92/231, train_loss: 0.1903\n",
      "93/231, train_loss: 0.2000\n",
      "94/231, train_loss: 0.1951\n",
      "95/231, train_loss: 0.2646\n",
      "96/231, train_loss: 0.3179\n",
      "97/231, train_loss: 0.3513\n",
      "98/231, train_loss: 0.2515\n",
      "99/231, train_loss: 0.2112\n",
      "100/231, train_loss: 0.1808\n",
      "101/231, train_loss: 0.1576\n",
      "102/231, train_loss: 0.2566\n",
      "103/231, train_loss: 0.1891\n",
      "104/231, train_loss: 0.3887\n",
      "105/231, train_loss: 0.6016\n",
      "106/231, train_loss: 0.3865\n",
      "107/231, train_loss: 0.3062\n",
      "108/231, train_loss: 0.3535\n",
      "109/231, train_loss: 0.2489\n",
      "110/231, train_loss: 0.2024\n",
      "111/231, train_loss: 0.2974\n",
      "112/231, train_loss: 0.2350\n",
      "113/231, train_loss: 0.5063\n",
      "114/231, train_loss: 0.3025\n",
      "115/231, train_loss: 0.2332\n",
      "116/231, train_loss: 0.1287\n",
      "117/231, train_loss: 0.2717\n",
      "118/231, train_loss: 0.4241\n",
      "119/231, train_loss: 0.2886\n",
      "120/231, train_loss: 0.2646\n",
      "121/231, train_loss: 0.2139\n",
      "122/231, train_loss: 0.3149\n",
      "123/231, train_loss: 0.1450\n",
      "124/231, train_loss: 0.2278\n",
      "125/231, train_loss: 0.2202\n",
      "126/231, train_loss: 0.4431\n",
      "127/231, train_loss: 0.1146\n",
      "128/231, train_loss: 0.2103\n",
      "129/231, train_loss: 0.1268\n",
      "130/231, train_loss: 0.2002\n",
      "131/231, train_loss: 0.3428\n",
      "132/231, train_loss: 0.3491\n",
      "133/231, train_loss: 0.2303\n",
      "134/231, train_loss: 0.1848\n",
      "135/231, train_loss: 0.1686\n",
      "136/231, train_loss: 0.1243\n",
      "137/231, train_loss: 0.2861\n",
      "138/231, train_loss: 0.3550\n",
      "139/231, train_loss: 0.1444\n",
      "140/231, train_loss: 0.1394\n",
      "141/231, train_loss: 0.0875\n",
      "142/231, train_loss: 0.3271\n",
      "143/231, train_loss: 0.2274\n",
      "144/231, train_loss: 0.3167\n",
      "145/231, train_loss: 0.1704\n",
      "146/231, train_loss: 0.3774\n",
      "147/231, train_loss: 0.2405\n",
      "148/231, train_loss: 0.2085\n",
      "149/231, train_loss: 0.3291\n",
      "150/231, train_loss: 0.4634\n",
      "151/231, train_loss: 0.1661\n",
      "152/231, train_loss: 0.1641\n",
      "153/231, train_loss: 0.4055\n",
      "154/231, train_loss: 0.1501\n",
      "155/231, train_loss: 0.2676\n",
      "156/231, train_loss: 0.1710\n",
      "157/231, train_loss: 0.2979\n",
      "158/231, train_loss: 0.5361\n",
      "159/231, train_loss: 0.2227\n",
      "160/231, train_loss: 0.2798\n",
      "161/231, train_loss: 0.1787\n",
      "162/231, train_loss: 0.2059\n",
      "163/231, train_loss: 0.1622\n",
      "164/231, train_loss: 0.1301\n",
      "165/231, train_loss: 0.4270\n",
      "166/231, train_loss: 0.2947\n",
      "167/231, train_loss: 0.3271\n",
      "168/231, train_loss: 0.2534\n",
      "169/231, train_loss: 0.4319\n",
      "170/231, train_loss: 0.4783\n",
      "171/231, train_loss: 0.2094\n",
      "172/231, train_loss: 0.2485\n",
      "173/231, train_loss: 0.1782\n",
      "174/231, train_loss: 0.3022\n",
      "175/231, train_loss: 0.2241\n",
      "176/231, train_loss: 0.4358\n",
      "177/231, train_loss: 0.2871\n",
      "178/231, train_loss: 0.1960\n",
      "179/231, train_loss: 0.3967\n",
      "180/231, train_loss: 0.4407\n",
      "181/231, train_loss: 0.1791\n",
      "182/231, train_loss: 0.1970\n",
      "183/231, train_loss: 0.2280\n",
      "184/231, train_loss: 0.1317\n",
      "185/231, train_loss: 0.2401\n",
      "186/231, train_loss: 0.2264\n",
      "187/231, train_loss: 0.1761\n",
      "188/231, train_loss: 0.4058\n",
      "189/231, train_loss: 0.1709\n",
      "190/231, train_loss: 0.2539\n",
      "191/231, train_loss: 0.2039\n",
      "192/231, train_loss: 0.3853\n",
      "193/231, train_loss: 0.2832\n",
      "194/231, train_loss: 0.1833\n",
      "195/231, train_loss: 0.1345\n",
      "196/231, train_loss: 0.4338\n",
      "197/231, train_loss: 0.4434\n",
      "198/231, train_loss: 0.5068\n",
      "199/231, train_loss: 0.3325\n",
      "200/231, train_loss: 0.4194\n",
      "201/231, train_loss: 0.1946\n",
      "202/231, train_loss: 0.4543\n",
      "203/231, train_loss: 0.3337\n",
      "204/231, train_loss: 0.2368\n",
      "205/231, train_loss: 0.3501\n",
      "206/231, train_loss: 0.3174\n",
      "207/231, train_loss: 0.2922\n",
      "208/231, train_loss: 0.4111\n",
      "209/231, train_loss: 0.4592\n",
      "210/231, train_loss: 0.3025\n",
      "211/231, train_loss: 0.2671\n",
      "212/231, train_loss: 0.2402\n",
      "213/231, train_loss: 0.2515\n",
      "214/231, train_loss: 0.2598\n",
      "215/231, train_loss: 0.2869\n",
      "216/231, train_loss: 0.2861\n",
      "217/231, train_loss: 0.2161\n",
      "218/231, train_loss: 0.5098\n",
      "219/231, train_loss: 0.1796\n",
      "220/231, train_loss: 0.3970\n",
      "221/231, train_loss: 0.1936\n",
      "222/231, train_loss: 0.1912\n",
      "223/231, train_loss: 0.2964\n",
      "224/231, train_loss: 0.1189\n",
      "225/231, train_loss: 0.3093\n",
      "226/231, train_loss: 0.3508\n",
      "227/231, train_loss: 0.6934\n",
      "228/231, train_loss: 0.4617\n",
      "229/231, train_loss: 0.2510\n",
      "230/231, train_loss: 0.3140\n",
      "231/231, train_loss: 0.2996\n",
      "232/231, train_loss: 0.2096\n",
      "epoch 8 average loss: 0.2873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 20:11:31 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 20:11:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 9/100\n",
      "1/231, train_loss: 0.3076\n",
      "2/231, train_loss: 0.3406\n",
      "3/231, train_loss: 0.2639\n",
      "4/231, train_loss: 0.2776\n",
      "5/231, train_loss: 0.1350\n",
      "6/231, train_loss: 0.2018\n",
      "7/231, train_loss: 0.1758\n",
      "8/231, train_loss: 0.1570\n",
      "9/231, train_loss: 0.1989\n",
      "10/231, train_loss: 0.2032\n",
      "11/231, train_loss: 0.2642\n",
      "12/231, train_loss: 0.1418\n",
      "13/231, train_loss: 0.3506\n",
      "14/231, train_loss: 0.2947\n",
      "15/231, train_loss: 0.2979\n",
      "16/231, train_loss: 0.2729\n",
      "17/231, train_loss: 0.4182\n",
      "18/231, train_loss: 0.1915\n",
      "19/231, train_loss: 0.3186\n",
      "20/231, train_loss: 0.2090\n",
      "21/231, train_loss: 0.1821\n",
      "22/231, train_loss: 0.2361\n",
      "23/231, train_loss: 0.2600\n",
      "24/231, train_loss: 0.2842\n",
      "25/231, train_loss: 0.2949\n",
      "26/231, train_loss: 0.2388\n",
      "27/231, train_loss: 0.1378\n",
      "28/231, train_loss: 0.5142\n",
      "29/231, train_loss: 0.1665\n",
      "30/231, train_loss: 0.2581\n",
      "31/231, train_loss: 0.2959\n",
      "32/231, train_loss: 0.2200\n",
      "33/231, train_loss: 0.2329\n",
      "34/231, train_loss: 0.4436\n",
      "35/231, train_loss: 0.2222\n",
      "36/231, train_loss: 0.5039\n",
      "37/231, train_loss: 0.2893\n",
      "38/231, train_loss: 0.3875\n",
      "39/231, train_loss: 0.3645\n",
      "40/231, train_loss: 0.2344\n",
      "41/231, train_loss: 0.1254\n",
      "42/231, train_loss: 0.2375\n",
      "43/231, train_loss: 0.3936\n",
      "44/231, train_loss: 0.3828\n",
      "45/231, train_loss: 0.4534\n",
      "46/231, train_loss: 0.3335\n",
      "47/231, train_loss: 0.1282\n",
      "48/231, train_loss: 0.2183\n",
      "49/231, train_loss: 0.1873\n",
      "50/231, train_loss: 0.1595\n",
      "51/231, train_loss: 0.2710\n",
      "52/231, train_loss: 0.3796\n",
      "53/231, train_loss: 0.3174\n",
      "54/231, train_loss: 0.3186\n",
      "55/231, train_loss: 0.6377\n",
      "56/231, train_loss: 0.2744\n",
      "57/231, train_loss: 0.2074\n",
      "58/231, train_loss: 0.2837\n",
      "59/231, train_loss: 0.2273\n",
      "60/231, train_loss: 0.3245\n",
      "61/231, train_loss: 0.1454\n",
      "62/231, train_loss: 0.1769\n",
      "63/231, train_loss: 0.3186\n",
      "64/231, train_loss: 0.2418\n",
      "65/231, train_loss: 0.3049\n",
      "66/231, train_loss: 0.1575\n",
      "67/231, train_loss: 0.2251\n",
      "68/231, train_loss: 0.3965\n",
      "69/231, train_loss: 0.2140\n",
      "70/231, train_loss: 0.3108\n",
      "71/231, train_loss: 0.2319\n",
      "72/231, train_loss: 0.2063\n",
      "73/231, train_loss: 0.2710\n",
      "74/231, train_loss: 0.1835\n",
      "75/231, train_loss: 0.1548\n",
      "76/231, train_loss: 0.1263\n",
      "77/231, train_loss: 0.5342\n",
      "78/231, train_loss: 0.2600\n",
      "79/231, train_loss: 0.2375\n",
      "80/231, train_loss: 0.2622\n",
      "81/231, train_loss: 0.2856\n",
      "82/231, train_loss: 0.4158\n",
      "83/231, train_loss: 0.1923\n",
      "84/231, train_loss: 0.3613\n",
      "85/231, train_loss: 0.5210\n",
      "86/231, train_loss: 0.1830\n",
      "87/231, train_loss: 0.3657\n",
      "88/231, train_loss: 0.4702\n",
      "89/231, train_loss: 0.1892\n",
      "90/231, train_loss: 0.1899\n",
      "91/231, train_loss: 0.5605\n",
      "92/231, train_loss: 0.2742\n",
      "93/231, train_loss: 0.1981\n",
      "94/231, train_loss: 0.1116\n",
      "95/231, train_loss: 0.1954\n",
      "96/231, train_loss: 0.3066\n",
      "97/231, train_loss: 0.1494\n",
      "98/231, train_loss: 0.1887\n",
      "99/231, train_loss: 0.2350\n",
      "100/231, train_loss: 0.2632\n",
      "101/231, train_loss: 0.3545\n",
      "102/231, train_loss: 0.2837\n",
      "103/231, train_loss: 0.3223\n",
      "104/231, train_loss: 0.2424\n",
      "105/231, train_loss: 0.2869\n",
      "106/231, train_loss: 0.1433\n",
      "107/231, train_loss: 0.1841\n",
      "108/231, train_loss: 0.1589\n",
      "109/231, train_loss: 0.2625\n",
      "110/231, train_loss: 0.2903\n",
      "111/231, train_loss: 0.4919\n",
      "112/231, train_loss: 0.5386\n",
      "113/231, train_loss: 0.3364\n",
      "114/231, train_loss: 0.2168\n",
      "115/231, train_loss: 0.1167\n",
      "116/231, train_loss: 0.3262\n",
      "117/231, train_loss: 0.4509\n",
      "118/231, train_loss: 0.2595\n",
      "119/231, train_loss: 0.2812\n",
      "120/231, train_loss: 0.5493\n",
      "121/231, train_loss: 0.3225\n",
      "122/231, train_loss: 0.3315\n",
      "123/231, train_loss: 0.1881\n",
      "124/231, train_loss: 0.3479\n",
      "125/231, train_loss: 0.2783\n",
      "126/231, train_loss: 0.3782\n",
      "127/231, train_loss: 0.5205\n",
      "128/231, train_loss: 0.2454\n",
      "129/231, train_loss: 0.1227\n",
      "130/231, train_loss: 0.2603\n",
      "131/231, train_loss: 0.1519\n",
      "132/231, train_loss: 0.2698\n",
      "133/231, train_loss: 0.2827\n",
      "134/231, train_loss: 0.1708\n",
      "135/231, train_loss: 0.2644\n",
      "136/231, train_loss: 0.3633\n",
      "137/231, train_loss: 0.1855\n",
      "138/231, train_loss: 0.3071\n",
      "139/231, train_loss: 0.6587\n",
      "140/231, train_loss: 0.5444\n",
      "141/231, train_loss: 0.1681\n",
      "142/231, train_loss: 0.3809\n",
      "143/231, train_loss: 0.3220\n",
      "144/231, train_loss: 0.3577\n",
      "145/231, train_loss: 0.2388\n",
      "146/231, train_loss: 0.4497\n",
      "147/231, train_loss: 0.3738\n",
      "148/231, train_loss: 0.2288\n",
      "149/231, train_loss: 0.4856\n",
      "150/231, train_loss: 0.3049\n",
      "151/231, train_loss: 0.3716\n",
      "152/231, train_loss: 0.3872\n",
      "153/231, train_loss: 0.3359\n",
      "154/231, train_loss: 0.1641\n",
      "155/231, train_loss: 0.2279\n",
      "156/231, train_loss: 0.2024\n",
      "157/231, train_loss: 0.2295\n",
      "158/231, train_loss: 0.2832\n",
      "159/231, train_loss: 0.2045\n",
      "160/231, train_loss: 0.1819\n",
      "161/231, train_loss: 0.3228\n",
      "162/231, train_loss: 0.3369\n",
      "163/231, train_loss: 0.1943\n",
      "164/231, train_loss: 0.3506\n",
      "165/231, train_loss: 0.6240\n",
      "166/231, train_loss: 0.3818\n",
      "167/231, train_loss: 0.2886\n",
      "168/231, train_loss: 0.3521\n",
      "169/231, train_loss: 0.3821\n",
      "170/231, train_loss: 0.3896\n",
      "171/231, train_loss: 0.2095\n",
      "172/231, train_loss: 0.2812\n",
      "173/231, train_loss: 0.2357\n",
      "174/231, train_loss: 0.1484\n",
      "175/231, train_loss: 0.2251\n",
      "176/231, train_loss: 0.2512\n",
      "177/231, train_loss: 0.2128\n",
      "178/231, train_loss: 0.2793\n",
      "179/231, train_loss: 0.4116\n",
      "180/231, train_loss: 0.3662\n",
      "181/231, train_loss: 0.2399\n",
      "182/231, train_loss: 0.2324\n",
      "183/231, train_loss: 0.2534\n",
      "184/231, train_loss: 0.2024\n",
      "185/231, train_loss: 0.2263\n",
      "186/231, train_loss: 0.1931\n",
      "187/231, train_loss: 0.2278\n",
      "188/231, train_loss: 0.2235\n",
      "189/231, train_loss: 0.2632\n",
      "190/231, train_loss: 0.2698\n",
      "191/231, train_loss: 0.3079\n",
      "192/231, train_loss: 0.2571\n",
      "193/231, train_loss: 0.4583\n",
      "194/231, train_loss: 0.3445\n",
      "195/231, train_loss: 0.3262\n",
      "196/231, train_loss: 0.1350\n",
      "197/231, train_loss: 0.3342\n",
      "198/231, train_loss: 0.1833\n",
      "199/231, train_loss: 0.2052\n",
      "200/231, train_loss: 0.4536\n",
      "201/231, train_loss: 0.4463\n",
      "202/231, train_loss: 0.1479\n",
      "203/231, train_loss: 0.2437\n",
      "204/231, train_loss: 0.3230\n",
      "205/231, train_loss: 0.1985\n",
      "206/231, train_loss: 0.2217\n",
      "207/231, train_loss: 0.1329\n",
      "208/231, train_loss: 0.3682\n",
      "209/231, train_loss: 0.3760\n",
      "210/231, train_loss: 0.1160\n",
      "211/231, train_loss: 0.3186\n",
      "212/231, train_loss: 0.5210\n",
      "213/231, train_loss: 0.1970\n",
      "214/231, train_loss: 0.2452\n",
      "215/231, train_loss: 0.1185\n",
      "216/231, train_loss: 0.2058\n",
      "217/231, train_loss: 0.2693\n",
      "218/231, train_loss: 0.2307\n",
      "219/231, train_loss: 0.2651\n",
      "220/231, train_loss: 0.1631\n",
      "221/231, train_loss: 0.3689\n",
      "222/231, train_loss: 0.1432\n",
      "223/231, train_loss: 0.1281\n",
      "224/231, train_loss: 0.1685\n",
      "225/231, train_loss: 0.0893\n",
      "226/231, train_loss: 0.3901\n",
      "227/231, train_loss: 0.1921\n",
      "228/231, train_loss: 0.2379\n",
      "229/231, train_loss: 0.4053\n",
      "230/231, train_loss: 0.2690\n",
      "231/231, train_loss: 0.1726\n",
      "232/231, train_loss: 0.3384\n",
      "epoch 9 average loss: 0.2804\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 20:58:43 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 20:58:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 10/100\n",
      "1/231, train_loss: 0.3071\n",
      "2/231, train_loss: 0.3721\n",
      "3/231, train_loss: 0.2798\n",
      "4/231, train_loss: 0.2788\n",
      "5/231, train_loss: 0.2395\n",
      "6/231, train_loss: 0.2061\n",
      "7/231, train_loss: 0.1257\n",
      "8/231, train_loss: 0.2610\n",
      "9/231, train_loss: 0.2808\n",
      "10/231, train_loss: 0.1406\n",
      "11/231, train_loss: 0.1323\n",
      "12/231, train_loss: 0.3625\n",
      "13/231, train_loss: 0.2539\n",
      "14/231, train_loss: 0.1943\n",
      "15/231, train_loss: 0.3708\n",
      "16/231, train_loss: 0.2688\n",
      "17/231, train_loss: 0.2478\n",
      "18/231, train_loss: 0.1790\n",
      "19/231, train_loss: 0.3706\n",
      "20/231, train_loss: 0.1614\n",
      "21/231, train_loss: 0.1897\n",
      "22/231, train_loss: 0.4207\n",
      "23/231, train_loss: 0.2283\n",
      "24/231, train_loss: 0.2224\n",
      "25/231, train_loss: 0.2368\n",
      "26/231, train_loss: 0.3867\n",
      "27/231, train_loss: 0.5596\n",
      "28/231, train_loss: 0.1899\n",
      "29/231, train_loss: 0.4404\n",
      "30/231, train_loss: 0.2817\n",
      "31/231, train_loss: 0.3613\n",
      "32/231, train_loss: 0.2520\n",
      "33/231, train_loss: 0.4055\n",
      "34/231, train_loss: 0.2006\n",
      "35/231, train_loss: 0.2920\n",
      "36/231, train_loss: 0.2029\n",
      "37/231, train_loss: 0.2118\n",
      "38/231, train_loss: 0.2178\n",
      "39/231, train_loss: 0.2603\n",
      "40/231, train_loss: 0.2336\n",
      "41/231, train_loss: 0.2203\n",
      "42/231, train_loss: 0.2622\n",
      "43/231, train_loss: 0.1337\n",
      "44/231, train_loss: 0.1154\n",
      "45/231, train_loss: 0.4077\n",
      "46/231, train_loss: 0.2554\n",
      "47/231, train_loss: 0.2395\n",
      "48/231, train_loss: 0.4080\n",
      "49/231, train_loss: 0.3577\n",
      "50/231, train_loss: 0.2839\n",
      "51/231, train_loss: 0.6318\n",
      "52/231, train_loss: 0.1726\n",
      "53/231, train_loss: 0.3770\n",
      "54/231, train_loss: 0.2700\n",
      "55/231, train_loss: 0.1671\n",
      "56/231, train_loss: 0.3540\n",
      "57/231, train_loss: 0.3604\n",
      "58/231, train_loss: 0.4299\n",
      "59/231, train_loss: 0.1619\n",
      "60/231, train_loss: 0.1841\n",
      "61/231, train_loss: 0.2546\n",
      "62/231, train_loss: 0.1576\n",
      "63/231, train_loss: 0.5190\n",
      "64/231, train_loss: 0.6304\n",
      "65/231, train_loss: 0.2064\n",
      "66/231, train_loss: 0.4617\n",
      "67/231, train_loss: 0.2241\n",
      "68/231, train_loss: 0.2900\n",
      "69/231, train_loss: 0.2147\n",
      "70/231, train_loss: 0.2385\n",
      "71/231, train_loss: 0.2209\n",
      "72/231, train_loss: 0.1543\n",
      "73/231, train_loss: 0.1630\n",
      "74/231, train_loss: 0.2198\n",
      "75/231, train_loss: 0.1731\n",
      "76/231, train_loss: 0.4092\n",
      "77/231, train_loss: 0.2715\n",
      "78/231, train_loss: 0.1995\n",
      "79/231, train_loss: 0.2485\n",
      "80/231, train_loss: 0.2563\n",
      "81/231, train_loss: 0.1599\n",
      "82/231, train_loss: 0.1809\n",
      "83/231, train_loss: 0.2810\n",
      "84/231, train_loss: 0.2173\n",
      "85/231, train_loss: 0.2529\n",
      "86/231, train_loss: 0.0910\n",
      "87/231, train_loss: 0.3069\n",
      "88/231, train_loss: 0.2120\n",
      "89/231, train_loss: 0.2615\n",
      "90/231, train_loss: 0.1492\n",
      "91/231, train_loss: 0.1738\n",
      "92/231, train_loss: 0.3162\n",
      "93/231, train_loss: 0.2085\n",
      "94/231, train_loss: 0.1449\n",
      "95/231, train_loss: 0.4709\n",
      "96/231, train_loss: 0.1948\n",
      "97/231, train_loss: 0.4360\n",
      "98/231, train_loss: 0.1354\n",
      "99/231, train_loss: 0.3079\n",
      "100/231, train_loss: 0.1782\n",
      "101/231, train_loss: 0.1370\n",
      "102/231, train_loss: 0.2871\n",
      "103/231, train_loss: 0.1593\n",
      "104/231, train_loss: 0.3354\n",
      "105/231, train_loss: 0.2827\n",
      "106/231, train_loss: 0.3176\n",
      "107/231, train_loss: 0.3252\n",
      "108/231, train_loss: 0.2559\n",
      "109/231, train_loss: 0.2268\n",
      "110/231, train_loss: 0.1953\n",
      "111/231, train_loss: 0.1555\n",
      "112/231, train_loss: 0.4062\n",
      "113/231, train_loss: 0.3872\n",
      "114/231, train_loss: 0.4204\n",
      "115/231, train_loss: 0.2278\n",
      "116/231, train_loss: 0.2028\n",
      "117/231, train_loss: 0.2089\n",
      "118/231, train_loss: 0.6226\n",
      "119/231, train_loss: 0.6118\n",
      "120/231, train_loss: 0.1187\n",
      "121/231, train_loss: 0.2178\n",
      "122/231, train_loss: 0.2661\n",
      "123/231, train_loss: 0.2357\n",
      "124/231, train_loss: 0.3250\n",
      "125/231, train_loss: 0.2751\n",
      "126/231, train_loss: 0.1976\n",
      "127/231, train_loss: 0.3584\n",
      "128/231, train_loss: 0.2612\n",
      "129/231, train_loss: 0.3313\n",
      "130/231, train_loss: 0.1637\n",
      "131/231, train_loss: 0.2057\n",
      "132/231, train_loss: 0.4209\n",
      "133/231, train_loss: 0.2642\n",
      "134/231, train_loss: 0.3850\n",
      "135/231, train_loss: 0.2418\n",
      "136/231, train_loss: 0.2703\n",
      "137/231, train_loss: 0.2242\n",
      "138/231, train_loss: 0.2036\n",
      "139/231, train_loss: 0.2622\n",
      "140/231, train_loss: 0.4211\n",
      "141/231, train_loss: 0.3887\n",
      "142/231, train_loss: 0.3125\n",
      "143/231, train_loss: 0.5508\n",
      "144/231, train_loss: 0.2910\n",
      "145/231, train_loss: 0.5688\n",
      "146/231, train_loss: 0.3516\n",
      "147/231, train_loss: 0.2163\n",
      "148/231, train_loss: 0.2708\n",
      "149/231, train_loss: 0.4072\n",
      "150/231, train_loss: 0.2207\n",
      "151/231, train_loss: 0.1698\n",
      "152/231, train_loss: 0.5586\n",
      "153/231, train_loss: 0.1600\n",
      "154/231, train_loss: 0.3315\n",
      "155/231, train_loss: 0.3760\n",
      "156/231, train_loss: 0.1636\n",
      "157/231, train_loss: 0.2452\n",
      "158/231, train_loss: 0.1192\n",
      "159/231, train_loss: 0.1958\n",
      "160/231, train_loss: 0.4565\n",
      "161/231, train_loss: 0.1577\n",
      "162/231, train_loss: 0.1682\n",
      "163/231, train_loss: 0.3132\n",
      "164/231, train_loss: 0.1985\n",
      "165/231, train_loss: 0.1710\n",
      "166/231, train_loss: 0.3669\n",
      "167/231, train_loss: 0.4956\n",
      "168/231, train_loss: 0.3896\n",
      "169/231, train_loss: 0.2383\n",
      "170/231, train_loss: 0.1353\n",
      "171/231, train_loss: 0.2556\n",
      "172/231, train_loss: 0.1359\n",
      "173/231, train_loss: 0.1987\n",
      "174/231, train_loss: 0.1667\n",
      "175/231, train_loss: 0.3149\n",
      "176/231, train_loss: 0.2477\n",
      "177/231, train_loss: 0.1400\n",
      "178/231, train_loss: 0.3152\n",
      "179/231, train_loss: 0.2510\n",
      "180/231, train_loss: 0.5645\n",
      "181/231, train_loss: 0.2371\n",
      "182/231, train_loss: 0.3381\n",
      "183/231, train_loss: 0.3794\n",
      "184/231, train_loss: 0.4504\n",
      "185/231, train_loss: 0.5176\n",
      "186/231, train_loss: 0.7070\n",
      "187/231, train_loss: 0.2869\n",
      "188/231, train_loss: 0.1932\n",
      "189/231, train_loss: 0.4016\n",
      "190/231, train_loss: 0.1648\n",
      "191/231, train_loss: 0.2852\n",
      "192/231, train_loss: 0.2184\n",
      "193/231, train_loss: 0.1302\n",
      "194/231, train_loss: 0.2341\n",
      "195/231, train_loss: 0.4409\n",
      "196/231, train_loss: 0.2292\n",
      "197/231, train_loss: 0.2729\n",
      "198/231, train_loss: 0.2529\n",
      "199/231, train_loss: 0.4595\n",
      "200/231, train_loss: 0.2179\n",
      "201/231, train_loss: 0.2335\n",
      "202/231, train_loss: 0.1445\n",
      "203/231, train_loss: 0.2104\n",
      "204/231, train_loss: 0.3081\n",
      "205/231, train_loss: 0.3440\n",
      "206/231, train_loss: 0.1558\n",
      "207/231, train_loss: 0.3140\n",
      "208/231, train_loss: 0.2051\n",
      "209/231, train_loss: 0.3245\n",
      "210/231, train_loss: 0.2146\n",
      "211/231, train_loss: 0.1190\n",
      "212/231, train_loss: 0.3020\n",
      "213/231, train_loss: 0.1736\n",
      "214/231, train_loss: 0.2349\n",
      "215/231, train_loss: 0.2419\n",
      "216/231, train_loss: 0.3535\n",
      "217/231, train_loss: 0.1760\n",
      "218/231, train_loss: 0.3782\n",
      "219/231, train_loss: 0.4951\n",
      "220/231, train_loss: 0.2169\n",
      "221/231, train_loss: 0.2404\n",
      "222/231, train_loss: 0.2793\n",
      "223/231, train_loss: 0.2905\n",
      "224/231, train_loss: 0.2722\n",
      "225/231, train_loss: 0.3789\n",
      "226/231, train_loss: 0.1772\n",
      "227/231, train_loss: 0.1379\n",
      "228/231, train_loss: 0.2634\n",
      "229/231, train_loss: 0.4619\n",
      "230/231, train_loss: 0.2129\n",
      "231/231, train_loss: 0.2380\n",
      "232/231, train_loss: 0.3438\n",
      "epoch 10 average loss: 0.2787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 21:43:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 21:43:48 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.34051997863920147, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.34051997863920147, 'AP_IoU_0.10_MaxDet_100': 0.41007043397293824, 'nodule_AP_IoU_0.10_MaxDet_100': 0.41007043397293824, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8461538553237915, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8461538553237915, 'AR_IoU_0.10_MaxDet_100': 0.9743589758872986, 'nodule_AR_IoU_0.10_MaxDet_100': 0.9743589758872986}\n",
      "current epoch: 10 current metric: 0.6428 best metric: 0.6777 at epoch 5\n",
      "----------\n",
      "epoch 11/100\n",
      "1/231, train_loss: 0.1783\n",
      "2/231, train_loss: 0.2240\n",
      "3/231, train_loss: 0.3105\n",
      "4/231, train_loss: 0.3149\n",
      "5/231, train_loss: 0.1407\n",
      "6/231, train_loss: 0.2100\n",
      "7/231, train_loss: 0.1687\n",
      "8/231, train_loss: 0.3027\n",
      "9/231, train_loss: 0.1543\n",
      "10/231, train_loss: 0.1208\n",
      "11/231, train_loss: 0.2317\n",
      "12/231, train_loss: 0.2288\n",
      "13/231, train_loss: 0.3013\n",
      "14/231, train_loss: 0.1106\n",
      "15/231, train_loss: 0.2600\n",
      "16/231, train_loss: 0.1528\n",
      "17/231, train_loss: 0.3640\n",
      "18/231, train_loss: 0.5298\n",
      "19/231, train_loss: 0.1934\n",
      "20/231, train_loss: 0.2367\n",
      "21/231, train_loss: 0.2878\n",
      "22/231, train_loss: 0.2803\n",
      "23/231, train_loss: 0.1600\n",
      "24/231, train_loss: 0.4656\n",
      "25/231, train_loss: 0.2832\n",
      "26/231, train_loss: 0.2539\n",
      "27/231, train_loss: 0.2588\n",
      "28/231, train_loss: 0.3938\n",
      "29/231, train_loss: 0.1492\n",
      "30/231, train_loss: 0.2026\n",
      "31/231, train_loss: 0.2021\n",
      "32/231, train_loss: 0.1010\n",
      "33/231, train_loss: 0.1750\n",
      "34/231, train_loss: 0.5010\n",
      "35/231, train_loss: 0.2727\n",
      "36/231, train_loss: 0.4829\n",
      "37/231, train_loss: 0.2083\n",
      "38/231, train_loss: 0.1855\n",
      "39/231, train_loss: 0.1270\n",
      "40/231, train_loss: 0.4324\n",
      "41/231, train_loss: 0.2461\n",
      "42/231, train_loss: 0.1403\n",
      "43/231, train_loss: 0.1658\n",
      "44/231, train_loss: 0.1401\n",
      "45/231, train_loss: 0.1316\n",
      "46/231, train_loss: 0.1509\n",
      "47/231, train_loss: 0.0983\n",
      "48/231, train_loss: 0.1895\n",
      "49/231, train_loss: 0.2190\n",
      "50/231, train_loss: 0.6689\n",
      "51/231, train_loss: 0.1564\n",
      "52/231, train_loss: 0.2069\n",
      "53/231, train_loss: 0.1299\n",
      "54/231, train_loss: 0.0729\n",
      "55/231, train_loss: 0.3325\n",
      "56/231, train_loss: 0.1168\n",
      "57/231, train_loss: 0.2056\n",
      "58/231, train_loss: 0.1488\n",
      "59/231, train_loss: 0.3242\n",
      "60/231, train_loss: 0.1777\n",
      "61/231, train_loss: 0.2036\n",
      "62/231, train_loss: 0.2271\n",
      "63/231, train_loss: 0.0981\n",
      "64/231, train_loss: 0.3569\n",
      "65/231, train_loss: 0.2566\n",
      "66/231, train_loss: 0.2410\n",
      "67/231, train_loss: 0.2856\n",
      "68/231, train_loss: 0.2581\n",
      "69/231, train_loss: 0.3032\n",
      "70/231, train_loss: 0.2678\n",
      "71/231, train_loss: 0.4307\n",
      "72/231, train_loss: 0.2710\n",
      "73/231, train_loss: 0.3013\n",
      "74/231, train_loss: 0.2391\n",
      "75/231, train_loss: 0.2104\n",
      "76/231, train_loss: 0.4370\n",
      "77/231, train_loss: 0.3145\n",
      "78/231, train_loss: 0.2759\n",
      "79/231, train_loss: 0.3127\n",
      "80/231, train_loss: 0.3101\n",
      "81/231, train_loss: 0.2236\n",
      "82/231, train_loss: 0.1600\n",
      "83/231, train_loss: 0.2910\n",
      "84/231, train_loss: 0.3662\n",
      "85/231, train_loss: 0.3447\n",
      "86/231, train_loss: 0.3811\n",
      "87/231, train_loss: 0.0984\n",
      "88/231, train_loss: 0.5400\n",
      "89/231, train_loss: 0.2957\n",
      "90/231, train_loss: 0.4387\n",
      "91/231, train_loss: 0.2397\n",
      "92/231, train_loss: 0.1936\n",
      "93/231, train_loss: 0.4250\n",
      "94/231, train_loss: 0.2424\n",
      "95/231, train_loss: 0.2023\n",
      "96/231, train_loss: 0.5493\n",
      "97/231, train_loss: 0.1907\n",
      "98/231, train_loss: 0.2427\n",
      "99/231, train_loss: 0.3684\n",
      "100/231, train_loss: 0.3040\n",
      "101/231, train_loss: 0.3076\n",
      "102/231, train_loss: 0.3213\n",
      "103/231, train_loss: 0.2532\n",
      "104/231, train_loss: 0.2605\n",
      "105/231, train_loss: 0.2646\n",
      "106/231, train_loss: 0.4824\n",
      "107/231, train_loss: 0.1519\n",
      "108/231, train_loss: 0.3013\n",
      "109/231, train_loss: 0.3474\n",
      "110/231, train_loss: 0.3459\n",
      "111/231, train_loss: 0.2139\n",
      "112/231, train_loss: 0.4121\n",
      "113/231, train_loss: 0.1443\n",
      "114/231, train_loss: 0.2191\n",
      "115/231, train_loss: 0.5586\n",
      "116/231, train_loss: 0.2759\n",
      "117/231, train_loss: 0.2004\n",
      "118/231, train_loss: 0.1550\n",
      "119/231, train_loss: 0.1667\n",
      "120/231, train_loss: 0.1416\n",
      "121/231, train_loss: 0.1262\n",
      "122/231, train_loss: 0.2412\n",
      "123/231, train_loss: 0.3896\n",
      "124/231, train_loss: 0.3540\n",
      "125/231, train_loss: 0.3184\n",
      "126/231, train_loss: 0.1075\n",
      "127/231, train_loss: 0.2366\n",
      "128/231, train_loss: 0.1278\n",
      "129/231, train_loss: 0.1711\n",
      "130/231, train_loss: 0.1356\n",
      "131/231, train_loss: 0.2363\n",
      "132/231, train_loss: 0.1163\n",
      "133/231, train_loss: 0.2515\n",
      "134/231, train_loss: 0.1633\n",
      "135/231, train_loss: 0.3384\n",
      "136/231, train_loss: 0.2725\n",
      "137/231, train_loss: 0.2566\n",
      "138/231, train_loss: 0.2067\n",
      "139/231, train_loss: 0.1361\n",
      "140/231, train_loss: 0.2542\n",
      "141/231, train_loss: 0.4719\n",
      "142/231, train_loss: 0.2573\n",
      "143/231, train_loss: 0.2184\n",
      "144/231, train_loss: 0.1560\n",
      "145/231, train_loss: 0.1687\n",
      "146/231, train_loss: 0.3003\n",
      "147/231, train_loss: 0.1382\n",
      "148/231, train_loss: 0.1229\n",
      "149/231, train_loss: 0.4226\n",
      "150/231, train_loss: 0.1636\n",
      "151/231, train_loss: 0.1301\n",
      "152/231, train_loss: 0.3550\n",
      "153/231, train_loss: 0.2344\n",
      "154/231, train_loss: 0.3899\n",
      "155/231, train_loss: 0.5103\n",
      "156/231, train_loss: 0.1682\n",
      "157/231, train_loss: 0.1801\n",
      "158/231, train_loss: 0.1447\n",
      "159/231, train_loss: 0.0917\n",
      "160/231, train_loss: 0.5767\n",
      "161/231, train_loss: 0.3240\n",
      "162/231, train_loss: 0.4189\n",
      "163/231, train_loss: 0.1438\n",
      "164/231, train_loss: 0.3601\n",
      "165/231, train_loss: 0.3538\n",
      "166/231, train_loss: 0.1816\n",
      "167/231, train_loss: 0.1079\n",
      "168/231, train_loss: 0.3289\n",
      "169/231, train_loss: 0.3293\n",
      "170/231, train_loss: 0.1814\n",
      "171/231, train_loss: 0.2378\n",
      "172/231, train_loss: 0.3701\n",
      "173/231, train_loss: 0.3503\n",
      "174/231, train_loss: 0.1978\n",
      "175/231, train_loss: 0.2576\n",
      "176/231, train_loss: 0.3000\n",
      "177/231, train_loss: 0.1346\n",
      "178/231, train_loss: 0.1538\n",
      "179/231, train_loss: 0.2197\n",
      "180/231, train_loss: 0.1261\n",
      "181/231, train_loss: 0.1918\n",
      "182/231, train_loss: 0.1478\n",
      "183/231, train_loss: 0.2080\n",
      "184/231, train_loss: 0.3257\n",
      "185/231, train_loss: 0.2893\n",
      "186/231, train_loss: 0.4187\n",
      "187/231, train_loss: 0.3608\n",
      "188/231, train_loss: 0.1891\n",
      "189/231, train_loss: 0.3838\n",
      "190/231, train_loss: 0.3667\n",
      "191/231, train_loss: 0.3030\n",
      "192/231, train_loss: 0.3101\n",
      "193/231, train_loss: 0.2246\n",
      "194/231, train_loss: 0.2346\n",
      "195/231, train_loss: 0.1587\n",
      "196/231, train_loss: 0.2891\n",
      "197/231, train_loss: 0.2109\n",
      "198/231, train_loss: 0.4753\n",
      "199/231, train_loss: 0.3525\n",
      "200/231, train_loss: 0.2180\n",
      "201/231, train_loss: 0.2185\n",
      "202/231, train_loss: 0.3030\n",
      "203/231, train_loss: 0.1633\n",
      "204/231, train_loss: 0.2009\n",
      "205/231, train_loss: 0.3215\n",
      "206/231, train_loss: 0.2498\n",
      "207/231, train_loss: 0.5859\n",
      "208/231, train_loss: 0.1671\n",
      "209/231, train_loss: 0.2783\n",
      "210/231, train_loss: 0.3210\n",
      "211/231, train_loss: 0.1760\n",
      "212/231, train_loss: 0.4661\n",
      "213/231, train_loss: 0.2179\n",
      "214/231, train_loss: 0.2549\n",
      "215/231, train_loss: 0.1898\n",
      "216/231, train_loss: 0.1680\n",
      "217/231, train_loss: 0.2054\n",
      "218/231, train_loss: 0.2200\n",
      "219/231, train_loss: 0.2144\n",
      "220/231, train_loss: 0.2444\n",
      "221/231, train_loss: 0.2061\n",
      "222/231, train_loss: 0.2416\n",
      "223/231, train_loss: 0.2700\n",
      "224/231, train_loss: 0.1692\n",
      "225/231, train_loss: 0.3230\n",
      "226/231, train_loss: 0.1357\n",
      "227/231, train_loss: 0.1843\n",
      "228/231, train_loss: 0.1921\n",
      "229/231, train_loss: 0.2461\n",
      "230/231, train_loss: 0.3145\n",
      "231/231, train_loss: 0.2410\n",
      "232/231, train_loss: 0.1160\n",
      "epoch 11 average loss: 0.2568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 22:41:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 22:41:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 12/100\n",
      "1/231, train_loss: 0.2817\n",
      "2/231, train_loss: 0.2559\n",
      "3/231, train_loss: 0.1636\n",
      "4/231, train_loss: 0.1694\n",
      "5/231, train_loss: 0.3584\n",
      "6/231, train_loss: 0.3564\n",
      "7/231, train_loss: 0.2020\n",
      "8/231, train_loss: 0.3020\n",
      "9/231, train_loss: 0.2395\n",
      "10/231, train_loss: 0.2573\n",
      "11/231, train_loss: 0.3303\n",
      "12/231, train_loss: 0.1456\n",
      "13/231, train_loss: 0.1172\n",
      "14/231, train_loss: 0.1741\n",
      "15/231, train_loss: 0.3447\n",
      "16/231, train_loss: 0.3291\n",
      "17/231, train_loss: 0.3745\n",
      "18/231, train_loss: 0.1796\n",
      "19/231, train_loss: 0.2869\n",
      "20/231, train_loss: 0.2202\n",
      "21/231, train_loss: 0.1521\n",
      "22/231, train_loss: 0.2102\n",
      "23/231, train_loss: 0.2478\n",
      "24/231, train_loss: 0.3657\n",
      "25/231, train_loss: 0.2944\n",
      "26/231, train_loss: 0.1521\n",
      "27/231, train_loss: 0.2227\n",
      "28/231, train_loss: 0.3315\n",
      "29/231, train_loss: 0.2209\n",
      "30/231, train_loss: 0.1882\n",
      "31/231, train_loss: 0.3318\n",
      "32/231, train_loss: 0.2515\n",
      "33/231, train_loss: 0.1220\n",
      "34/231, train_loss: 0.1787\n",
      "35/231, train_loss: 0.2959\n",
      "36/231, train_loss: 0.1833\n",
      "37/231, train_loss: 0.1782\n",
      "38/231, train_loss: 0.2234\n",
      "39/231, train_loss: 0.2222\n",
      "40/231, train_loss: 0.2698\n",
      "41/231, train_loss: 0.3381\n",
      "42/231, train_loss: 0.1562\n",
      "43/231, train_loss: 0.1821\n",
      "44/231, train_loss: 0.0933\n",
      "45/231, train_loss: 0.2104\n",
      "46/231, train_loss: 0.1309\n",
      "47/231, train_loss: 0.2211\n",
      "48/231, train_loss: 0.4080\n",
      "49/231, train_loss: 0.1318\n",
      "50/231, train_loss: 0.1414\n",
      "51/231, train_loss: 0.2661\n",
      "52/231, train_loss: 0.1909\n",
      "53/231, train_loss: 0.1354\n",
      "54/231, train_loss: 0.1519\n",
      "55/231, train_loss: 0.3911\n",
      "56/231, train_loss: 0.1740\n",
      "57/231, train_loss: 0.5898\n",
      "58/231, train_loss: 0.2227\n",
      "59/231, train_loss: 0.2141\n",
      "60/231, train_loss: 0.2034\n",
      "61/231, train_loss: 0.2700\n",
      "62/231, train_loss: 0.3213\n",
      "63/231, train_loss: 0.3357\n",
      "64/231, train_loss: 0.2864\n",
      "65/231, train_loss: 0.3960\n",
      "66/231, train_loss: 0.1677\n",
      "67/231, train_loss: 0.1262\n",
      "68/231, train_loss: 0.3591\n",
      "69/231, train_loss: 0.3381\n",
      "70/231, train_loss: 0.1675\n",
      "71/231, train_loss: 0.3269\n",
      "72/231, train_loss: 0.2134\n",
      "73/231, train_loss: 0.1426\n",
      "74/231, train_loss: 0.2590\n",
      "75/231, train_loss: 0.3794\n",
      "76/231, train_loss: 0.4417\n",
      "77/231, train_loss: 0.2461\n",
      "78/231, train_loss: 0.1942\n",
      "79/231, train_loss: 0.1608\n",
      "80/231, train_loss: 0.2008\n",
      "81/231, train_loss: 0.1401\n",
      "82/231, train_loss: 0.1292\n",
      "83/231, train_loss: 0.2705\n",
      "84/231, train_loss: 0.2057\n",
      "85/231, train_loss: 0.2489\n",
      "86/231, train_loss: 0.1719\n",
      "87/231, train_loss: 0.1931\n",
      "88/231, train_loss: 0.1792\n",
      "89/231, train_loss: 0.2871\n",
      "90/231, train_loss: 0.1277\n",
      "91/231, train_loss: 0.1632\n",
      "92/231, train_loss: 0.2893\n",
      "93/231, train_loss: 0.1868\n",
      "94/231, train_loss: 0.2749\n",
      "95/231, train_loss: 0.3450\n",
      "96/231, train_loss: 0.1687\n",
      "97/231, train_loss: 0.2249\n",
      "98/231, train_loss: 0.2998\n",
      "99/231, train_loss: 0.2045\n",
      "100/231, train_loss: 0.2910\n",
      "101/231, train_loss: 0.2998\n",
      "102/231, train_loss: 0.1385\n",
      "103/231, train_loss: 0.1514\n",
      "104/231, train_loss: 0.2720\n",
      "105/231, train_loss: 0.1831\n",
      "106/231, train_loss: 0.1533\n",
      "107/231, train_loss: 0.6895\n",
      "108/231, train_loss: 0.2566\n",
      "109/231, train_loss: 0.1279\n",
      "110/231, train_loss: 0.1504\n",
      "111/231, train_loss: 0.3286\n",
      "112/231, train_loss: 0.1409\n",
      "113/231, train_loss: 0.1052\n",
      "114/231, train_loss: 0.1683\n",
      "115/231, train_loss: 0.1316\n",
      "116/231, train_loss: 0.1682\n",
      "117/231, train_loss: 0.2246\n",
      "118/231, train_loss: 0.4832\n",
      "119/231, train_loss: 0.3618\n",
      "120/231, train_loss: 0.2017\n",
      "121/231, train_loss: 0.1582\n",
      "122/231, train_loss: 0.2249\n",
      "123/231, train_loss: 0.1777\n",
      "124/231, train_loss: 0.2175\n",
      "125/231, train_loss: 0.2764\n",
      "126/231, train_loss: 0.2664\n",
      "127/231, train_loss: 0.3789\n",
      "128/231, train_loss: 0.2216\n",
      "129/231, train_loss: 0.2708\n",
      "130/231, train_loss: 0.1990\n",
      "131/231, train_loss: 0.2976\n",
      "132/231, train_loss: 0.1361\n",
      "133/231, train_loss: 0.1262\n",
      "134/231, train_loss: 0.2161\n",
      "135/231, train_loss: 0.3750\n",
      "136/231, train_loss: 0.1069\n",
      "137/231, train_loss: 0.3083\n",
      "138/231, train_loss: 0.1575\n",
      "139/231, train_loss: 0.3582\n",
      "140/231, train_loss: 0.2190\n",
      "141/231, train_loss: 0.1731\n",
      "142/231, train_loss: 0.3535\n",
      "143/231, train_loss: 0.1361\n",
      "144/231, train_loss: 0.2803\n",
      "145/231, train_loss: 0.1832\n",
      "146/231, train_loss: 0.2212\n",
      "147/231, train_loss: 0.1121\n",
      "148/231, train_loss: 0.1667\n",
      "149/231, train_loss: 0.1656\n",
      "150/231, train_loss: 0.1237\n",
      "151/231, train_loss: 0.3035\n",
      "152/231, train_loss: 0.1875\n",
      "153/231, train_loss: 0.4058\n",
      "154/231, train_loss: 0.2131\n",
      "155/231, train_loss: 0.2983\n",
      "156/231, train_loss: 0.4592\n",
      "157/231, train_loss: 0.2793\n",
      "158/231, train_loss: 0.2397\n",
      "159/231, train_loss: 0.2368\n",
      "160/231, train_loss: 0.1497\n",
      "161/231, train_loss: 0.3223\n",
      "162/231, train_loss: 0.4048\n",
      "163/231, train_loss: 0.1720\n",
      "164/231, train_loss: 0.4231\n",
      "165/231, train_loss: 0.2230\n",
      "166/231, train_loss: 0.1677\n",
      "167/231, train_loss: 0.1364\n",
      "168/231, train_loss: 0.2111\n",
      "169/231, train_loss: 0.1990\n",
      "170/231, train_loss: 0.3889\n",
      "171/231, train_loss: 0.1495\n",
      "172/231, train_loss: 0.2449\n",
      "173/231, train_loss: 0.1688\n",
      "174/231, train_loss: 0.2349\n",
      "175/231, train_loss: 0.1851\n",
      "176/231, train_loss: 0.2004\n",
      "177/231, train_loss: 0.3562\n",
      "178/231, train_loss: 0.0983\n",
      "179/231, train_loss: 0.2585\n",
      "180/231, train_loss: 0.1431\n",
      "181/231, train_loss: 0.3220\n",
      "182/231, train_loss: 0.3750\n",
      "183/231, train_loss: 0.1064\n",
      "184/231, train_loss: 0.2214\n",
      "185/231, train_loss: 0.2083\n",
      "186/231, train_loss: 0.2615\n",
      "187/231, train_loss: 0.5010\n",
      "188/231, train_loss: 0.1938\n",
      "189/231, train_loss: 0.2379\n",
      "190/231, train_loss: 0.2568\n",
      "191/231, train_loss: 0.3115\n",
      "192/231, train_loss: 0.2690\n",
      "193/231, train_loss: 0.2247\n",
      "194/231, train_loss: 0.1453\n",
      "195/231, train_loss: 0.3591\n",
      "196/231, train_loss: 0.5659\n",
      "197/231, train_loss: 0.1787\n",
      "198/231, train_loss: 0.2124\n",
      "199/231, train_loss: 0.2285\n",
      "200/231, train_loss: 0.1052\n",
      "201/231, train_loss: 0.3047\n",
      "202/231, train_loss: 0.2822\n",
      "203/231, train_loss: 0.1462\n",
      "204/231, train_loss: 0.3367\n",
      "205/231, train_loss: 0.2725\n",
      "206/231, train_loss: 0.2285\n",
      "207/231, train_loss: 0.4487\n",
      "208/231, train_loss: 0.2942\n",
      "209/231, train_loss: 0.3887\n",
      "210/231, train_loss: 0.1782\n",
      "211/231, train_loss: 0.1796\n",
      "212/231, train_loss: 0.1201\n",
      "213/231, train_loss: 0.1885\n",
      "214/231, train_loss: 0.1113\n",
      "215/231, train_loss: 0.2278\n",
      "216/231, train_loss: 0.1543\n",
      "217/231, train_loss: 0.2253\n",
      "218/231, train_loss: 0.1521\n",
      "219/231, train_loss: 0.1802\n",
      "220/231, train_loss: 0.1492\n",
      "221/231, train_loss: 0.2090\n",
      "222/231, train_loss: 0.1619\n",
      "223/231, train_loss: 0.1863\n",
      "224/231, train_loss: 0.2473\n",
      "225/231, train_loss: 0.1292\n",
      "226/231, train_loss: 0.1425\n",
      "227/231, train_loss: 0.3665\n",
      "228/231, train_loss: 0.2285\n",
      "229/231, train_loss: 0.2554\n",
      "230/231, train_loss: 0.2252\n",
      "231/231, train_loss: 0.3423\n",
      "232/231, train_loss: 0.1265\n",
      "epoch 12 average loss: 0.2383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/04 23:26:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/04 23:26:31 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 13/100\n",
      "1/231, train_loss: 0.1620\n",
      "2/231, train_loss: 0.3179\n",
      "3/231, train_loss: 0.1658\n",
      "4/231, train_loss: 0.4321\n",
      "5/231, train_loss: 0.1874\n",
      "6/231, train_loss: 0.2021\n",
      "7/231, train_loss: 0.1462\n",
      "8/231, train_loss: 0.1400\n",
      "9/231, train_loss: 0.0900\n",
      "10/231, train_loss: 0.3354\n",
      "11/231, train_loss: 0.1340\n",
      "12/231, train_loss: 0.2607\n",
      "13/231, train_loss: 0.1949\n",
      "14/231, train_loss: 0.1785\n",
      "15/231, train_loss: 0.2183\n",
      "16/231, train_loss: 0.2690\n",
      "17/231, train_loss: 0.2668\n",
      "18/231, train_loss: 0.3149\n",
      "19/231, train_loss: 0.1157\n",
      "20/231, train_loss: 0.2262\n",
      "21/231, train_loss: 0.2544\n",
      "22/231, train_loss: 0.2429\n",
      "23/231, train_loss: 0.1732\n",
      "24/231, train_loss: 0.1715\n",
      "25/231, train_loss: 0.2150\n",
      "26/231, train_loss: 0.1295\n",
      "27/231, train_loss: 0.2151\n",
      "28/231, train_loss: 0.4915\n",
      "29/231, train_loss: 0.3875\n",
      "30/231, train_loss: 0.2690\n",
      "31/231, train_loss: 0.1681\n",
      "32/231, train_loss: 0.3018\n",
      "33/231, train_loss: 0.3628\n",
      "34/231, train_loss: 0.2798\n",
      "35/231, train_loss: 0.2578\n",
      "36/231, train_loss: 0.2109\n",
      "37/231, train_loss: 0.3623\n",
      "38/231, train_loss: 0.3806\n",
      "39/231, train_loss: 0.3420\n",
      "40/231, train_loss: 0.3113\n",
      "41/231, train_loss: 0.1394\n",
      "42/231, train_loss: 0.2542\n",
      "43/231, train_loss: 0.4141\n",
      "44/231, train_loss: 0.1331\n",
      "45/231, train_loss: 0.3982\n",
      "46/231, train_loss: 0.2354\n",
      "47/231, train_loss: 0.1614\n",
      "48/231, train_loss: 0.2192\n",
      "49/231, train_loss: 0.1176\n",
      "50/231, train_loss: 0.2651\n",
      "51/231, train_loss: 0.1113\n",
      "52/231, train_loss: 0.1919\n",
      "53/231, train_loss: 0.1715\n",
      "54/231, train_loss: 0.2297\n",
      "55/231, train_loss: 0.2404\n",
      "56/231, train_loss: 0.2646\n",
      "57/231, train_loss: 0.1193\n",
      "58/231, train_loss: 0.3096\n",
      "59/231, train_loss: 0.1532\n",
      "60/231, train_loss: 0.2285\n",
      "61/231, train_loss: 0.2214\n",
      "62/231, train_loss: 0.1349\n",
      "63/231, train_loss: 0.1826\n",
      "64/231, train_loss: 0.1829\n",
      "65/231, train_loss: 0.1312\n",
      "66/231, train_loss: 0.1506\n",
      "67/231, train_loss: 0.1844\n",
      "68/231, train_loss: 0.3955\n",
      "69/231, train_loss: 0.1561\n",
      "70/231, train_loss: 0.2156\n",
      "71/231, train_loss: 0.1200\n",
      "72/231, train_loss: 0.1431\n",
      "73/231, train_loss: 0.2388\n",
      "74/231, train_loss: 0.4211\n",
      "75/231, train_loss: 0.1610\n",
      "76/231, train_loss: 0.1611\n",
      "77/231, train_loss: 0.2478\n",
      "78/231, train_loss: 0.2449\n",
      "79/231, train_loss: 0.2703\n",
      "80/231, train_loss: 0.1317\n",
      "81/231, train_loss: 0.2578\n",
      "82/231, train_loss: 0.1438\n",
      "83/231, train_loss: 0.1335\n",
      "84/231, train_loss: 0.3235\n",
      "85/231, train_loss: 0.1492\n",
      "86/231, train_loss: 0.3223\n",
      "87/231, train_loss: 0.1741\n",
      "88/231, train_loss: 0.2925\n",
      "89/231, train_loss: 0.1687\n",
      "90/231, train_loss: 0.2205\n",
      "91/231, train_loss: 0.4021\n",
      "92/231, train_loss: 0.2766\n",
      "93/231, train_loss: 0.1959\n",
      "94/231, train_loss: 0.1166\n",
      "95/231, train_loss: 0.1121\n",
      "96/231, train_loss: 0.1862\n",
      "97/231, train_loss: 0.1155\n",
      "98/231, train_loss: 0.2444\n",
      "99/231, train_loss: 0.2086\n",
      "100/231, train_loss: 0.3123\n",
      "101/231, train_loss: 0.3403\n",
      "102/231, train_loss: 0.6221\n",
      "103/231, train_loss: 0.1177\n",
      "104/231, train_loss: 0.1876\n",
      "105/231, train_loss: 0.5605\n",
      "106/231, train_loss: 0.2145\n",
      "107/231, train_loss: 0.1392\n",
      "108/231, train_loss: 0.2280\n",
      "109/231, train_loss: 0.2380\n",
      "110/231, train_loss: 0.0908\n",
      "111/231, train_loss: 0.2886\n",
      "112/231, train_loss: 0.3252\n",
      "113/231, train_loss: 0.2080\n",
      "114/231, train_loss: 0.2222\n",
      "115/231, train_loss: 0.1671\n",
      "116/231, train_loss: 0.2415\n",
      "117/231, train_loss: 0.1837\n",
      "118/231, train_loss: 0.4011\n",
      "119/231, train_loss: 0.4248\n",
      "120/231, train_loss: 0.1626\n",
      "121/231, train_loss: 0.1895\n",
      "122/231, train_loss: 0.1306\n",
      "123/231, train_loss: 0.1368\n",
      "124/231, train_loss: 0.2163\n",
      "125/231, train_loss: 0.3010\n",
      "126/231, train_loss: 0.2075\n",
      "127/231, train_loss: 0.0850\n",
      "128/231, train_loss: 0.1526\n",
      "129/231, train_loss: 0.1056\n",
      "130/231, train_loss: 0.1763\n",
      "131/231, train_loss: 0.1558\n",
      "132/231, train_loss: 0.4546\n",
      "133/231, train_loss: 0.0897\n",
      "134/231, train_loss: 0.3088\n",
      "135/231, train_loss: 0.2842\n",
      "136/231, train_loss: 0.2139\n",
      "137/231, train_loss: 0.2273\n",
      "138/231, train_loss: 0.1373\n",
      "139/231, train_loss: 0.1147\n",
      "140/231, train_loss: 0.1467\n",
      "141/231, train_loss: 0.2388\n",
      "142/231, train_loss: 0.1552\n",
      "143/231, train_loss: 0.3296\n",
      "144/231, train_loss: 0.1580\n",
      "145/231, train_loss: 0.1411\n",
      "146/231, train_loss: 0.2336\n",
      "147/231, train_loss: 0.3894\n",
      "148/231, train_loss: 0.3887\n",
      "149/231, train_loss: 0.2024\n",
      "150/231, train_loss: 0.2832\n",
      "151/231, train_loss: 0.1655\n",
      "152/231, train_loss: 0.2529\n",
      "153/231, train_loss: 0.3130\n",
      "154/231, train_loss: 0.3438\n",
      "155/231, train_loss: 0.2086\n",
      "156/231, train_loss: 0.1906\n",
      "157/231, train_loss: 0.4602\n",
      "158/231, train_loss: 0.2588\n",
      "159/231, train_loss: 0.2268\n",
      "160/231, train_loss: 0.1764\n",
      "161/231, train_loss: 0.1874\n",
      "162/231, train_loss: 0.3206\n",
      "163/231, train_loss: 0.2791\n",
      "164/231, train_loss: 0.2472\n",
      "165/231, train_loss: 0.1554\n",
      "166/231, train_loss: 0.0876\n",
      "167/231, train_loss: 0.1064\n",
      "168/231, train_loss: 0.2949\n",
      "169/231, train_loss: 0.2213\n",
      "170/231, train_loss: 0.1587\n",
      "171/231, train_loss: 0.3945\n",
      "172/231, train_loss: 0.1516\n",
      "173/231, train_loss: 0.3335\n",
      "174/231, train_loss: 0.2030\n",
      "175/231, train_loss: 0.1436\n",
      "176/231, train_loss: 0.2832\n",
      "177/231, train_loss: 0.1195\n",
      "178/231, train_loss: 0.1069\n",
      "179/231, train_loss: 0.5400\n",
      "180/231, train_loss: 0.1415\n",
      "181/231, train_loss: 0.7334\n",
      "182/231, train_loss: 0.4082\n",
      "183/231, train_loss: 0.1692\n",
      "184/231, train_loss: 0.2502\n",
      "185/231, train_loss: 0.4229\n",
      "186/231, train_loss: 0.2881\n",
      "187/231, train_loss: 0.3743\n",
      "188/231, train_loss: 0.3804\n",
      "189/231, train_loss: 0.3584\n",
      "190/231, train_loss: 0.1740\n",
      "191/231, train_loss: 0.2903\n",
      "192/231, train_loss: 0.2057\n",
      "193/231, train_loss: 0.2915\n",
      "194/231, train_loss: 0.2893\n",
      "195/231, train_loss: 0.2051\n",
      "196/231, train_loss: 0.2749\n",
      "197/231, train_loss: 0.3071\n",
      "198/231, train_loss: 0.1959\n",
      "199/231, train_loss: 0.2656\n",
      "200/231, train_loss: 0.2452\n",
      "201/231, train_loss: 0.1716\n",
      "202/231, train_loss: 0.1289\n",
      "203/231, train_loss: 0.2090\n",
      "204/231, train_loss: 0.7900\n",
      "205/231, train_loss: 0.2800\n",
      "206/231, train_loss: 0.2001\n",
      "207/231, train_loss: 0.1104\n",
      "208/231, train_loss: 0.1770\n",
      "209/231, train_loss: 0.3503\n",
      "210/231, train_loss: 0.1519\n",
      "211/231, train_loss: 0.1877\n",
      "212/231, train_loss: 0.0931\n",
      "213/231, train_loss: 0.2412\n",
      "214/231, train_loss: 0.1605\n",
      "215/231, train_loss: 0.2957\n",
      "216/231, train_loss: 0.1868\n",
      "217/231, train_loss: 0.1477\n",
      "218/231, train_loss: 0.2371\n",
      "219/231, train_loss: 0.1663\n",
      "220/231, train_loss: 0.2161\n",
      "221/231, train_loss: 0.1965\n",
      "222/231, train_loss: 0.1206\n",
      "223/231, train_loss: 0.5742\n",
      "224/231, train_loss: 0.2871\n",
      "225/231, train_loss: 0.2883\n",
      "226/231, train_loss: 0.0958\n",
      "227/231, train_loss: 0.1133\n",
      "228/231, train_loss: 0.2512\n",
      "229/231, train_loss: 0.4751\n",
      "230/231, train_loss: 0.2030\n",
      "231/231, train_loss: 0.1945\n",
      "232/231, train_loss: 0.3584\n",
      "epoch 13 average loss: 0.2382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/11/05 00:11:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/11/05 00:11:50 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 14/100\n",
      "1/231, train_loss: 0.1378\n",
      "2/231, train_loss: 0.2817\n",
      "3/231, train_loss: 0.1962\n",
      "4/231, train_loss: 0.3362\n",
      "5/231, train_loss: 0.1241\n",
      "6/231, train_loss: 0.3530\n",
      "7/231, train_loss: 0.3306\n",
      "8/231, train_loss: 0.1680\n",
      "9/231, train_loss: 0.5591\n",
      "10/231, train_loss: 0.2139\n",
      "11/231, train_loss: 0.1750\n",
      "12/231, train_loss: 0.1698\n",
      "13/231, train_loss: 0.1753\n",
      "14/231, train_loss: 0.2876\n",
      "15/231, train_loss: 0.3091\n",
      "16/231, train_loss: 0.2622\n",
      "17/231, train_loss: 0.6602\n",
      "18/231, train_loss: 0.1826\n",
      "19/231, train_loss: 0.2061\n",
      "20/231, train_loss: 0.1605\n",
      "21/231, train_loss: 0.2388\n",
      "22/231, train_loss: 0.1040\n",
      "23/231, train_loss: 0.2039\n",
      "24/231, train_loss: 0.2925\n",
      "25/231, train_loss: 0.2666\n",
      "26/231, train_loss: 0.1570\n",
      "27/231, train_loss: 0.1296\n",
      "28/231, train_loss: 0.2251\n",
      "29/231, train_loss: 0.3965\n",
      "30/231, train_loss: 0.1421\n",
      "31/231, train_loss: 0.1886\n",
      "32/231, train_loss: 0.3521\n",
      "33/231, train_loss: 0.2119\n",
      "34/231, train_loss: 0.1971\n",
      "35/231, train_loss: 0.1892\n",
      "36/231, train_loss: 0.2307\n",
      "37/231, train_loss: 0.2952\n",
      "38/231, train_loss: 0.1833\n",
      "39/231, train_loss: 0.1472\n",
      "40/231, train_loss: 0.3384\n",
      "41/231, train_loss: 0.2164\n",
      "42/231, train_loss: 0.3022\n",
      "43/231, train_loss: 0.2090\n",
      "44/231, train_loss: 0.4077\n",
      "45/231, train_loss: 0.1147\n",
      "46/231, train_loss: 0.4548\n",
      "47/231, train_loss: 0.2179\n",
      "48/231, train_loss: 0.2405\n",
      "49/231, train_loss: 0.4187\n",
      "50/231, train_loss: 0.1036\n",
      "51/231, train_loss: 0.1262\n",
      "52/231, train_loss: 0.3059\n",
      "53/231, train_loss: 0.1779\n",
      "54/231, train_loss: 0.1367\n",
      "55/231, train_loss: 0.1934\n",
      "56/231, train_loss: 0.2261\n",
      "57/231, train_loss: 0.1274\n",
      "58/231, train_loss: 0.2126\n",
      "59/231, train_loss: 0.3181\n",
      "60/231, train_loss: 0.2478\n",
      "61/231, train_loss: 0.2211\n",
      "62/231, train_loss: 0.1019\n",
      "63/231, train_loss: 0.1638\n",
      "64/231, train_loss: 0.1140\n",
      "65/231, train_loss: 0.1313\n",
      "66/231, train_loss: 0.3782\n",
      "67/231, train_loss: 0.2549\n",
      "68/231, train_loss: 0.2617\n",
      "69/231, train_loss: 0.2571\n",
      "70/231, train_loss: 0.1655\n",
      "71/231, train_loss: 0.2478\n",
      "72/231, train_loss: 0.1416\n",
      "73/231, train_loss: 0.2766\n",
      "74/231, train_loss: 0.2554\n",
      "75/231, train_loss: 0.1414\n",
      "76/231, train_loss: 0.2634\n",
      "77/231, train_loss: 0.3418\n",
      "78/231, train_loss: 0.1923\n",
      "79/231, train_loss: 0.3608\n",
      "80/231, train_loss: 0.5508\n",
      "81/231, train_loss: 0.1904\n",
      "82/231, train_loss: 0.4131\n",
      "83/231, train_loss: 0.1375\n",
      "84/231, train_loss: 0.2568\n",
      "85/231, train_loss: 0.2273\n",
      "86/231, train_loss: 0.2250\n",
      "87/231, train_loss: 0.3760\n",
      "88/231, train_loss: 0.1454\n",
      "89/231, train_loss: 0.3052\n",
      "90/231, train_loss: 0.1561\n",
      "91/231, train_loss: 0.1193\n",
      "92/231, train_loss: 0.1669\n",
      "93/231, train_loss: 0.1444\n",
      "94/231, train_loss: 0.1709\n",
      "95/231, train_loss: 0.2471\n",
      "96/231, train_loss: 0.1807\n",
      "97/231, train_loss: 0.1541\n",
      "98/231, train_loss: 0.3999\n",
      "99/231, train_loss: 0.1523\n",
      "100/231, train_loss: 0.6392\n",
      "101/231, train_loss: 0.3162\n",
      "102/231, train_loss: 0.1871\n",
      "103/231, train_loss: 0.1719\n",
      "104/231, train_loss: 0.2322\n",
      "105/231, train_loss: 0.1792\n",
      "106/231, train_loss: 0.1936\n",
      "107/231, train_loss: 0.3774\n",
      "108/231, train_loss: 0.1526\n",
      "109/231, train_loss: 0.2449\n",
      "110/231, train_loss: 0.2981\n",
      "111/231, train_loss: 0.3562\n",
      "112/231, train_loss: 0.2484\n",
      "113/231, train_loss: 0.2715\n",
      "114/231, train_loss: 0.5430\n",
      "115/231, train_loss: 0.2798\n",
      "116/231, train_loss: 0.1732\n",
      "117/231, train_loss: 0.5649\n",
      "118/231, train_loss: 0.2163\n",
      "119/231, train_loss: 0.2068\n",
      "120/231, train_loss: 0.3354\n",
      "121/231, train_loss: 0.3848\n",
      "122/231, train_loss: 0.2861\n",
      "123/231, train_loss: 0.1035\n",
      "124/231, train_loss: 0.0923\n",
      "125/231, train_loss: 0.1995\n",
      "126/231, train_loss: 0.2661\n",
      "127/231, train_loss: 0.2015\n",
      "128/231, train_loss: 0.1689\n",
      "129/231, train_loss: 0.1359\n",
      "130/231, train_loss: 0.1565\n",
      "131/231, train_loss: 0.1495\n",
      "132/231, train_loss: 0.2098\n",
      "133/231, train_loss: 0.3474\n",
      "134/231, train_loss: 0.2330\n",
      "135/231, train_loss: 0.3203\n",
      "136/231, train_loss: 0.2329\n",
      "137/231, train_loss: 0.2020\n",
      "138/231, train_loss: 0.3767\n",
      "139/231, train_loss: 0.0850\n",
      "140/231, train_loss: 0.1868\n",
      "141/231, train_loss: 0.1581\n",
      "142/231, train_loss: 0.1124\n",
      "143/231, train_loss: 0.1129\n",
      "144/231, train_loss: 0.1748\n",
      "145/231, train_loss: 0.3828\n",
      "146/231, train_loss: 0.1096\n",
      "147/231, train_loss: 0.2180\n",
      "148/231, train_loss: 0.1737\n",
      "149/231, train_loss: 0.2244\n",
      "150/231, train_loss: 0.1328\n",
      "151/231, train_loss: 0.1754\n",
      "152/231, train_loss: 0.2949\n",
      "153/231, train_loss: 0.2085\n",
      "154/231, train_loss: 0.3203\n",
      "155/231, train_loss: 0.3770\n",
      "156/231, train_loss: 0.2993\n",
      "157/231, train_loss: 0.4609\n",
      "158/231, train_loss: 0.5186\n",
      "159/231, train_loss: 0.3711\n",
      "160/231, train_loss: 0.1011\n",
      "161/231, train_loss: 0.2678\n",
      "162/231, train_loss: 0.2395\n",
      "163/231, train_loss: 0.3047\n",
      "164/231, train_loss: 0.6660\n",
      "165/231, train_loss: 0.3645\n",
      "166/231, train_loss: 0.1201\n",
      "167/231, train_loss: 0.1854\n",
      "168/231, train_loss: 0.2090\n",
      "169/231, train_loss: 0.1058\n",
      "170/231, train_loss: 0.2737\n",
      "171/231, train_loss: 0.2517\n",
      "172/231, train_loss: 0.1831\n",
      "173/231, train_loss: 0.3901\n",
      "174/231, train_loss: 0.1948\n",
      "175/231, train_loss: 0.1740\n",
      "176/231, train_loss: 0.3164\n",
      "177/231, train_loss: 0.1495\n",
      "178/231, train_loss: 0.1470\n",
      "179/231, train_loss: 0.2006\n",
      "180/231, train_loss: 0.1707\n",
      "181/231, train_loss: 0.4688\n",
      "182/231, train_loss: 0.1970\n",
      "183/231, train_loss: 0.2031\n",
      "184/231, train_loss: 0.3167\n",
      "185/231, train_loss: 0.3730\n",
      "186/231, train_loss: 0.1813\n",
      "187/231, train_loss: 0.1759\n",
      "188/231, train_loss: 0.1877\n",
      "189/231, train_loss: 0.2920\n",
      "190/231, train_loss: 0.1918\n",
      "191/231, train_loss: 0.2126\n",
      "192/231, train_loss: 0.3823\n",
      "193/231, train_loss: 0.1648\n",
      "194/231, train_loss: 0.1866\n",
      "195/231, train_loss: 0.2603\n",
      "196/231, train_loss: 0.1770\n",
      "197/231, train_loss: 0.2712\n",
      "198/231, train_loss: 0.3625\n",
      "199/231, train_loss: 0.1704\n",
      "200/231, train_loss: 0.1076\n",
      "201/231, train_loss: 0.1046\n",
      "202/231, train_loss: 0.0969\n",
      "203/231, train_loss: 0.4409\n",
      "204/231, train_loss: 0.1837\n",
      "205/231, train_loss: 0.1907\n",
      "206/231, train_loss: 0.1292\n"
     ]
    }
   ],
   "source": [
    "epoch_len = len_train_ds // train_loader.batch_size\n",
    "\n",
    "with mlflow.start_run(description=description) as run:\n",
    "\n",
    "    mlflow.log_param(\"gt_box_mode\", gt_box_mode)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"patch_size\", patch_size)\n",
    "    mlflow.log_param(\"data_list_file_path\", data_list_file_path)\n",
    "    mlflow.log_param(\"data_base_dir\", data_base_dir)\n",
    "    mlflow.log_param(\"amp\", amp)\n",
    "    \n",
    "    mlflow.log_param(\"n_input_channels\", n_input_channels)\n",
    "    mlflow.log_param(\"spatial_dims\", spatial_dims)\n",
    "    mlflow.log_param(\"balanced_sampler_pos_fraction\", balanced_sampler_pos_fraction)\n",
    "    mlflow.log_param(\"score_thresh\", score_thresh)\n",
    "    mlflow.log_param(\"nms_thresh\", nms_thresh)\n",
    "    \n",
    "    mlflow.log_param(\"initial_lr\", lr)\n",
    "    mlflow.log_param(\"val_interval\", val_interval)\n",
    "    mlflow.log_param(\"max_epochs\", max_epochs)\n",
    "    mlflow.log_param(\"w_cls\", w_cls)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        detector.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_cls_loss = 0\n",
    "        epoch_box_reg_loss = 0\n",
    "        step = 0\n",
    "        scheduler_warmup.step()\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = [\n",
    "                batch_data_ii[\"image\"].to(device) for batch_data_i in batch_data for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "            targets = [\n",
    "                dict(\n",
    "                    label=batch_data_ii[\"label\"].to(device),\n",
    "                    box=batch_data_ii[\"box\"].to(device)\n",
    "                )\n",
    "                for batch_data_i in batch_data\n",
    "                for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "\n",
    "            for param in detector.network.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            if amp and (scaler is not None):\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = detector(inputs, targets)\n",
    "                    loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = detector(inputs, targets)\n",
    "                loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # saving into mlflow\n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_cls_loss += outputs[detector.cls_key].detach().item()\n",
    "            epoch_box_reg_loss += outputs[detector.box_reg_key].detach().item()\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            mlflow.log_metric(\"train_loss\", loss.detach().item(), epoch_len * epoch + step)\n",
    "\n",
    "        del inputs, batch_data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_cls_loss /= step\n",
    "        epoch_box_reg_loss /= step\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        mlflow.log_metric(\"avg_train_loss\", epoch_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_cls_loss\", epoch_cls_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_box_reg_loss\", epoch_box_reg_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"train_lr\", optimizer.param_groups[0][\"lr\"], epoch + 1)\n",
    "\n",
    "        # saving last trained model\n",
    "        mlflow.pytorch.log_model(detector.network, \"model\")\n",
    "\n",
    "        # validation for model selection\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            detector.eval()\n",
    "            val_outputs_all = []\n",
    "            val_targets_all = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    # if all val_data_i[\"image\"] smaller than val_patch_size, no need to use inferer\n",
    "                    # otherwise, need inferer to handle large input images.\n",
    "                    use_inferer = not all(\n",
    "                        [val_data_i[\"image\"][0, ...].numel() < np.prod(val_patch_size) for val_data_i in val_data]\n",
    "                    )\n",
    "                    val_inputs = [val_data_i.pop(\"image\").to(device) for val_data_i in val_data]\n",
    "\n",
    "                    if amp:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "                    else:\n",
    "                        val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "\n",
    "                    # save outputs for evaluation\n",
    "                    val_outputs_all += val_outputs\n",
    "                    val_targets_all += val_data\n",
    "\n",
    "            # visualize an inference image and boxes\n",
    "            draw_img = visualize_one_xy_slice_in_3d_image(\n",
    "                gt_boxes=val_data[0][\"box\"].cpu().detach().numpy(),\n",
    "                image=val_inputs[0][0, ...].cpu().detach().numpy(),\n",
    "                pred_boxes=val_outputs[0][detector.target_box_key].cpu().detach().numpy(),\n",
    "            )\n",
    "            # mlflow.log_image(draw_img.transpose([2, 1, 0]), \"val_img_xy.png\")\n",
    "            mlflow.log_image(draw_img, str(epoch + 1) + \"_val_img_xy.png\")\n",
    "\n",
    "            # compute metrics\n",
    "            del val_inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            results_metric = matching_batch(\n",
    "                iou_fn=box_utils.box_iou,\n",
    "                iou_thresholds=coco_metric.iou_thresholds,\n",
    "                pred_boxes=[\n",
    "                    val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_scores=[\n",
    "                    val_data_i[detector.pred_score_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                gt_boxes=[val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_targets_all],\n",
    "                gt_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_targets_all\n",
    "                ]\n",
    "            )\n",
    "            val_epoch_metric_dict = coco_metric(results_metric)[0]\n",
    "            print(val_epoch_metric_dict)\n",
    "\n",
    "            # write metrics\n",
    "            for k in val_epoch_metric_dict.keys():\n",
    "                mlflow.log_metric(\"val_\" + k, val_epoch_metric_dict[k], epoch + 1)\n",
    "            val_epoch_metric = val_epoch_metric_dict.values()\n",
    "            val_epoch_metric = sum(val_epoch_metric) / len(val_epoch_metric)\n",
    "            mlflow.log_metric(\"val_metric\", val_epoch_metric, epoch + 1)\n",
    "\n",
    "            # save best trained model\n",
    "            if val_epoch_metric > best_val_epoch_metric:\n",
    "                best_val_epoch_metric = val_epoch_metric\n",
    "                best_val_epoch = epoch + 1\n",
    "                mlflow.pytorch.log_model(detector.network, \"best_model\")\n",
    "            print(\n",
    "                \"current epoch: {} current metric: {:.4f} \"\n",
    "                \"best metric: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, val_epoch_metric, best_val_epoch_metric, best_val_epoch\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(f\"train completed, best_metric: {best_val_epoch_metric:.4f} \" f\"at epoch: {best_val_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018d3e53-201c-460b-8edf-9a82bba4c9d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79dfad-c3c3-4e70-9f34-cee54b016a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
