{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8931abf-4b0b-4731-adf9-7c5b2840c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import gc\n",
    "\n",
    "from visualize_image import visualize_one_xy_slice_in_3d_image\n",
    "from loading_dataset import load_data\n",
    "from model import load_model\n",
    "import numpy as np\n",
    "from monai.data import box_utils\n",
    "from monai.apps.detection.metrics.matching import matching_batch\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from monai.apps.detection.metrics.coco import COCOMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6948feac-77f3-43ab-91e3-5e2a79c3dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'Eddy'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'Usp1#'\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = 'LUNA16 + HC + MSD - without lung seg'\n",
    "description = \"Mixed dataset training without lung segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2c25b3-b7bc-4abc-856a-f2a4d88259bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box_mode = 'cccwhd'\n",
    "batch_size = 8\n",
    "patch_size = [96,96,40]\n",
    "data_list_file_path = '/data/output/mixed_data/mixed_train_val0.json'\n",
    "data_base_dir = '/data/mixed/'\n",
    "# data_base_dir = '/data/HC_Images_resample/'\n",
    "# data_base_dir = '/data/MSD_Images_resample/'\n",
    "# data_base_dir = '/data/LUNA16_Images_resample/'\n",
    "amp=True\n",
    "\n",
    "returned_layers = [1,2]\n",
    "base_anchor_shapes = [[6,8,4],[8,6,5],[10,10,6]]\n",
    "conv1_t_stride = [2,2,1]\n",
    "n_input_channels = 1\n",
    "spatial_dims = 3\n",
    "fg_labels = [0]\n",
    "verbose = False\n",
    "balanced_sampler_pos_fraction = 0.3\n",
    "score_thresh = 0.02\n",
    "nms_thresh = 0.22\n",
    "val_patch_size = [256,256,104]\n",
    "\n",
    "lr = 1e-2\n",
    "val_interval = 5\n",
    "coco_metric = COCOMetric(classes=[\"nodule\"], iou_list=[0.1], max_detection=[100])\n",
    "best_val_epoch_metric = 0.0\n",
    "best_val_epoch = -1\n",
    "max_epochs = 100\n",
    "w_cls = 1.0\n",
    "\n",
    "compute_dtype = torch.float32\n",
    "if amp:\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ad2cc-31a4-4f4d-a131-256b6cfd9e61",
   "metadata": {},
   "source": [
    "### loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9bf6dd-f147-4974-8b8b-54204a268866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, len_train_ds = load_data(\n",
    "    gt_box_mode, patch_size, batch_size, amp, data_list_file_path, data_base_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e514-7ab6-45dd-bd31-2f7bf68ff2d6",
   "metadata": {},
   "source": [
    "### loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf21214-041f-401e-a2f7-f0072929bf90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector, device = load_model(\n",
    "    returned_layers, base_anchor_shapes, conv1_t_stride, n_input_channels,\n",
    "    spatial_dims, fg_labels, verbose, balanced_sampler_pos_fraction,\n",
    "    score_thresh, nms_thresh, val_patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7938680-00c0-4d79-bb31-4320bac9e332",
   "metadata": {},
   "source": [
    "### Initialize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7489f0c5-5ffc-4005-994d-8d74c85cae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    detector.network.parameters(),\n",
    "    lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=3e-5,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "after_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=150, gamma=0.1\n",
    ")\n",
    "scheduler_warmup = GradualWarmupScheduler(\n",
    "    optimizer, multiplier=1, total_epoch=10, after_scheduler=after_scheduler\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler() if amp else None\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ca37e-e17d-4e84-b9bc-4029ba3dcaf2",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5beae8c-e323-4dd2-84b6-eeebf82bf10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n",
      "1/463, train_loss: 1.0020\n",
      "2/463, train_loss: 1.1660\n",
      "3/463, train_loss: 1.3271\n",
      "4/463, train_loss: 1.1895\n",
      "5/463, train_loss: 1.1875\n",
      "6/463, train_loss: 1.0645\n",
      "7/463, train_loss: 0.8818\n",
      "8/463, train_loss: 0.7129\n",
      "9/463, train_loss: 1.4824\n",
      "10/463, train_loss: 0.9961\n",
      "11/463, train_loss: 1.0508\n",
      "12/463, train_loss: 1.1592\n",
      "13/463, train_loss: 0.8228\n",
      "14/463, train_loss: 0.6079\n",
      "15/463, train_loss: 0.7148\n",
      "16/463, train_loss: 1.1650\n",
      "17/463, train_loss: 0.5742\n",
      "18/463, train_loss: 1.0312\n",
      "19/463, train_loss: 0.7124\n",
      "20/463, train_loss: 0.7246\n",
      "21/463, train_loss: 0.6182\n",
      "22/463, train_loss: 0.8145\n",
      "23/463, train_loss: 1.1562\n",
      "24/463, train_loss: 0.9102\n",
      "25/463, train_loss: 0.9253\n",
      "26/463, train_loss: 0.8535\n",
      "27/463, train_loss: 0.7021\n",
      "28/463, train_loss: 0.9316\n",
      "29/463, train_loss: 0.7158\n",
      "30/463, train_loss: 0.9092\n",
      "31/463, train_loss: 0.9204\n",
      "32/463, train_loss: 0.4470\n",
      "33/463, train_loss: 0.5957\n",
      "34/463, train_loss: 0.6455\n",
      "35/463, train_loss: 1.0957\n",
      "36/463, train_loss: 1.1113\n",
      "37/463, train_loss: 0.8125\n",
      "38/463, train_loss: 1.1113\n",
      "39/463, train_loss: 0.6787\n",
      "40/463, train_loss: 1.0820\n",
      "41/463, train_loss: 0.8779\n",
      "42/463, train_loss: 0.9893\n",
      "43/463, train_loss: 1.0557\n",
      "44/463, train_loss: 0.7417\n",
      "45/463, train_loss: 0.7080\n",
      "46/463, train_loss: 0.8691\n",
      "47/463, train_loss: 0.7656\n",
      "48/463, train_loss: 0.8408\n",
      "49/463, train_loss: 0.7817\n",
      "50/463, train_loss: 0.7910\n",
      "51/463, train_loss: 0.6782\n",
      "52/463, train_loss: 0.6709\n",
      "53/463, train_loss: 0.7886\n",
      "54/463, train_loss: 0.8594\n",
      "55/463, train_loss: 0.6660\n",
      "56/463, train_loss: 0.7515\n",
      "57/463, train_loss: 0.8350\n",
      "58/463, train_loss: 0.6948\n",
      "59/463, train_loss: 0.8320\n",
      "60/463, train_loss: 0.8755\n",
      "61/463, train_loss: 0.4873\n",
      "62/463, train_loss: 0.8672\n",
      "63/463, train_loss: 0.6099\n",
      "64/463, train_loss: 0.7441\n",
      "65/463, train_loss: 0.8076\n",
      "66/463, train_loss: 0.6279\n",
      "67/463, train_loss: 0.7080\n",
      "68/463, train_loss: 0.7422\n",
      "69/463, train_loss: 0.8740\n",
      "70/463, train_loss: 0.7754\n",
      "71/463, train_loss: 0.5181\n",
      "72/463, train_loss: 0.5508\n",
      "73/463, train_loss: 0.6343\n",
      "74/463, train_loss: 0.7749\n",
      "75/463, train_loss: 0.6777\n",
      "76/463, train_loss: 0.7920\n",
      "77/463, train_loss: 0.9443\n",
      "78/463, train_loss: 0.9219\n",
      "79/463, train_loss: 0.7783\n",
      "80/463, train_loss: 0.7397\n",
      "81/463, train_loss: 0.8740\n",
      "82/463, train_loss: 0.9062\n",
      "83/463, train_loss: 0.8867\n",
      "84/463, train_loss: 0.6299\n",
      "85/463, train_loss: 0.5889\n",
      "86/463, train_loss: 0.7339\n",
      "87/463, train_loss: 0.8955\n",
      "88/463, train_loss: 0.8252\n",
      "89/463, train_loss: 0.4583\n",
      "90/463, train_loss: 0.5791\n",
      "91/463, train_loss: 0.6382\n",
      "92/463, train_loss: 0.7461\n",
      "93/463, train_loss: 0.6089\n",
      "94/463, train_loss: 0.5664\n",
      "95/463, train_loss: 0.8091\n",
      "96/463, train_loss: 0.7422\n",
      "97/463, train_loss: 0.6729\n",
      "98/463, train_loss: 0.5762\n",
      "99/463, train_loss: 0.8799\n",
      "100/463, train_loss: 0.6191\n",
      "101/463, train_loss: 0.9097\n",
      "102/463, train_loss: 0.8271\n",
      "103/463, train_loss: 0.7266\n",
      "104/463, train_loss: 0.6641\n",
      "105/463, train_loss: 0.9111\n",
      "106/463, train_loss: 1.1191\n",
      "107/463, train_loss: 0.1824\n",
      "108/463, train_loss: 0.9189\n",
      "109/463, train_loss: 0.6724\n",
      "110/463, train_loss: 0.6978\n",
      "111/463, train_loss: 0.6216\n",
      "112/463, train_loss: 0.6758\n",
      "113/463, train_loss: 0.7100\n",
      "114/463, train_loss: 0.6572\n",
      "115/463, train_loss: 0.6440\n",
      "116/463, train_loss: 0.6357\n",
      "117/463, train_loss: 0.7158\n",
      "118/463, train_loss: 0.5005\n",
      "119/463, train_loss: 0.5698\n",
      "120/463, train_loss: 0.7959\n",
      "121/463, train_loss: 0.5684\n",
      "122/463, train_loss: 0.5244\n",
      "123/463, train_loss: 0.5659\n",
      "124/463, train_loss: 0.7773\n",
      "125/463, train_loss: 0.4668\n",
      "126/463, train_loss: 0.7178\n",
      "127/463, train_loss: 0.5356\n",
      "128/463, train_loss: 0.5430\n",
      "129/463, train_loss: 0.5615\n",
      "130/463, train_loss: 0.6787\n",
      "131/463, train_loss: 0.6255\n",
      "132/463, train_loss: 0.4946\n",
      "133/463, train_loss: 0.6777\n",
      "134/463, train_loss: 0.8013\n",
      "135/463, train_loss: 0.8086\n",
      "136/463, train_loss: 0.5210\n",
      "137/463, train_loss: 0.6816\n",
      "138/463, train_loss: 0.7183\n",
      "139/463, train_loss: 0.5278\n",
      "140/463, train_loss: 0.7041\n",
      "141/463, train_loss: 0.4370\n",
      "142/463, train_loss: 0.5854\n",
      "143/463, train_loss: 0.6729\n",
      "144/463, train_loss: 0.6621\n",
      "145/463, train_loss: 0.6592\n",
      "146/463, train_loss: 0.4814\n",
      "147/463, train_loss: 0.5278\n",
      "148/463, train_loss: 0.5029\n",
      "149/463, train_loss: 0.7300\n",
      "150/463, train_loss: 0.5264\n",
      "151/463, train_loss: 0.4556\n",
      "152/463, train_loss: 0.4766\n",
      "153/463, train_loss: 0.7417\n",
      "154/463, train_loss: 0.7964\n",
      "155/463, train_loss: 0.7065\n",
      "156/463, train_loss: 0.7607\n",
      "157/463, train_loss: 0.6509\n",
      "158/463, train_loss: 0.7744\n",
      "159/463, train_loss: 0.6084\n",
      "160/463, train_loss: 0.7637\n",
      "161/463, train_loss: 0.6255\n",
      "162/463, train_loss: 0.5854\n",
      "163/463, train_loss: 0.6064\n",
      "164/463, train_loss: 0.8037\n",
      "165/463, train_loss: 0.6006\n",
      "166/463, train_loss: 0.6084\n",
      "167/463, train_loss: 0.6787\n",
      "168/463, train_loss: 0.6514\n",
      "169/463, train_loss: 0.7119\n",
      "170/463, train_loss: 0.3994\n",
      "171/463, train_loss: 0.8027\n",
      "172/463, train_loss: 0.5269\n",
      "173/463, train_loss: 0.5386\n",
      "174/463, train_loss: 0.5869\n",
      "175/463, train_loss: 0.6514\n",
      "176/463, train_loss: 0.3533\n",
      "177/463, train_loss: 0.8105\n",
      "178/463, train_loss: 0.7812\n",
      "179/463, train_loss: 0.6582\n",
      "180/463, train_loss: 0.7856\n",
      "181/463, train_loss: 0.6729\n",
      "182/463, train_loss: 0.7783\n",
      "183/463, train_loss: 0.7373\n",
      "184/463, train_loss: 0.9170\n",
      "185/463, train_loss: 0.7441\n",
      "186/463, train_loss: 0.4929\n",
      "187/463, train_loss: 0.6074\n",
      "188/463, train_loss: 0.5205\n",
      "189/463, train_loss: 0.4580\n",
      "190/463, train_loss: 0.5996\n",
      "191/463, train_loss: 0.6831\n",
      "192/463, train_loss: 0.6616\n",
      "193/463, train_loss: 0.4497\n",
      "194/463, train_loss: 0.6514\n",
      "195/463, train_loss: 0.7061\n",
      "196/463, train_loss: 0.6758\n",
      "197/463, train_loss: 0.7510\n",
      "198/463, train_loss: 0.5312\n",
      "199/463, train_loss: 0.4951\n",
      "200/463, train_loss: 0.4890\n",
      "201/463, train_loss: 0.5957\n",
      "202/463, train_loss: 0.6675\n",
      "203/463, train_loss: 0.6553\n",
      "204/463, train_loss: 0.6255\n",
      "205/463, train_loss: 0.6064\n",
      "206/463, train_loss: 0.5981\n",
      "207/463, train_loss: 0.5234\n",
      "208/463, train_loss: 0.4724\n",
      "209/463, train_loss: 0.5679\n",
      "210/463, train_loss: 0.7612\n",
      "211/463, train_loss: 0.4495\n",
      "212/463, train_loss: 0.5396\n",
      "213/463, train_loss: 0.4734\n",
      "214/463, train_loss: 0.5752\n",
      "215/463, train_loss: 0.5176\n",
      "216/463, train_loss: 0.4155\n",
      "217/463, train_loss: 0.7354\n",
      "218/463, train_loss: 0.4854\n",
      "219/463, train_loss: 0.5532\n",
      "220/463, train_loss: 0.6240\n",
      "221/463, train_loss: 0.7266\n",
      "222/463, train_loss: 0.5513\n",
      "223/463, train_loss: 0.6455\n",
      "224/463, train_loss: 0.6938\n",
      "225/463, train_loss: 0.4939\n",
      "226/463, train_loss: 0.5898\n",
      "227/463, train_loss: 0.6880\n",
      "228/463, train_loss: 0.4080\n",
      "229/463, train_loss: 0.7007\n",
      "230/463, train_loss: 0.7539\n",
      "231/463, train_loss: 0.5356\n",
      "232/463, train_loss: 0.4375\n",
      "233/463, train_loss: 0.5747\n",
      "234/463, train_loss: 0.5898\n",
      "235/463, train_loss: 0.4644\n",
      "236/463, train_loss: 0.8423\n",
      "237/463, train_loss: 0.3477\n",
      "238/463, train_loss: 0.4788\n",
      "239/463, train_loss: 0.4771\n",
      "240/463, train_loss: 0.6904\n",
      "241/463, train_loss: 0.5562\n",
      "242/463, train_loss: 0.6250\n",
      "243/463, train_loss: 0.7373\n",
      "244/463, train_loss: 0.6816\n",
      "245/463, train_loss: 0.6289\n",
      "246/463, train_loss: 0.5557\n",
      "247/463, train_loss: 0.6660\n",
      "248/463, train_loss: 0.5615\n",
      "249/463, train_loss: 0.6992\n",
      "250/463, train_loss: 0.7100\n",
      "251/463, train_loss: 0.6060\n",
      "252/463, train_loss: 0.7349\n",
      "253/463, train_loss: 0.7061\n",
      "254/463, train_loss: 0.7798\n",
      "255/463, train_loss: 0.6289\n",
      "256/463, train_loss: 0.6108\n",
      "257/463, train_loss: 0.6699\n",
      "258/463, train_loss: 0.6592\n",
      "259/463, train_loss: 0.7153\n",
      "260/463, train_loss: 0.6904\n",
      "261/463, train_loss: 0.6904\n",
      "262/463, train_loss: 0.5977\n",
      "263/463, train_loss: 0.6045\n",
      "264/463, train_loss: 0.5684\n",
      "265/463, train_loss: 0.6714\n",
      "266/463, train_loss: 0.6641\n",
      "267/463, train_loss: 0.6621\n",
      "268/463, train_loss: 0.7378\n",
      "269/463, train_loss: 0.5049\n",
      "270/463, train_loss: 0.7046\n",
      "271/463, train_loss: 0.4902\n",
      "272/463, train_loss: 0.7578\n",
      "273/463, train_loss: 0.5547\n",
      "274/463, train_loss: 0.1725\n",
      "275/463, train_loss: 0.4304\n",
      "276/463, train_loss: 0.6284\n",
      "277/463, train_loss: 0.6348\n",
      "278/463, train_loss: 0.5947\n",
      "279/463, train_loss: 0.7607\n",
      "280/463, train_loss: 0.5176\n",
      "281/463, train_loss: 0.4697\n",
      "282/463, train_loss: 0.5439\n",
      "283/463, train_loss: 0.5996\n",
      "284/463, train_loss: 0.8135\n",
      "285/463, train_loss: 0.6201\n",
      "286/463, train_loss: 0.4172\n",
      "287/463, train_loss: 0.6108\n",
      "288/463, train_loss: 0.5864\n",
      "289/463, train_loss: 0.6377\n",
      "290/463, train_loss: 0.3994\n",
      "291/463, train_loss: 0.7329\n",
      "292/463, train_loss: 0.6655\n",
      "293/463, train_loss: 0.6187\n",
      "294/463, train_loss: 0.6382\n",
      "295/463, train_loss: 0.6172\n",
      "296/463, train_loss: 0.4817\n",
      "297/463, train_loss: 0.7549\n",
      "298/463, train_loss: 0.6377\n",
      "299/463, train_loss: 0.5386\n",
      "300/463, train_loss: 0.5596\n",
      "301/463, train_loss: 0.4734\n",
      "302/463, train_loss: 0.8145\n",
      "303/463, train_loss: 0.6484\n",
      "304/463, train_loss: 0.6597\n",
      "305/463, train_loss: 0.6294\n",
      "306/463, train_loss: 0.5918\n",
      "307/463, train_loss: 0.6802\n",
      "308/463, train_loss: 0.6860\n",
      "309/463, train_loss: 0.5537\n",
      "310/463, train_loss: 0.6216\n",
      "311/463, train_loss: 0.6030\n",
      "312/463, train_loss: 0.7090\n",
      "313/463, train_loss: 0.5728\n",
      "314/463, train_loss: 0.4968\n",
      "315/463, train_loss: 0.6084\n",
      "316/463, train_loss: 0.6011\n",
      "317/463, train_loss: 0.6572\n",
      "318/463, train_loss: 0.6172\n",
      "319/463, train_loss: 0.6982\n",
      "320/463, train_loss: 0.6968\n",
      "321/463, train_loss: 0.6191\n",
      "322/463, train_loss: 0.1957\n",
      "323/463, train_loss: 0.6006\n",
      "324/463, train_loss: 0.6792\n",
      "325/463, train_loss: 0.6738\n",
      "326/463, train_loss: 0.4946\n",
      "327/463, train_loss: 0.6162\n",
      "328/463, train_loss: 0.4617\n",
      "329/463, train_loss: 0.5215\n",
      "330/463, train_loss: 0.5654\n",
      "331/463, train_loss: 0.6333\n",
      "332/463, train_loss: 0.7617\n",
      "333/463, train_loss: 0.5732\n",
      "334/463, train_loss: 0.5596\n",
      "335/463, train_loss: 0.5264\n",
      "336/463, train_loss: 0.5889\n",
      "337/463, train_loss: 0.5571\n",
      "338/463, train_loss: 0.5938\n",
      "339/463, train_loss: 0.7485\n",
      "340/463, train_loss: 0.6162\n",
      "341/463, train_loss: 0.6768\n",
      "342/463, train_loss: 0.6997\n",
      "343/463, train_loss: 0.5977\n",
      "344/463, train_loss: 0.6621\n",
      "345/463, train_loss: 0.6372\n",
      "346/463, train_loss: 0.4363\n",
      "347/463, train_loss: 0.5088\n",
      "348/463, train_loss: 0.7026\n",
      "349/463, train_loss: 0.5322\n",
      "350/463, train_loss: 0.7324\n",
      "351/463, train_loss: 0.6553\n",
      "352/463, train_loss: 0.4778\n",
      "353/463, train_loss: 0.4072\n",
      "354/463, train_loss: 0.4705\n",
      "355/463, train_loss: 0.6030\n",
      "356/463, train_loss: 0.5464\n",
      "357/463, train_loss: 0.6548\n",
      "358/463, train_loss: 0.7656\n",
      "359/463, train_loss: 0.4985\n",
      "360/463, train_loss: 0.5068\n",
      "361/463, train_loss: 0.5605\n",
      "362/463, train_loss: 0.7231\n",
      "363/463, train_loss: 0.5547\n",
      "364/463, train_loss: 0.5396\n",
      "365/463, train_loss: 0.6982\n",
      "366/463, train_loss: 0.5396\n",
      "367/463, train_loss: 0.7080\n",
      "368/463, train_loss: 0.6147\n",
      "369/463, train_loss: 0.6943\n",
      "370/463, train_loss: 0.5986\n",
      "371/463, train_loss: 0.6416\n",
      "372/463, train_loss: 0.6943\n",
      "373/463, train_loss: 0.4788\n",
      "374/463, train_loss: 0.3999\n",
      "375/463, train_loss: 0.6133\n",
      "376/463, train_loss: 0.4141\n",
      "377/463, train_loss: 0.5234\n",
      "378/463, train_loss: 0.7778\n",
      "379/463, train_loss: 0.7881\n",
      "380/463, train_loss: 0.5303\n",
      "381/463, train_loss: 0.5132\n",
      "382/463, train_loss: 0.4880\n",
      "383/463, train_loss: 0.6431\n",
      "384/463, train_loss: 0.5410\n",
      "385/463, train_loss: 0.5010\n",
      "386/463, train_loss: 0.5576\n",
      "387/463, train_loss: 0.5469\n",
      "388/463, train_loss: 0.4883\n",
      "389/463, train_loss: 0.4277\n",
      "390/463, train_loss: 0.5679\n",
      "391/463, train_loss: 0.6807\n",
      "392/463, train_loss: 0.5410\n",
      "393/463, train_loss: 0.6484\n",
      "394/463, train_loss: 0.6562\n",
      "395/463, train_loss: 0.6953\n",
      "396/463, train_loss: 0.3521\n",
      "397/463, train_loss: 0.6172\n",
      "398/463, train_loss: 0.3765\n",
      "399/463, train_loss: 0.5059\n",
      "400/463, train_loss: 0.8975\n",
      "401/463, train_loss: 0.6631\n",
      "402/463, train_loss: 0.7070\n",
      "403/463, train_loss: 0.6387\n",
      "404/463, train_loss: 0.6592\n",
      "405/463, train_loss: 0.7520\n",
      "406/463, train_loss: 0.5981\n",
      "407/463, train_loss: 0.6123\n",
      "408/463, train_loss: 0.5210\n",
      "409/463, train_loss: 0.5371\n",
      "410/463, train_loss: 0.6318\n",
      "411/463, train_loss: 0.4971\n",
      "412/463, train_loss: 0.5840\n",
      "413/463, train_loss: 0.7334\n",
      "414/463, train_loss: 0.5376\n",
      "415/463, train_loss: 0.5527\n",
      "416/463, train_loss: 0.7861\n",
      "417/463, train_loss: 0.4409\n",
      "418/463, train_loss: 0.8188\n",
      "419/463, train_loss: 0.5498\n",
      "420/463, train_loss: 0.5117\n",
      "421/463, train_loss: 0.6572\n",
      "422/463, train_loss: 0.6670\n",
      "423/463, train_loss: 0.6094\n",
      "424/463, train_loss: 0.6846\n",
      "425/463, train_loss: 0.6533\n",
      "426/463, train_loss: 0.6982\n",
      "427/463, train_loss: 0.5542\n",
      "428/463, train_loss: 0.6284\n",
      "429/463, train_loss: 0.7041\n",
      "430/463, train_loss: 0.5439\n",
      "431/463, train_loss: 0.6133\n",
      "432/463, train_loss: 0.6670\n",
      "433/463, train_loss: 0.5117\n",
      "434/463, train_loss: 0.5342\n",
      "435/463, train_loss: 0.6074\n",
      "436/463, train_loss: 0.6699\n",
      "437/463, train_loss: 0.6431\n",
      "438/463, train_loss: 0.5977\n",
      "439/463, train_loss: 0.6133\n",
      "440/463, train_loss: 0.4890\n",
      "441/463, train_loss: 0.7812\n",
      "442/463, train_loss: 0.7876\n",
      "443/463, train_loss: 0.7383\n",
      "444/463, train_loss: 0.5669\n",
      "445/463, train_loss: 0.5127\n",
      "446/463, train_loss: 0.5127\n",
      "447/463, train_loss: 0.4993\n",
      "448/463, train_loss: 0.5044\n",
      "449/463, train_loss: 0.4976\n",
      "450/463, train_loss: 0.1395\n",
      "451/463, train_loss: 0.6318\n",
      "452/463, train_loss: 0.8301\n",
      "453/463, train_loss: 0.5723\n",
      "454/463, train_loss: 0.5972\n",
      "455/463, train_loss: 0.6011\n",
      "456/463, train_loss: 0.4441\n",
      "457/463, train_loss: 0.6802\n",
      "458/463, train_loss: 0.7056\n",
      "459/463, train_loss: 0.5986\n",
      "460/463, train_loss: 0.6880\n",
      "461/463, train_loss: 0.6201\n",
      "462/463, train_loss: 0.7710\n",
      "463/463, train_loss: 0.4497\n",
      "epoch 1 average loss: 0.6567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 06:01:07 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 06:01:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 2/100\n",
      "1/463, train_loss: 0.5210\n",
      "2/463, train_loss: 0.5825\n",
      "3/463, train_loss: 0.7500\n",
      "4/463, train_loss: 0.6016\n",
      "5/463, train_loss: 0.7520\n",
      "6/463, train_loss: 0.6543\n",
      "7/463, train_loss: 0.5947\n",
      "8/463, train_loss: 0.5342\n",
      "9/463, train_loss: 0.7490\n",
      "10/463, train_loss: 0.6055\n",
      "11/463, train_loss: 0.6465\n",
      "12/463, train_loss: 0.5337\n",
      "13/463, train_loss: 0.6865\n",
      "14/463, train_loss: 0.4043\n",
      "15/463, train_loss: 0.7573\n",
      "16/463, train_loss: 0.6128\n",
      "17/463, train_loss: 0.5449\n",
      "18/463, train_loss: 0.5933\n",
      "19/463, train_loss: 0.6377\n",
      "20/463, train_loss: 0.4551\n",
      "21/463, train_loss: 0.7363\n",
      "22/463, train_loss: 0.5278\n",
      "23/463, train_loss: 0.7686\n",
      "24/463, train_loss: 0.5542\n",
      "25/463, train_loss: 0.4312\n",
      "26/463, train_loss: 0.6133\n",
      "27/463, train_loss: 0.5806\n",
      "28/463, train_loss: 0.5771\n",
      "29/463, train_loss: 0.3804\n",
      "30/463, train_loss: 0.7607\n",
      "31/463, train_loss: 0.7729\n",
      "32/463, train_loss: 0.6050\n",
      "33/463, train_loss: 0.8184\n",
      "34/463, train_loss: 0.6616\n",
      "35/463, train_loss: 0.4937\n",
      "36/463, train_loss: 0.5386\n",
      "37/463, train_loss: 0.6455\n",
      "38/463, train_loss: 0.6934\n",
      "39/463, train_loss: 0.7603\n",
      "40/463, train_loss: 0.4888\n",
      "41/463, train_loss: 0.5269\n",
      "42/463, train_loss: 0.7207\n",
      "43/463, train_loss: 0.6875\n",
      "44/463, train_loss: 0.6025\n",
      "45/463, train_loss: 0.7202\n",
      "46/463, train_loss: 0.5151\n",
      "47/463, train_loss: 0.3506\n",
      "48/463, train_loss: 0.5459\n",
      "49/463, train_loss: 0.6523\n",
      "50/463, train_loss: 0.5654\n",
      "51/463, train_loss: 0.3770\n",
      "52/463, train_loss: 0.6621\n",
      "53/463, train_loss: 0.5742\n",
      "54/463, train_loss: 0.6631\n",
      "55/463, train_loss: 0.5791\n",
      "56/463, train_loss: 0.5161\n",
      "57/463, train_loss: 0.8384\n",
      "58/463, train_loss: 0.6304\n",
      "59/463, train_loss: 0.7075\n",
      "60/463, train_loss: 0.6289\n",
      "61/463, train_loss: 0.7188\n",
      "62/463, train_loss: 0.6992\n",
      "63/463, train_loss: 0.5977\n",
      "64/463, train_loss: 0.6230\n",
      "65/463, train_loss: 0.4683\n",
      "66/463, train_loss: 0.6904\n",
      "67/463, train_loss: 0.6567\n",
      "68/463, train_loss: 0.6670\n",
      "69/463, train_loss: 0.4778\n",
      "70/463, train_loss: 0.5986\n",
      "71/463, train_loss: 0.6587\n",
      "72/463, train_loss: 0.5391\n",
      "73/463, train_loss: 0.7637\n",
      "74/463, train_loss: 0.6904\n",
      "75/463, train_loss: 0.8076\n",
      "76/463, train_loss: 0.7275\n",
      "77/463, train_loss: 0.7144\n",
      "78/463, train_loss: 0.7495\n",
      "79/463, train_loss: 0.4385\n",
      "80/463, train_loss: 0.6221\n",
      "81/463, train_loss: 0.4607\n",
      "82/463, train_loss: 0.6143\n",
      "83/463, train_loss: 0.7769\n",
      "84/463, train_loss: 0.5854\n",
      "85/463, train_loss: 0.7324\n",
      "86/463, train_loss: 0.7656\n",
      "87/463, train_loss: 0.6270\n",
      "88/463, train_loss: 0.5381\n",
      "89/463, train_loss: 0.4126\n",
      "90/463, train_loss: 0.5811\n",
      "91/463, train_loss: 0.5317\n",
      "92/463, train_loss: 0.4138\n",
      "93/463, train_loss: 0.4519\n",
      "94/463, train_loss: 0.6636\n",
      "95/463, train_loss: 0.5190\n",
      "96/463, train_loss: 0.8242\n",
      "97/463, train_loss: 0.4712\n",
      "98/463, train_loss: 0.7109\n",
      "99/463, train_loss: 0.5356\n",
      "100/463, train_loss: 0.7271\n",
      "101/463, train_loss: 0.5244\n",
      "102/463, train_loss: 0.6367\n",
      "103/463, train_loss: 0.6675\n",
      "104/463, train_loss: 0.6167\n",
      "105/463, train_loss: 0.6377\n",
      "106/463, train_loss: 0.4924\n",
      "107/463, train_loss: 0.6567\n",
      "108/463, train_loss: 0.4438\n",
      "109/463, train_loss: 0.5464\n",
      "110/463, train_loss: 0.4922\n",
      "111/463, train_loss: 0.5200\n",
      "112/463, train_loss: 0.6401\n",
      "113/463, train_loss: 0.7041\n",
      "114/463, train_loss: 0.7607\n",
      "115/463, train_loss: 0.6880\n",
      "116/463, train_loss: 0.7432\n",
      "117/463, train_loss: 0.6216\n",
      "118/463, train_loss: 0.5864\n",
      "119/463, train_loss: 0.6191\n",
      "120/463, train_loss: 0.4075\n",
      "121/463, train_loss: 0.8232\n",
      "122/463, train_loss: 0.6982\n",
      "123/463, train_loss: 0.6924\n",
      "124/463, train_loss: 0.6904\n",
      "125/463, train_loss: 0.5176\n",
      "126/463, train_loss: 0.5220\n",
      "127/463, train_loss: 0.6289\n",
      "128/463, train_loss: 0.4783\n",
      "129/463, train_loss: 0.4370\n",
      "130/463, train_loss: 0.4712\n",
      "131/463, train_loss: 0.7998\n",
      "132/463, train_loss: 0.4631\n",
      "133/463, train_loss: 0.6021\n",
      "134/463, train_loss: 0.5220\n",
      "135/463, train_loss: 0.6167\n",
      "136/463, train_loss: 0.6377\n",
      "137/463, train_loss: 0.6992\n",
      "138/463, train_loss: 0.5059\n",
      "139/463, train_loss: 0.6699\n",
      "140/463, train_loss: 0.4226\n",
      "141/463, train_loss: 0.4321\n",
      "142/463, train_loss: 0.4385\n",
      "143/463, train_loss: 0.5249\n",
      "144/463, train_loss: 0.2910\n",
      "145/463, train_loss: 0.8594\n",
      "146/463, train_loss: 0.6855\n",
      "147/463, train_loss: 0.4431\n",
      "148/463, train_loss: 0.5488\n",
      "149/463, train_loss: 0.8184\n",
      "150/463, train_loss: 0.4612\n",
      "151/463, train_loss: 0.4895\n",
      "152/463, train_loss: 0.7080\n",
      "153/463, train_loss: 0.5312\n",
      "154/463, train_loss: 0.9214\n",
      "155/463, train_loss: 0.7832\n",
      "156/463, train_loss: 0.6499\n",
      "157/463, train_loss: 0.4761\n",
      "158/463, train_loss: 0.5938\n",
      "159/463, train_loss: 0.6372\n",
      "160/463, train_loss: 0.3801\n",
      "161/463, train_loss: 0.4448\n",
      "162/463, train_loss: 0.5366\n",
      "163/463, train_loss: 0.4702\n",
      "164/463, train_loss: 0.7583\n",
      "165/463, train_loss: 0.3472\n",
      "166/463, train_loss: 0.6035\n",
      "167/463, train_loss: 0.5557\n",
      "168/463, train_loss: 0.5508\n",
      "169/463, train_loss: 0.5249\n",
      "170/463, train_loss: 0.6548\n",
      "171/463, train_loss: 0.5103\n",
      "172/463, train_loss: 0.4580\n",
      "173/463, train_loss: 0.6270\n",
      "174/463, train_loss: 0.5703\n",
      "175/463, train_loss: 0.5317\n",
      "176/463, train_loss: 0.6680\n",
      "177/463, train_loss: 0.6460\n",
      "178/463, train_loss: 0.3542\n",
      "179/463, train_loss: 0.5146\n",
      "180/463, train_loss: 0.6074\n",
      "181/463, train_loss: 0.6616\n",
      "182/463, train_loss: 0.5522\n",
      "183/463, train_loss: 0.4436\n",
      "184/463, train_loss: 0.4824\n",
      "185/463, train_loss: 0.5288\n",
      "186/463, train_loss: 0.7783\n",
      "187/463, train_loss: 0.4644\n",
      "188/463, train_loss: 0.7231\n",
      "189/463, train_loss: 0.7891\n",
      "190/463, train_loss: 0.6489\n",
      "191/463, train_loss: 0.5322\n",
      "192/463, train_loss: 0.7642\n",
      "193/463, train_loss: 0.4180\n",
      "194/463, train_loss: 0.4270\n",
      "195/463, train_loss: 0.4521\n",
      "196/463, train_loss: 0.5371\n",
      "197/463, train_loss: 0.5674\n",
      "198/463, train_loss: 0.5083\n",
      "199/463, train_loss: 0.3760\n",
      "200/463, train_loss: 0.6421\n",
      "201/463, train_loss: 0.5645\n",
      "202/463, train_loss: 0.8164\n",
      "203/463, train_loss: 0.4722\n",
      "204/463, train_loss: 0.5771\n",
      "205/463, train_loss: 0.6426\n",
      "206/463, train_loss: 0.4995\n",
      "207/463, train_loss: 0.4543\n",
      "208/463, train_loss: 0.3621\n",
      "209/463, train_loss: 0.3911\n",
      "210/463, train_loss: 0.6260\n",
      "211/463, train_loss: 0.2922\n",
      "212/463, train_loss: 0.1107\n",
      "213/463, train_loss: 0.5166\n",
      "214/463, train_loss: 0.7080\n",
      "215/463, train_loss: 0.5425\n",
      "216/463, train_loss: 0.4846\n",
      "217/463, train_loss: 0.6631\n",
      "218/463, train_loss: 0.5757\n",
      "219/463, train_loss: 0.4868\n",
      "220/463, train_loss: 0.5059\n",
      "221/463, train_loss: 0.5693\n",
      "222/463, train_loss: 0.3508\n",
      "223/463, train_loss: 0.4773\n",
      "224/463, train_loss: 0.5449\n",
      "225/463, train_loss: 0.6538\n",
      "226/463, train_loss: 0.4946\n",
      "227/463, train_loss: 0.2708\n",
      "228/463, train_loss: 0.4268\n",
      "229/463, train_loss: 0.1584\n",
      "230/463, train_loss: 0.3252\n",
      "231/463, train_loss: 0.4028\n",
      "232/463, train_loss: 1.0430\n",
      "233/463, train_loss: 0.3467\n",
      "234/463, train_loss: 0.8984\n",
      "235/463, train_loss: 0.4519\n",
      "236/463, train_loss: 0.3701\n",
      "237/463, train_loss: 0.6953\n",
      "238/463, train_loss: 0.5859\n",
      "239/463, train_loss: 0.5928\n",
      "240/463, train_loss: 0.5376\n",
      "241/463, train_loss: 0.7305\n",
      "242/463, train_loss: 0.5269\n",
      "243/463, train_loss: 0.4990\n",
      "244/463, train_loss: 0.5532\n",
      "245/463, train_loss: 0.5376\n",
      "246/463, train_loss: 0.4116\n",
      "247/463, train_loss: 0.6021\n",
      "248/463, train_loss: 0.5835\n",
      "249/463, train_loss: 0.5381\n",
      "250/463, train_loss: 0.6357\n",
      "251/463, train_loss: 0.8218\n",
      "252/463, train_loss: 0.6694\n",
      "253/463, train_loss: 0.6372\n",
      "254/463, train_loss: 0.4758\n",
      "255/463, train_loss: 0.4648\n",
      "256/463, train_loss: 0.3906\n",
      "257/463, train_loss: 0.4531\n",
      "258/463, train_loss: 0.4504\n",
      "259/463, train_loss: 0.6733\n",
      "260/463, train_loss: 0.3311\n",
      "261/463, train_loss: 0.6152\n",
      "262/463, train_loss: 0.7061\n",
      "263/463, train_loss: 0.5889\n",
      "264/463, train_loss: 0.3833\n",
      "265/463, train_loss: 0.7310\n",
      "266/463, train_loss: 0.4390\n",
      "267/463, train_loss: 0.5220\n",
      "268/463, train_loss: 0.6118\n",
      "269/463, train_loss: 0.4189\n",
      "270/463, train_loss: 0.4495\n",
      "271/463, train_loss: 0.2905\n",
      "272/463, train_loss: 0.5869\n",
      "273/463, train_loss: 0.4746\n",
      "274/463, train_loss: 0.6016\n",
      "275/463, train_loss: 0.5859\n",
      "276/463, train_loss: 0.7070\n",
      "277/463, train_loss: 0.8521\n",
      "278/463, train_loss: 0.5029\n",
      "279/463, train_loss: 0.6758\n",
      "280/463, train_loss: 0.5698\n",
      "281/463, train_loss: 0.5059\n",
      "282/463, train_loss: 0.3459\n",
      "283/463, train_loss: 0.3804\n",
      "284/463, train_loss: 0.8076\n",
      "285/463, train_loss: 0.3787\n",
      "286/463, train_loss: 0.7021\n",
      "287/463, train_loss: 0.4250\n",
      "288/463, train_loss: 0.4326\n",
      "289/463, train_loss: 0.4539\n",
      "290/463, train_loss: 0.3940\n",
      "291/463, train_loss: 0.4287\n",
      "292/463, train_loss: 0.8281\n",
      "293/463, train_loss: 0.4875\n",
      "294/463, train_loss: 0.6597\n",
      "295/463, train_loss: 0.3677\n",
      "296/463, train_loss: 0.3428\n",
      "297/463, train_loss: 0.5439\n",
      "298/463, train_loss: 0.6465\n",
      "299/463, train_loss: 0.3923\n",
      "300/463, train_loss: 0.4814\n",
      "301/463, train_loss: 0.6284\n",
      "302/463, train_loss: 0.3433\n",
      "303/463, train_loss: 0.3960\n",
      "304/463, train_loss: 0.3376\n",
      "305/463, train_loss: 0.6128\n",
      "306/463, train_loss: 0.5830\n",
      "307/463, train_loss: 0.5986\n",
      "308/463, train_loss: 0.2935\n",
      "309/463, train_loss: 0.3679\n",
      "310/463, train_loss: 0.4277\n",
      "311/463, train_loss: 0.7549\n",
      "312/463, train_loss: 0.5400\n",
      "313/463, train_loss: 0.5674\n",
      "314/463, train_loss: 0.7295\n",
      "315/463, train_loss: 0.2642\n",
      "316/463, train_loss: 0.3853\n",
      "317/463, train_loss: 0.6201\n",
      "318/463, train_loss: 0.4790\n",
      "319/463, train_loss: 0.3716\n",
      "320/463, train_loss: 0.5806\n",
      "321/463, train_loss: 0.3071\n",
      "322/463, train_loss: 0.3364\n",
      "323/463, train_loss: 0.7725\n",
      "324/463, train_loss: 0.6738\n",
      "325/463, train_loss: 0.5664\n",
      "326/463, train_loss: 0.4897\n",
      "327/463, train_loss: 0.3853\n",
      "328/463, train_loss: 0.4009\n",
      "329/463, train_loss: 0.9141\n",
      "330/463, train_loss: 0.5303\n",
      "331/463, train_loss: 0.3931\n",
      "332/463, train_loss: 0.3523\n",
      "333/463, train_loss: 0.6772\n",
      "334/463, train_loss: 0.6470\n",
      "335/463, train_loss: 0.5928\n",
      "336/463, train_loss: 0.2598\n",
      "337/463, train_loss: 0.8203\n",
      "338/463, train_loss: 0.4285\n",
      "339/463, train_loss: 0.4033\n",
      "340/463, train_loss: 0.4480\n",
      "341/463, train_loss: 0.6885\n",
      "342/463, train_loss: 0.3994\n",
      "343/463, train_loss: 0.4688\n",
      "344/463, train_loss: 0.4846\n",
      "345/463, train_loss: 0.7173\n",
      "346/463, train_loss: 0.2646\n",
      "347/463, train_loss: 0.5195\n",
      "348/463, train_loss: 0.6128\n",
      "349/463, train_loss: 0.4885\n",
      "350/463, train_loss: 0.3618\n",
      "351/463, train_loss: 0.6113\n",
      "352/463, train_loss: 0.5659\n",
      "353/463, train_loss: 0.2983\n",
      "354/463, train_loss: 0.2715\n",
      "355/463, train_loss: 0.8481\n",
      "356/463, train_loss: 0.5518\n",
      "357/463, train_loss: 0.2383\n",
      "358/463, train_loss: 0.5762\n",
      "359/463, train_loss: 0.3486\n",
      "360/463, train_loss: 0.5728\n",
      "361/463, train_loss: 0.4119\n",
      "362/463, train_loss: 0.3918\n",
      "363/463, train_loss: 0.5415\n",
      "364/463, train_loss: 0.5801\n",
      "365/463, train_loss: 0.4138\n",
      "366/463, train_loss: 0.5522\n",
      "367/463, train_loss: 0.4778\n",
      "368/463, train_loss: 0.6538\n",
      "369/463, train_loss: 0.7671\n",
      "370/463, train_loss: 0.9463\n",
      "371/463, train_loss: 0.4778\n",
      "372/463, train_loss: 0.4939\n",
      "373/463, train_loss: 0.3438\n",
      "374/463, train_loss: 0.4360\n",
      "375/463, train_loss: 0.4285\n",
      "376/463, train_loss: 0.7295\n",
      "377/463, train_loss: 0.8193\n",
      "378/463, train_loss: 0.3628\n",
      "379/463, train_loss: 0.6841\n",
      "380/463, train_loss: 0.5015\n",
      "381/463, train_loss: 0.6001\n",
      "382/463, train_loss: 0.5581\n",
      "383/463, train_loss: 0.3208\n",
      "384/463, train_loss: 0.6021\n",
      "385/463, train_loss: 0.3760\n",
      "386/463, train_loss: 0.3652\n",
      "387/463, train_loss: 0.7031\n",
      "388/463, train_loss: 0.4292\n",
      "389/463, train_loss: 0.6099\n",
      "390/463, train_loss: 0.2290\n",
      "391/463, train_loss: 0.3462\n",
      "392/463, train_loss: 0.5088\n",
      "393/463, train_loss: 0.6426\n",
      "394/463, train_loss: 0.5430\n",
      "395/463, train_loss: 0.4575\n",
      "396/463, train_loss: 0.4722\n",
      "397/463, train_loss: 0.5010\n",
      "398/463, train_loss: 0.4536\n",
      "399/463, train_loss: 0.6719\n",
      "400/463, train_loss: 0.4915\n",
      "401/463, train_loss: 0.3655\n",
      "402/463, train_loss: 0.3918\n",
      "403/463, train_loss: 0.4272\n",
      "404/463, train_loss: 0.2949\n",
      "405/463, train_loss: 0.6172\n",
      "406/463, train_loss: 0.3052\n",
      "407/463, train_loss: 0.3313\n",
      "408/463, train_loss: 0.4741\n",
      "409/463, train_loss: 0.6611\n",
      "410/463, train_loss: 0.3330\n",
      "411/463, train_loss: 0.4929\n",
      "412/463, train_loss: 0.5161\n",
      "413/463, train_loss: 0.3149\n",
      "414/463, train_loss: 0.5078\n",
      "415/463, train_loss: 0.3413\n",
      "416/463, train_loss: 0.4255\n",
      "417/463, train_loss: 0.5713\n",
      "418/463, train_loss: 0.3599\n",
      "419/463, train_loss: 0.7178\n",
      "420/463, train_loss: 0.5586\n",
      "421/463, train_loss: 0.3257\n",
      "422/463, train_loss: 0.3032\n",
      "423/463, train_loss: 0.3604\n",
      "424/463, train_loss: 0.6445\n",
      "425/463, train_loss: 0.4917\n",
      "426/463, train_loss: 0.2598\n",
      "427/463, train_loss: 0.6362\n",
      "428/463, train_loss: 0.5488\n",
      "429/463, train_loss: 0.7803\n",
      "430/463, train_loss: 0.2617\n",
      "431/463, train_loss: 0.2935\n",
      "432/463, train_loss: 0.5728\n",
      "433/463, train_loss: 0.3301\n",
      "434/463, train_loss: 0.9292\n",
      "435/463, train_loss: 0.4539\n",
      "436/463, train_loss: 0.3191\n",
      "437/463, train_loss: 0.6016\n",
      "438/463, train_loss: 0.6592\n",
      "439/463, train_loss: 0.3035\n",
      "440/463, train_loss: 0.5498\n",
      "441/463, train_loss: 0.4609\n",
      "442/463, train_loss: 0.6079\n",
      "443/463, train_loss: 0.4468\n",
      "444/463, train_loss: 0.6807\n",
      "445/463, train_loss: 0.3679\n",
      "446/463, train_loss: 0.5361\n",
      "447/463, train_loss: 0.4109\n",
      "448/463, train_loss: 0.4023\n",
      "449/463, train_loss: 0.7002\n",
      "450/463, train_loss: 0.5347\n",
      "451/463, train_loss: 0.5645\n",
      "452/463, train_loss: 0.4153\n",
      "453/463, train_loss: 0.6528\n",
      "454/463, train_loss: 0.6631\n",
      "455/463, train_loss: 0.6133\n",
      "456/463, train_loss: 0.5151\n",
      "457/463, train_loss: 0.3604\n",
      "458/463, train_loss: 0.4448\n",
      "459/463, train_loss: 0.6265\n",
      "460/463, train_loss: 0.4231\n",
      "461/463, train_loss: 0.5923\n",
      "462/463, train_loss: 0.5757\n",
      "463/463, train_loss: 0.5015\n",
      "epoch 2 average loss: 0.5485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 08:14:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 08:14:50 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 3/100\n",
      "1/463, train_loss: 0.6772\n",
      "2/463, train_loss: 0.3438\n",
      "3/463, train_loss: 0.2708\n",
      "4/463, train_loss: 0.2263\n",
      "5/463, train_loss: 0.4644\n",
      "6/463, train_loss: 0.3750\n",
      "7/463, train_loss: 0.7588\n",
      "8/463, train_loss: 0.2141\n",
      "9/463, train_loss: 0.2717\n",
      "10/463, train_loss: 0.4111\n",
      "11/463, train_loss: 0.4399\n",
      "12/463, train_loss: 0.6338\n",
      "13/463, train_loss: 0.7871\n",
      "14/463, train_loss: 0.4341\n",
      "15/463, train_loss: 0.5020\n",
      "16/463, train_loss: 0.4668\n",
      "17/463, train_loss: 0.2776\n",
      "18/463, train_loss: 0.7881\n",
      "19/463, train_loss: 0.4004\n",
      "20/463, train_loss: 0.4614\n",
      "21/463, train_loss: 0.5132\n",
      "22/463, train_loss: 0.3274\n",
      "23/463, train_loss: 0.5581\n",
      "24/463, train_loss: 0.5605\n",
      "25/463, train_loss: 0.6201\n",
      "26/463, train_loss: 0.3008\n",
      "27/463, train_loss: 0.5298\n",
      "28/463, train_loss: 0.4277\n",
      "29/463, train_loss: 0.6938\n",
      "30/463, train_loss: 0.4531\n",
      "31/463, train_loss: 0.8223\n",
      "32/463, train_loss: 0.2412\n",
      "33/463, train_loss: 0.6216\n",
      "34/463, train_loss: 0.6064\n",
      "35/463, train_loss: 0.3394\n",
      "36/463, train_loss: 0.3594\n",
      "37/463, train_loss: 0.4058\n",
      "38/463, train_loss: 0.4653\n",
      "39/463, train_loss: 0.5273\n",
      "40/463, train_loss: 0.4578\n",
      "41/463, train_loss: 0.3511\n",
      "42/463, train_loss: 0.6040\n",
      "43/463, train_loss: 0.4194\n",
      "44/463, train_loss: 0.3936\n",
      "45/463, train_loss: 0.3159\n",
      "46/463, train_loss: 0.3018\n",
      "47/463, train_loss: 0.6289\n",
      "48/463, train_loss: 0.1469\n",
      "49/463, train_loss: 0.5752\n",
      "50/463, train_loss: 0.6157\n",
      "51/463, train_loss: 0.5742\n",
      "52/463, train_loss: 0.5674\n",
      "53/463, train_loss: 0.2134\n",
      "54/463, train_loss: 0.6460\n",
      "55/463, train_loss: 0.4473\n",
      "56/463, train_loss: 0.4517\n",
      "57/463, train_loss: 0.2715\n",
      "58/463, train_loss: 0.6523\n",
      "59/463, train_loss: 0.3494\n",
      "60/463, train_loss: 0.5361\n",
      "61/463, train_loss: 0.5225\n",
      "62/463, train_loss: 0.4114\n",
      "63/463, train_loss: 0.4077\n",
      "64/463, train_loss: 0.5039\n",
      "65/463, train_loss: 0.2480\n",
      "66/463, train_loss: 0.4402\n",
      "67/463, train_loss: 0.4968\n",
      "68/463, train_loss: 0.1919\n",
      "69/463, train_loss: 0.8975\n",
      "70/463, train_loss: 0.3992\n",
      "71/463, train_loss: 0.8594\n",
      "72/463, train_loss: 0.2388\n",
      "73/463, train_loss: 0.4194\n",
      "74/463, train_loss: 0.6533\n",
      "75/463, train_loss: 0.5063\n",
      "76/463, train_loss: 0.4141\n",
      "77/463, train_loss: 0.5024\n",
      "78/463, train_loss: 0.4600\n",
      "79/463, train_loss: 0.5503\n",
      "80/463, train_loss: 0.4600\n",
      "81/463, train_loss: 0.6416\n",
      "82/463, train_loss: 0.3257\n",
      "83/463, train_loss: 0.5859\n",
      "84/463, train_loss: 0.2115\n",
      "85/463, train_loss: 0.6621\n",
      "86/463, train_loss: 0.5342\n",
      "87/463, train_loss: 0.4504\n",
      "88/463, train_loss: 0.6206\n",
      "89/463, train_loss: 0.3792\n",
      "90/463, train_loss: 0.4583\n",
      "91/463, train_loss: 0.2363\n",
      "92/463, train_loss: 0.5249\n",
      "93/463, train_loss: 0.7402\n",
      "94/463, train_loss: 0.3232\n",
      "95/463, train_loss: 0.3369\n",
      "96/463, train_loss: 0.3540\n",
      "97/463, train_loss: 0.3701\n",
      "98/463, train_loss: 0.4375\n",
      "99/463, train_loss: 0.2168\n",
      "100/463, train_loss: 0.4431\n",
      "101/463, train_loss: 0.4053\n",
      "102/463, train_loss: 0.4443\n",
      "103/463, train_loss: 0.3599\n",
      "104/463, train_loss: 0.3032\n",
      "105/463, train_loss: 0.3975\n",
      "106/463, train_loss: 0.4866\n",
      "107/463, train_loss: 0.2416\n",
      "108/463, train_loss: 0.3066\n",
      "109/463, train_loss: 0.7212\n",
      "110/463, train_loss: 0.3203\n",
      "111/463, train_loss: 0.3347\n",
      "112/463, train_loss: 0.6704\n",
      "113/463, train_loss: 0.3516\n",
      "114/463, train_loss: 0.4209\n",
      "115/463, train_loss: 0.6201\n",
      "116/463, train_loss: 0.3330\n",
      "117/463, train_loss: 0.6851\n",
      "118/463, train_loss: 0.9604\n",
      "119/463, train_loss: 0.3440\n",
      "120/463, train_loss: 0.3992\n",
      "121/463, train_loss: 0.2048\n",
      "122/463, train_loss: 0.5122\n",
      "123/463, train_loss: 0.4033\n",
      "124/463, train_loss: 0.6528\n",
      "125/463, train_loss: 0.5225\n",
      "126/463, train_loss: 0.4727\n",
      "127/463, train_loss: 0.5356\n",
      "128/463, train_loss: 0.3955\n",
      "129/463, train_loss: 0.4949\n",
      "130/463, train_loss: 0.4338\n",
      "131/463, train_loss: 0.6533\n",
      "132/463, train_loss: 0.4856\n",
      "133/463, train_loss: 0.3071\n",
      "134/463, train_loss: 0.2876\n",
      "135/463, train_loss: 0.6387\n",
      "136/463, train_loss: 0.8877\n",
      "137/463, train_loss: 0.5371\n",
      "138/463, train_loss: 0.6602\n",
      "139/463, train_loss: 0.4858\n",
      "140/463, train_loss: 0.6675\n",
      "141/463, train_loss: 0.4917\n",
      "142/463, train_loss: 0.2617\n",
      "143/463, train_loss: 0.5840\n",
      "144/463, train_loss: 0.2988\n",
      "145/463, train_loss: 0.3179\n",
      "146/463, train_loss: 0.5557\n",
      "147/463, train_loss: 0.2932\n",
      "148/463, train_loss: 0.2827\n",
      "149/463, train_loss: 0.4302\n",
      "150/463, train_loss: 0.3115\n",
      "151/463, train_loss: 0.8242\n",
      "152/463, train_loss: 0.4185\n",
      "153/463, train_loss: 0.2930\n",
      "154/463, train_loss: 0.2854\n",
      "155/463, train_loss: 0.2590\n",
      "156/463, train_loss: 0.2678\n",
      "157/463, train_loss: 0.3191\n",
      "158/463, train_loss: 0.4404\n",
      "159/463, train_loss: 0.4458\n",
      "160/463, train_loss: 0.3298\n",
      "161/463, train_loss: 0.4724\n",
      "162/463, train_loss: 0.4160\n",
      "163/463, train_loss: 0.4270\n",
      "164/463, train_loss: 0.3987\n",
      "165/463, train_loss: 0.5889\n",
      "166/463, train_loss: 0.4006\n",
      "167/463, train_loss: 0.2546\n",
      "168/463, train_loss: 0.3726\n",
      "169/463, train_loss: 0.4360\n",
      "170/463, train_loss: 0.5107\n",
      "171/463, train_loss: 0.3167\n",
      "172/463, train_loss: 0.1895\n",
      "173/463, train_loss: 0.8818\n",
      "174/463, train_loss: 0.5562\n",
      "175/463, train_loss: 0.5029\n",
      "176/463, train_loss: 0.2527\n",
      "177/463, train_loss: 0.3428\n",
      "178/463, train_loss: 0.5278\n",
      "179/463, train_loss: 0.4058\n",
      "180/463, train_loss: 0.2239\n",
      "181/463, train_loss: 0.7241\n",
      "182/463, train_loss: 0.5054\n",
      "183/463, train_loss: 0.3467\n",
      "184/463, train_loss: 0.3611\n",
      "185/463, train_loss: 0.4690\n",
      "186/463, train_loss: 0.2651\n",
      "187/463, train_loss: 0.4233\n",
      "188/463, train_loss: 0.5732\n",
      "189/463, train_loss: 0.2593\n",
      "190/463, train_loss: 0.5151\n",
      "191/463, train_loss: 0.3364\n",
      "192/463, train_loss: 0.2551\n",
      "193/463, train_loss: 0.6758\n",
      "194/463, train_loss: 0.2478\n",
      "195/463, train_loss: 0.2202\n",
      "196/463, train_loss: 0.2092\n",
      "197/463, train_loss: 0.6055\n",
      "198/463, train_loss: 0.5967\n",
      "199/463, train_loss: 0.2336\n",
      "200/463, train_loss: 0.3865\n",
      "201/463, train_loss: 0.3142\n",
      "202/463, train_loss: 0.4783\n",
      "203/463, train_loss: 0.6113\n",
      "204/463, train_loss: 0.6514\n",
      "205/463, train_loss: 0.4365\n",
      "206/463, train_loss: 0.3911\n",
      "207/463, train_loss: 0.4832\n",
      "208/463, train_loss: 0.3733\n",
      "209/463, train_loss: 0.3521\n",
      "210/463, train_loss: 0.6611\n",
      "211/463, train_loss: 0.9214\n",
      "212/463, train_loss: 0.4268\n",
      "213/463, train_loss: 0.5498\n",
      "214/463, train_loss: 0.7617\n",
      "215/463, train_loss: 0.5283\n",
      "216/463, train_loss: 0.3530\n",
      "217/463, train_loss: 0.3914\n",
      "218/463, train_loss: 0.6084\n",
      "219/463, train_loss: 0.8008\n",
      "220/463, train_loss: 0.6074\n",
      "221/463, train_loss: 0.7534\n",
      "222/463, train_loss: 0.3083\n",
      "223/463, train_loss: 0.3652\n",
      "224/463, train_loss: 0.3672\n",
      "225/463, train_loss: 0.5122\n",
      "226/463, train_loss: 0.6162\n",
      "227/463, train_loss: 0.5747\n",
      "228/463, train_loss: 0.3289\n",
      "229/463, train_loss: 0.2295\n",
      "230/463, train_loss: 0.2832\n",
      "231/463, train_loss: 0.4663\n",
      "232/463, train_loss: 0.3247\n",
      "233/463, train_loss: 0.7441\n",
      "234/463, train_loss: 0.2410\n",
      "235/463, train_loss: 0.3818\n",
      "236/463, train_loss: 0.2517\n",
      "237/463, train_loss: 0.5381\n",
      "238/463, train_loss: 0.7505\n",
      "239/463, train_loss: 0.3726\n",
      "240/463, train_loss: 0.6182\n",
      "241/463, train_loss: 0.3330\n",
      "242/463, train_loss: 0.4016\n",
      "243/463, train_loss: 0.1780\n",
      "244/463, train_loss: 0.7427\n",
      "245/463, train_loss: 0.6030\n",
      "246/463, train_loss: 0.6045\n",
      "247/463, train_loss: 0.5259\n",
      "248/463, train_loss: 0.4646\n",
      "249/463, train_loss: 0.3269\n",
      "250/463, train_loss: 0.3477\n",
      "251/463, train_loss: 0.5015\n",
      "252/463, train_loss: 0.2646\n",
      "253/463, train_loss: 0.7432\n",
      "254/463, train_loss: 0.6421\n",
      "255/463, train_loss: 0.7153\n",
      "256/463, train_loss: 0.4658\n",
      "257/463, train_loss: 0.6943\n",
      "258/463, train_loss: 0.3044\n",
      "259/463, train_loss: 0.4756\n",
      "260/463, train_loss: 0.3145\n",
      "261/463, train_loss: 0.2896\n",
      "262/463, train_loss: 0.2267\n",
      "263/463, train_loss: 0.5488\n",
      "264/463, train_loss: 0.4072\n",
      "265/463, train_loss: 0.5312\n",
      "266/463, train_loss: 0.3220\n",
      "267/463, train_loss: 0.2317\n",
      "268/463, train_loss: 0.6533\n",
      "269/463, train_loss: 1.0645\n",
      "270/463, train_loss: 0.2141\n",
      "271/463, train_loss: 0.4231\n",
      "272/463, train_loss: 0.3030\n",
      "273/463, train_loss: 0.8774\n",
      "274/463, train_loss: 0.2168\n",
      "275/463, train_loss: 0.4849\n",
      "276/463, train_loss: 0.5884\n",
      "277/463, train_loss: 0.0946\n",
      "278/463, train_loss: 0.2157\n",
      "279/463, train_loss: 0.2423\n",
      "280/463, train_loss: 0.4048\n",
      "281/463, train_loss: 0.2419\n",
      "282/463, train_loss: 0.7153\n",
      "283/463, train_loss: 0.2001\n",
      "284/463, train_loss: 0.7998\n",
      "285/463, train_loss: 0.2632\n",
      "286/463, train_loss: 0.3799\n",
      "287/463, train_loss: 0.2286\n",
      "288/463, train_loss: 0.4373\n",
      "289/463, train_loss: 0.4883\n",
      "290/463, train_loss: 0.2114\n",
      "291/463, train_loss: 0.5596\n",
      "292/463, train_loss: 0.3970\n",
      "293/463, train_loss: 0.5859\n",
      "294/463, train_loss: 0.2622\n",
      "295/463, train_loss: 0.2072\n",
      "296/463, train_loss: 0.1880\n",
      "297/463, train_loss: 0.5444\n",
      "298/463, train_loss: 0.5527\n",
      "299/463, train_loss: 0.6387\n",
      "300/463, train_loss: 0.2377\n",
      "301/463, train_loss: 0.2472\n",
      "302/463, train_loss: 0.2383\n",
      "303/463, train_loss: 0.4395\n",
      "304/463, train_loss: 0.3213\n",
      "305/463, train_loss: 0.5981\n",
      "306/463, train_loss: 0.3091\n",
      "307/463, train_loss: 0.2272\n",
      "308/463, train_loss: 0.3804\n",
      "309/463, train_loss: 0.1880\n",
      "310/463, train_loss: 0.2517\n",
      "311/463, train_loss: 0.3857\n",
      "312/463, train_loss: 0.2458\n",
      "313/463, train_loss: 0.6201\n",
      "314/463, train_loss: 0.3894\n",
      "315/463, train_loss: 0.1785\n",
      "316/463, train_loss: 0.1937\n",
      "317/463, train_loss: 0.6055\n",
      "318/463, train_loss: 0.2585\n",
      "319/463, train_loss: 0.2107\n",
      "320/463, train_loss: 0.3779\n",
      "321/463, train_loss: 0.4824\n",
      "322/463, train_loss: 0.2064\n",
      "323/463, train_loss: 0.6177\n",
      "324/463, train_loss: 0.3821\n",
      "325/463, train_loss: 0.2085\n",
      "326/463, train_loss: 0.1592\n",
      "327/463, train_loss: 0.4963\n",
      "328/463, train_loss: 0.2114\n",
      "329/463, train_loss: 0.3337\n",
      "330/463, train_loss: 0.4382\n",
      "331/463, train_loss: 0.2905\n",
      "332/463, train_loss: 0.2729\n",
      "333/463, train_loss: 0.2808\n",
      "334/463, train_loss: 0.1960\n",
      "335/463, train_loss: 0.2651\n",
      "336/463, train_loss: 0.9146\n",
      "337/463, train_loss: 0.3354\n",
      "338/463, train_loss: 0.5244\n",
      "339/463, train_loss: 0.5640\n",
      "340/463, train_loss: 0.5283\n",
      "341/463, train_loss: 0.3120\n",
      "342/463, train_loss: 0.7065\n",
      "343/463, train_loss: 0.6709\n",
      "344/463, train_loss: 0.3977\n",
      "345/463, train_loss: 0.1726\n",
      "346/463, train_loss: 0.3965\n",
      "347/463, train_loss: 0.5044\n",
      "348/463, train_loss: 0.2153\n",
      "349/463, train_loss: 0.5430\n",
      "350/463, train_loss: 0.5112\n",
      "351/463, train_loss: 0.2935\n",
      "352/463, train_loss: 0.2864\n",
      "353/463, train_loss: 0.5420\n",
      "354/463, train_loss: 0.4746\n",
      "355/463, train_loss: 0.3018\n",
      "356/463, train_loss: 0.2749\n",
      "357/463, train_loss: 0.6089\n",
      "358/463, train_loss: 0.3179\n",
      "359/463, train_loss: 0.4001\n",
      "360/463, train_loss: 0.6675\n",
      "361/463, train_loss: 0.3340\n",
      "362/463, train_loss: 0.2795\n",
      "363/463, train_loss: 0.3511\n",
      "364/463, train_loss: 0.5615\n",
      "365/463, train_loss: 0.1863\n",
      "366/463, train_loss: 0.3979\n",
      "367/463, train_loss: 0.2925\n",
      "368/463, train_loss: 0.5869\n",
      "369/463, train_loss: 0.3926\n",
      "370/463, train_loss: 0.5732\n",
      "371/463, train_loss: 0.3564\n",
      "372/463, train_loss: 0.4421\n",
      "373/463, train_loss: 0.3057\n",
      "374/463, train_loss: 0.2908\n",
      "375/463, train_loss: 0.6162\n",
      "376/463, train_loss: 0.5586\n",
      "377/463, train_loss: 0.2439\n",
      "378/463, train_loss: 0.3994\n",
      "379/463, train_loss: 0.4329\n",
      "380/463, train_loss: 0.2422\n",
      "381/463, train_loss: 0.2030\n",
      "382/463, train_loss: 0.0729\n",
      "383/463, train_loss: 0.3687\n",
      "384/463, train_loss: 0.5293\n",
      "385/463, train_loss: 0.5869\n",
      "386/463, train_loss: 0.3962\n",
      "387/463, train_loss: 0.5205\n",
      "388/463, train_loss: 0.3821\n",
      "389/463, train_loss: 0.4458\n",
      "390/463, train_loss: 0.3130\n",
      "391/463, train_loss: 0.1938\n",
      "392/463, train_loss: 0.3728\n",
      "393/463, train_loss: 0.4712\n",
      "394/463, train_loss: 0.2383\n",
      "395/463, train_loss: 0.1934\n",
      "396/463, train_loss: 0.3320\n",
      "397/463, train_loss: 0.4785\n",
      "398/463, train_loss: 0.2349\n",
      "399/463, train_loss: 0.4792\n",
      "400/463, train_loss: 0.2834\n",
      "401/463, train_loss: 0.4153\n",
      "402/463, train_loss: 0.2341\n",
      "403/463, train_loss: 0.2075\n",
      "404/463, train_loss: 0.4331\n",
      "405/463, train_loss: 0.2318\n",
      "406/463, train_loss: 0.2939\n",
      "407/463, train_loss: 0.5444\n",
      "408/463, train_loss: 0.4946\n",
      "409/463, train_loss: 0.4263\n",
      "410/463, train_loss: 0.4011\n",
      "411/463, train_loss: 0.2822\n",
      "412/463, train_loss: 0.2568\n",
      "413/463, train_loss: 0.5117\n",
      "414/463, train_loss: 0.2202\n",
      "415/463, train_loss: 0.3386\n",
      "416/463, train_loss: 0.2798\n",
      "417/463, train_loss: 0.2114\n",
      "418/463, train_loss: 0.2205\n",
      "419/463, train_loss: 0.2900\n",
      "420/463, train_loss: 0.2267\n",
      "421/463, train_loss: 0.2512\n",
      "422/463, train_loss: 0.3352\n",
      "423/463, train_loss: 0.2008\n",
      "424/463, train_loss: 0.3584\n",
      "425/463, train_loss: 0.3425\n",
      "426/463, train_loss: 0.1976\n",
      "427/463, train_loss: 0.5933\n",
      "428/463, train_loss: 0.2085\n",
      "429/463, train_loss: 0.3232\n",
      "430/463, train_loss: 0.5503\n",
      "431/463, train_loss: 0.1705\n",
      "432/463, train_loss: 0.8652\n",
      "433/463, train_loss: 0.9771\n",
      "434/463, train_loss: 0.5615\n",
      "435/463, train_loss: 0.2031\n",
      "436/463, train_loss: 0.2114\n",
      "437/463, train_loss: 0.2749\n",
      "438/463, train_loss: 0.3147\n",
      "439/463, train_loss: 0.3364\n",
      "440/463, train_loss: 0.1743\n",
      "441/463, train_loss: 0.3435\n",
      "442/463, train_loss: 0.3208\n",
      "443/463, train_loss: 0.7344\n",
      "444/463, train_loss: 0.2625\n",
      "445/463, train_loss: 1.1963\n",
      "446/463, train_loss: 0.2097\n",
      "447/463, train_loss: 0.3306\n",
      "448/463, train_loss: 0.4309\n",
      "449/463, train_loss: 0.2646\n",
      "450/463, train_loss: 0.5068\n",
      "451/463, train_loss: 0.1973\n",
      "452/463, train_loss: 0.3057\n",
      "453/463, train_loss: 0.1389\n",
      "454/463, train_loss: 0.3809\n",
      "455/463, train_loss: 0.1952\n",
      "456/463, train_loss: 0.1984\n",
      "457/463, train_loss: 0.3589\n",
      "458/463, train_loss: 0.0815\n",
      "459/463, train_loss: 0.1973\n",
      "460/463, train_loss: 0.3203\n",
      "461/463, train_loss: 0.2144\n",
      "462/463, train_loss: 0.1108\n",
      "463/463, train_loss: 0.5967\n",
      "epoch 3 average loss: 0.4231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 10:28:00 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 10:28:03 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 4/100\n",
      "1/463, train_loss: 0.1490\n",
      "2/463, train_loss: 0.6602\n",
      "3/463, train_loss: 0.1459\n",
      "4/463, train_loss: 0.1511\n",
      "5/463, train_loss: 0.3176\n",
      "6/463, train_loss: 0.3875\n",
      "7/463, train_loss: 0.3721\n",
      "8/463, train_loss: 0.2795\n",
      "9/463, train_loss: 0.4585\n",
      "10/463, train_loss: 0.2222\n",
      "11/463, train_loss: 0.8271\n",
      "12/463, train_loss: 0.3213\n",
      "13/463, train_loss: 0.4795\n",
      "14/463, train_loss: 0.2313\n",
      "15/463, train_loss: 0.1770\n",
      "16/463, train_loss: 0.5728\n",
      "17/463, train_loss: 0.3938\n",
      "18/463, train_loss: 0.4434\n",
      "19/463, train_loss: 0.7583\n",
      "20/463, train_loss: 0.3062\n",
      "21/463, train_loss: 0.3762\n",
      "22/463, train_loss: 0.3389\n",
      "23/463, train_loss: 0.3118\n",
      "24/463, train_loss: 0.3208\n",
      "25/463, train_loss: 0.2483\n",
      "26/463, train_loss: 0.3574\n",
      "27/463, train_loss: 0.3213\n",
      "28/463, train_loss: 0.3567\n",
      "29/463, train_loss: 0.2125\n",
      "30/463, train_loss: 0.3020\n",
      "31/463, train_loss: 0.7402\n",
      "32/463, train_loss: 0.4888\n",
      "33/463, train_loss: 0.3193\n",
      "34/463, train_loss: 0.3018\n",
      "35/463, train_loss: 0.3220\n",
      "36/463, train_loss: 0.4412\n",
      "37/463, train_loss: 0.2632\n",
      "38/463, train_loss: 0.2891\n",
      "39/463, train_loss: 0.1857\n",
      "40/463, train_loss: 0.5977\n",
      "41/463, train_loss: 0.3286\n",
      "42/463, train_loss: 0.1082\n",
      "43/463, train_loss: 0.3242\n",
      "44/463, train_loss: 0.2469\n",
      "45/463, train_loss: 0.2173\n",
      "46/463, train_loss: 0.2583\n",
      "47/463, train_loss: 0.4487\n",
      "48/463, train_loss: 0.2109\n",
      "49/463, train_loss: 0.2969\n",
      "50/463, train_loss: 0.4299\n",
      "51/463, train_loss: 0.6182\n",
      "52/463, train_loss: 0.6519\n",
      "53/463, train_loss: 0.2815\n",
      "54/463, train_loss: 0.5776\n",
      "55/463, train_loss: 0.4087\n",
      "56/463, train_loss: 0.1292\n",
      "57/463, train_loss: 0.8955\n",
      "58/463, train_loss: 0.6118\n",
      "59/463, train_loss: 0.3618\n",
      "60/463, train_loss: 0.5840\n",
      "61/463, train_loss: 0.2086\n",
      "62/463, train_loss: 0.2737\n",
      "63/463, train_loss: 0.3164\n",
      "64/463, train_loss: 0.3062\n",
      "65/463, train_loss: 0.2255\n",
      "66/463, train_loss: 0.4072\n",
      "67/463, train_loss: 0.6631\n",
      "68/463, train_loss: 0.3218\n",
      "69/463, train_loss: 0.3347\n",
      "70/463, train_loss: 0.2026\n",
      "71/463, train_loss: 0.3828\n",
      "72/463, train_loss: 0.6333\n",
      "73/463, train_loss: 0.9238\n",
      "74/463, train_loss: 0.3621\n",
      "75/463, train_loss: 0.3943\n",
      "76/463, train_loss: 0.2319\n",
      "77/463, train_loss: 0.1687\n",
      "78/463, train_loss: 0.3984\n",
      "79/463, train_loss: 0.2428\n",
      "80/463, train_loss: 0.1506\n",
      "81/463, train_loss: 0.4160\n",
      "82/463, train_loss: 0.1598\n",
      "83/463, train_loss: 0.5835\n",
      "84/463, train_loss: 0.3799\n",
      "85/463, train_loss: 0.3044\n",
      "86/463, train_loss: 0.5200\n",
      "87/463, train_loss: 0.3911\n",
      "88/463, train_loss: 0.8228\n",
      "89/463, train_loss: 0.1824\n",
      "90/463, train_loss: 0.2217\n",
      "91/463, train_loss: 0.3599\n",
      "92/463, train_loss: 0.2842\n",
      "93/463, train_loss: 0.5474\n",
      "94/463, train_loss: 0.6963\n",
      "95/463, train_loss: 0.2251\n",
      "96/463, train_loss: 0.4268\n",
      "97/463, train_loss: 0.2925\n",
      "98/463, train_loss: 0.3159\n",
      "99/463, train_loss: 0.2803\n",
      "100/463, train_loss: 0.5942\n",
      "101/463, train_loss: 0.3254\n",
      "102/463, train_loss: 0.3892\n",
      "103/463, train_loss: 0.2500\n",
      "104/463, train_loss: 0.2576\n",
      "105/463, train_loss: 0.2426\n",
      "106/463, train_loss: 0.9482\n",
      "107/463, train_loss: 0.3821\n",
      "108/463, train_loss: 1.0049\n",
      "109/463, train_loss: 0.2747\n",
      "110/463, train_loss: 0.4551\n",
      "111/463, train_loss: 0.4277\n",
      "112/463, train_loss: 0.2233\n",
      "113/463, train_loss: 0.3511\n",
      "114/463, train_loss: 0.5469\n",
      "115/463, train_loss: 0.4580\n",
      "116/463, train_loss: 0.1694\n",
      "117/463, train_loss: 0.3105\n",
      "118/463, train_loss: 0.3120\n",
      "119/463, train_loss: 0.5747\n",
      "120/463, train_loss: 0.3875\n",
      "121/463, train_loss: 0.6094\n",
      "122/463, train_loss: 0.2759\n",
      "123/463, train_loss: 0.5737\n",
      "124/463, train_loss: 0.3259\n",
      "125/463, train_loss: 0.4189\n",
      "126/463, train_loss: 0.2661\n",
      "127/463, train_loss: 0.3750\n",
      "128/463, train_loss: 0.5303\n",
      "129/463, train_loss: 0.3132\n",
      "130/463, train_loss: 0.5898\n",
      "131/463, train_loss: 0.2109\n",
      "132/463, train_loss: 0.3027\n",
      "133/463, train_loss: 0.6992\n",
      "134/463, train_loss: 0.2443\n",
      "135/463, train_loss: 0.5518\n",
      "136/463, train_loss: 0.2671\n",
      "137/463, train_loss: 0.3774\n",
      "138/463, train_loss: 0.4070\n",
      "139/463, train_loss: 0.1919\n",
      "140/463, train_loss: 0.2185\n",
      "141/463, train_loss: 0.4807\n",
      "142/463, train_loss: 0.6499\n",
      "143/463, train_loss: 0.2637\n",
      "144/463, train_loss: 0.2588\n",
      "145/463, train_loss: 0.2690\n",
      "146/463, train_loss: 0.1385\n",
      "147/463, train_loss: 0.5068\n",
      "148/463, train_loss: 0.1909\n",
      "149/463, train_loss: 0.3555\n",
      "150/463, train_loss: 0.2502\n",
      "151/463, train_loss: 0.2102\n",
      "152/463, train_loss: 0.2278\n",
      "153/463, train_loss: 0.2144\n",
      "154/463, train_loss: 0.1625\n",
      "155/463, train_loss: 0.7358\n",
      "156/463, train_loss: 0.5942\n",
      "157/463, train_loss: 0.4277\n",
      "158/463, train_loss: 0.1694\n",
      "159/463, train_loss: 0.3052\n",
      "160/463, train_loss: 0.3584\n",
      "161/463, train_loss: 0.2739\n",
      "162/463, train_loss: 0.1750\n",
      "163/463, train_loss: 0.6162\n",
      "164/463, train_loss: 0.3911\n",
      "165/463, train_loss: 0.1843\n",
      "166/463, train_loss: 0.1611\n",
      "167/463, train_loss: 0.6030\n",
      "168/463, train_loss: 0.2505\n",
      "169/463, train_loss: 0.2666\n",
      "170/463, train_loss: 0.7251\n",
      "171/463, train_loss: 0.2847\n",
      "172/463, train_loss: 0.3267\n",
      "173/463, train_loss: 0.8921\n",
      "174/463, train_loss: 0.2598\n",
      "175/463, train_loss: 0.6401\n",
      "176/463, train_loss: 0.1139\n",
      "177/463, train_loss: 0.2639\n",
      "178/463, train_loss: 0.7217\n",
      "179/463, train_loss: 0.3787\n",
      "180/463, train_loss: 0.2837\n",
      "181/463, train_loss: 0.2173\n",
      "182/463, train_loss: 0.1633\n",
      "183/463, train_loss: 0.2380\n",
      "184/463, train_loss: 0.2051\n",
      "185/463, train_loss: 0.5210\n",
      "186/463, train_loss: 0.4438\n",
      "187/463, train_loss: 0.2686\n",
      "188/463, train_loss: 0.2244\n",
      "189/463, train_loss: 0.3564\n",
      "190/463, train_loss: 0.3821\n",
      "191/463, train_loss: 0.2241\n",
      "192/463, train_loss: 0.3574\n",
      "193/463, train_loss: 0.2317\n",
      "194/463, train_loss: 0.5996\n",
      "195/463, train_loss: 0.4014\n",
      "196/463, train_loss: 0.4961\n",
      "197/463, train_loss: 0.1857\n",
      "198/463, train_loss: 0.2020\n",
      "199/463, train_loss: 0.5742\n",
      "200/463, train_loss: 0.2107\n",
      "201/463, train_loss: 0.2539\n",
      "202/463, train_loss: 0.5107\n",
      "203/463, train_loss: 0.4231\n",
      "204/463, train_loss: 0.2678\n",
      "205/463, train_loss: 0.7505\n",
      "206/463, train_loss: 0.6479\n",
      "207/463, train_loss: 0.2441\n",
      "208/463, train_loss: 0.1958\n",
      "209/463, train_loss: 0.4204\n",
      "210/463, train_loss: 0.4829\n",
      "211/463, train_loss: 0.2283\n",
      "212/463, train_loss: 0.4629\n",
      "213/463, train_loss: 0.3286\n",
      "214/463, train_loss: 0.3069\n",
      "215/463, train_loss: 0.6934\n",
      "216/463, train_loss: 0.2045\n",
      "217/463, train_loss: 0.2109\n",
      "218/463, train_loss: 0.4128\n",
      "219/463, train_loss: 0.4092\n",
      "220/463, train_loss: 0.4561\n",
      "221/463, train_loss: 0.6982\n",
      "222/463, train_loss: 0.1704\n",
      "223/463, train_loss: 0.1885\n",
      "224/463, train_loss: 0.3713\n",
      "225/463, train_loss: 0.1929\n",
      "226/463, train_loss: 0.3364\n",
      "227/463, train_loss: 0.6680\n",
      "228/463, train_loss: 0.3850\n",
      "229/463, train_loss: 0.8823\n",
      "230/463, train_loss: 0.2922\n",
      "231/463, train_loss: 0.5869\n",
      "232/463, train_loss: 0.3672\n",
      "233/463, train_loss: 0.2229\n",
      "234/463, train_loss: 0.2881\n",
      "235/463, train_loss: 0.3108\n",
      "236/463, train_loss: 0.4004\n",
      "237/463, train_loss: 0.5830\n",
      "238/463, train_loss: 0.1904\n",
      "239/463, train_loss: 0.3062\n",
      "240/463, train_loss: 0.2725\n",
      "241/463, train_loss: 0.4751\n",
      "242/463, train_loss: 0.2217\n",
      "243/463, train_loss: 0.1838\n",
      "244/463, train_loss: 0.5361\n",
      "245/463, train_loss: 0.3669\n",
      "246/463, train_loss: 0.4590\n",
      "247/463, train_loss: 0.4324\n",
      "248/463, train_loss: 0.7207\n",
      "249/463, train_loss: 0.6475\n",
      "250/463, train_loss: 0.2966\n",
      "251/463, train_loss: 0.3923\n",
      "252/463, train_loss: 0.2930\n",
      "253/463, train_loss: 0.4624\n",
      "254/463, train_loss: 0.3286\n",
      "255/463, train_loss: 0.3086\n",
      "256/463, train_loss: 0.4163\n",
      "257/463, train_loss: 0.1731\n",
      "258/463, train_loss: 0.1536\n",
      "259/463, train_loss: 0.3818\n",
      "260/463, train_loss: 0.2427\n",
      "261/463, train_loss: 0.1216\n",
      "262/463, train_loss: 0.1614\n",
      "263/463, train_loss: 0.6143\n",
      "264/463, train_loss: 0.3818\n",
      "265/463, train_loss: 0.3242\n",
      "266/463, train_loss: 0.4600\n",
      "267/463, train_loss: 0.2473\n",
      "268/463, train_loss: 0.3904\n",
      "269/463, train_loss: 0.3777\n",
      "270/463, train_loss: 0.4316\n",
      "271/463, train_loss: 0.3088\n",
      "272/463, train_loss: 0.0897\n",
      "273/463, train_loss: 0.2188\n",
      "274/463, train_loss: 0.2681\n",
      "275/463, train_loss: 0.1667\n",
      "276/463, train_loss: 0.3652\n",
      "277/463, train_loss: 0.2009\n",
      "278/463, train_loss: 0.1801\n",
      "279/463, train_loss: 0.3047\n",
      "280/463, train_loss: 0.0876\n",
      "281/463, train_loss: 0.6533\n",
      "282/463, train_loss: 0.5356\n",
      "283/463, train_loss: 0.3584\n",
      "284/463, train_loss: 0.8984\n",
      "285/463, train_loss: 0.4822\n",
      "286/463, train_loss: 0.5342\n",
      "287/463, train_loss: 0.4761\n",
      "288/463, train_loss: 0.3562\n",
      "289/463, train_loss: 0.2598\n",
      "290/463, train_loss: 0.2151\n",
      "291/463, train_loss: 0.3499\n",
      "292/463, train_loss: 0.8940\n",
      "293/463, train_loss: 0.3220\n",
      "294/463, train_loss: 0.1362\n",
      "295/463, train_loss: 0.4795\n",
      "296/463, train_loss: 0.4468\n",
      "297/463, train_loss: 0.4043\n",
      "298/463, train_loss: 0.2231\n",
      "299/463, train_loss: 0.2632\n",
      "300/463, train_loss: 0.1210\n",
      "301/463, train_loss: 0.6372\n",
      "302/463, train_loss: 0.2385\n",
      "303/463, train_loss: 0.6812\n",
      "304/463, train_loss: 0.6738\n",
      "305/463, train_loss: 0.2412\n",
      "306/463, train_loss: 0.2144\n",
      "307/463, train_loss: 0.3123\n",
      "308/463, train_loss: 0.3872\n",
      "309/463, train_loss: 0.2111\n",
      "310/463, train_loss: 0.4607\n",
      "311/463, train_loss: 0.3643\n",
      "312/463, train_loss: 0.1792\n",
      "313/463, train_loss: 0.2683\n",
      "314/463, train_loss: 0.4517\n",
      "315/463, train_loss: 0.5449\n",
      "316/463, train_loss: 0.3457\n",
      "317/463, train_loss: 0.1902\n",
      "318/463, train_loss: 0.3738\n",
      "319/463, train_loss: 0.3074\n",
      "320/463, train_loss: 0.4521\n",
      "321/463, train_loss: 0.2385\n",
      "322/463, train_loss: 0.3315\n",
      "323/463, train_loss: 0.6787\n",
      "324/463, train_loss: 0.3313\n",
      "325/463, train_loss: 0.4453\n",
      "326/463, train_loss: 0.3501\n",
      "327/463, train_loss: 0.3293\n",
      "328/463, train_loss: 0.2310\n",
      "329/463, train_loss: 0.5791\n",
      "330/463, train_loss: 1.0938\n",
      "331/463, train_loss: 0.3030\n",
      "332/463, train_loss: 0.3621\n",
      "333/463, train_loss: 0.3833\n",
      "334/463, train_loss: 0.2659\n",
      "335/463, train_loss: 0.1758\n",
      "336/463, train_loss: 0.2284\n",
      "337/463, train_loss: 0.2510\n",
      "338/463, train_loss: 0.3035\n",
      "339/463, train_loss: 0.3979\n",
      "340/463, train_loss: 0.1877\n",
      "341/463, train_loss: 0.2983\n",
      "342/463, train_loss: 0.4370\n",
      "343/463, train_loss: 0.2744\n",
      "344/463, train_loss: 0.1128\n",
      "345/463, train_loss: 0.2427\n",
      "346/463, train_loss: 0.2224\n",
      "347/463, train_loss: 0.3130\n",
      "348/463, train_loss: 0.1888\n",
      "349/463, train_loss: 0.5933\n",
      "350/463, train_loss: 0.1666\n",
      "351/463, train_loss: 0.1479\n",
      "352/463, train_loss: 0.3420\n",
      "353/463, train_loss: 0.2891\n",
      "354/463, train_loss: 0.1881\n",
      "355/463, train_loss: 0.1858\n",
      "356/463, train_loss: 0.5200\n",
      "357/463, train_loss: 0.2107\n",
      "358/463, train_loss: 0.5781\n",
      "359/463, train_loss: 0.0438\n",
      "360/463, train_loss: 0.1482\n",
      "361/463, train_loss: 0.2820\n",
      "362/463, train_loss: 0.3567\n",
      "363/463, train_loss: 0.1609\n",
      "364/463, train_loss: 0.6221\n",
      "365/463, train_loss: 0.1702\n",
      "366/463, train_loss: 0.1660\n",
      "367/463, train_loss: 0.3740\n",
      "368/463, train_loss: 0.2023\n",
      "369/463, train_loss: 0.4004\n",
      "370/463, train_loss: 0.2224\n",
      "371/463, train_loss: 0.3682\n",
      "372/463, train_loss: 0.1709\n",
      "373/463, train_loss: 0.3018\n",
      "374/463, train_loss: 0.4622\n",
      "375/463, train_loss: 0.1670\n",
      "376/463, train_loss: 0.8770\n",
      "377/463, train_loss: 0.2734\n",
      "378/463, train_loss: 0.2090\n",
      "379/463, train_loss: 0.5264\n",
      "380/463, train_loss: 0.6255\n",
      "381/463, train_loss: 0.2688\n",
      "382/463, train_loss: 0.5264\n",
      "383/463, train_loss: 0.6689\n",
      "384/463, train_loss: 0.3044\n",
      "385/463, train_loss: 0.2983\n",
      "386/463, train_loss: 0.2478\n",
      "387/463, train_loss: 0.2394\n",
      "388/463, train_loss: 0.1777\n",
      "389/463, train_loss: 0.2092\n",
      "390/463, train_loss: 0.4277\n",
      "391/463, train_loss: 0.6992\n",
      "392/463, train_loss: 0.3989\n",
      "393/463, train_loss: 0.1533\n",
      "394/463, train_loss: 0.1814\n",
      "395/463, train_loss: 0.3867\n",
      "396/463, train_loss: 0.3042\n",
      "397/463, train_loss: 0.2573\n",
      "398/463, train_loss: 0.2661\n",
      "399/463, train_loss: 0.4927\n",
      "400/463, train_loss: 0.2512\n",
      "401/463, train_loss: 0.1863\n",
      "402/463, train_loss: 0.4956\n",
      "403/463, train_loss: 0.2668\n",
      "404/463, train_loss: 0.2527\n",
      "405/463, train_loss: 0.3459\n",
      "406/463, train_loss: 0.4270\n",
      "407/463, train_loss: 0.9639\n",
      "408/463, train_loss: 0.5537\n",
      "409/463, train_loss: 0.3706\n",
      "410/463, train_loss: 0.3682\n",
      "411/463, train_loss: 0.1998\n",
      "412/463, train_loss: 0.2244\n",
      "413/463, train_loss: 0.2212\n",
      "414/463, train_loss: 0.7285\n",
      "415/463, train_loss: 0.1686\n",
      "416/463, train_loss: 0.2654\n",
      "417/463, train_loss: 0.2524\n",
      "418/463, train_loss: 0.2380\n",
      "419/463, train_loss: 0.3647\n",
      "420/463, train_loss: 0.5850\n",
      "421/463, train_loss: 0.2292\n",
      "422/463, train_loss: 0.4358\n",
      "423/463, train_loss: 0.4692\n",
      "424/463, train_loss: 0.2280\n",
      "425/463, train_loss: 0.5459\n",
      "426/463, train_loss: 0.7061\n",
      "427/463, train_loss: 0.1510\n",
      "428/463, train_loss: 0.1926\n",
      "429/463, train_loss: 0.2566\n",
      "430/463, train_loss: 0.2998\n",
      "431/463, train_loss: 0.3120\n",
      "432/463, train_loss: 0.3252\n",
      "433/463, train_loss: 0.1458\n",
      "434/463, train_loss: 0.3467\n",
      "435/463, train_loss: 0.1680\n",
      "436/463, train_loss: 0.3198\n",
      "437/463, train_loss: 0.1552\n",
      "438/463, train_loss: 0.2910\n",
      "439/463, train_loss: 0.7871\n",
      "440/463, train_loss: 0.5366\n",
      "441/463, train_loss: 0.4282\n",
      "442/463, train_loss: 0.1284\n",
      "443/463, train_loss: 0.4299\n",
      "444/463, train_loss: 0.4312\n",
      "445/463, train_loss: 0.2122\n",
      "446/463, train_loss: 0.2847\n",
      "447/463, train_loss: 0.4128\n",
      "448/463, train_loss: 0.9404\n",
      "449/463, train_loss: 0.4753\n",
      "450/463, train_loss: 0.1081\n",
      "451/463, train_loss: 0.5371\n",
      "452/463, train_loss: 0.2808\n",
      "453/463, train_loss: 0.3357\n",
      "454/463, train_loss: 0.4270\n",
      "455/463, train_loss: 0.3936\n",
      "456/463, train_loss: 0.1675\n",
      "457/463, train_loss: 0.2229\n",
      "458/463, train_loss: 0.3325\n",
      "459/463, train_loss: 0.2114\n",
      "460/463, train_loss: 0.2991\n",
      "461/463, train_loss: 0.3191\n",
      "462/463, train_loss: 0.4541\n",
      "463/463, train_loss: 0.5117\n",
      "epoch 4 average loss: 0.3654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 12:41:13 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 12:41:17 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 5/100\n",
      "1/463, train_loss: 0.4321\n",
      "2/463, train_loss: 0.2334\n",
      "3/463, train_loss: 0.2996\n",
      "4/463, train_loss: 0.3081\n",
      "5/463, train_loss: 0.5483\n",
      "6/463, train_loss: 0.4019\n",
      "7/463, train_loss: 0.2676\n",
      "8/463, train_loss: 0.3647\n",
      "9/463, train_loss: 0.1227\n",
      "10/463, train_loss: 0.7339\n",
      "11/463, train_loss: 0.8413\n",
      "12/463, train_loss: 0.2749\n",
      "13/463, train_loss: 0.3887\n",
      "14/463, train_loss: 0.1802\n",
      "15/463, train_loss: 0.3320\n",
      "16/463, train_loss: 0.2896\n",
      "17/463, train_loss: 0.3325\n",
      "18/463, train_loss: 0.2214\n",
      "19/463, train_loss: 0.2310\n",
      "20/463, train_loss: 0.2920\n",
      "21/463, train_loss: 0.1735\n",
      "22/463, train_loss: 0.2507\n",
      "23/463, train_loss: 0.3918\n",
      "24/463, train_loss: 0.4368\n",
      "25/463, train_loss: 0.5898\n",
      "26/463, train_loss: 0.2847\n",
      "27/463, train_loss: 0.5181\n",
      "28/463, train_loss: 0.2607\n",
      "29/463, train_loss: 0.2042\n",
      "30/463, train_loss: 0.4907\n",
      "31/463, train_loss: 0.1567\n",
      "32/463, train_loss: 0.2249\n",
      "33/463, train_loss: 0.1333\n",
      "34/463, train_loss: 0.3721\n",
      "35/463, train_loss: 0.3557\n",
      "36/463, train_loss: 0.5156\n",
      "37/463, train_loss: 0.2004\n",
      "38/463, train_loss: 0.7393\n",
      "39/463, train_loss: 0.2856\n",
      "40/463, train_loss: 0.4949\n",
      "41/463, train_loss: 0.7700\n",
      "42/463, train_loss: 0.4927\n",
      "43/463, train_loss: 0.3765\n",
      "44/463, train_loss: 0.4097\n",
      "45/463, train_loss: 0.2272\n",
      "46/463, train_loss: 0.6328\n",
      "47/463, train_loss: 0.6694\n",
      "48/463, train_loss: 0.4121\n",
      "49/463, train_loss: 0.3083\n",
      "50/463, train_loss: 0.4368\n",
      "51/463, train_loss: 0.6509\n",
      "52/463, train_loss: 0.2349\n",
      "53/463, train_loss: 0.2852\n",
      "54/463, train_loss: 0.2009\n",
      "55/463, train_loss: 0.1606\n",
      "56/463, train_loss: 0.2815\n",
      "57/463, train_loss: 0.3560\n",
      "58/463, train_loss: 0.3501\n",
      "59/463, train_loss: 0.1976\n",
      "60/463, train_loss: 0.3608\n",
      "61/463, train_loss: 0.2489\n",
      "62/463, train_loss: 0.2842\n",
      "63/463, train_loss: 0.1005\n",
      "64/463, train_loss: 0.7358\n",
      "65/463, train_loss: 0.2329\n",
      "66/463, train_loss: 0.5801\n",
      "67/463, train_loss: 0.2358\n",
      "68/463, train_loss: 0.3394\n",
      "69/463, train_loss: 0.4260\n",
      "70/463, train_loss: 0.2705\n",
      "71/463, train_loss: 0.2334\n",
      "72/463, train_loss: 0.2639\n",
      "73/463, train_loss: 0.5020\n",
      "74/463, train_loss: 0.3457\n",
      "75/463, train_loss: 0.4370\n",
      "76/463, train_loss: 0.1670\n",
      "77/463, train_loss: 0.1416\n",
      "78/463, train_loss: 0.3962\n",
      "79/463, train_loss: 0.7090\n",
      "80/463, train_loss: 0.3037\n",
      "81/463, train_loss: 0.8232\n",
      "82/463, train_loss: 0.5322\n",
      "83/463, train_loss: 0.5537\n",
      "84/463, train_loss: 0.3486\n",
      "85/463, train_loss: 0.2478\n",
      "86/463, train_loss: 0.2158\n",
      "87/463, train_loss: 0.7100\n",
      "88/463, train_loss: 0.2052\n",
      "89/463, train_loss: 0.2634\n",
      "90/463, train_loss: 0.4363\n",
      "91/463, train_loss: 0.4668\n",
      "92/463, train_loss: 0.1978\n",
      "93/463, train_loss: 0.2773\n",
      "94/463, train_loss: 0.5073\n",
      "95/463, train_loss: 0.8809\n",
      "96/463, train_loss: 0.3184\n",
      "97/463, train_loss: 0.1967\n",
      "98/463, train_loss: 0.2001\n",
      "99/463, train_loss: 0.1580\n",
      "100/463, train_loss: 0.3206\n",
      "101/463, train_loss: 0.2205\n",
      "102/463, train_loss: 0.2559\n",
      "103/463, train_loss: 0.3301\n",
      "104/463, train_loss: 0.9375\n",
      "105/463, train_loss: 0.2045\n",
      "106/463, train_loss: 0.2571\n",
      "107/463, train_loss: 0.4009\n",
      "108/463, train_loss: 0.4434\n",
      "109/463, train_loss: 0.2317\n",
      "110/463, train_loss: 0.4595\n",
      "111/463, train_loss: 0.3025\n",
      "112/463, train_loss: 0.2070\n",
      "113/463, train_loss: 0.3091\n",
      "114/463, train_loss: 0.2644\n",
      "115/463, train_loss: 0.4351\n",
      "116/463, train_loss: 0.2720\n",
      "117/463, train_loss: 0.1643\n",
      "118/463, train_loss: 0.2671\n",
      "119/463, train_loss: 0.2537\n",
      "120/463, train_loss: 0.4771\n",
      "121/463, train_loss: 0.2178\n",
      "122/463, train_loss: 0.2817\n",
      "123/463, train_loss: 0.2751\n",
      "124/463, train_loss: 0.5708\n",
      "125/463, train_loss: 0.1665\n",
      "126/463, train_loss: 0.3193\n",
      "127/463, train_loss: 0.3467\n",
      "128/463, train_loss: 0.1562\n",
      "129/463, train_loss: 0.3003\n",
      "130/463, train_loss: 0.2837\n",
      "131/463, train_loss: 0.4163\n",
      "132/463, train_loss: 0.3540\n",
      "133/463, train_loss: 0.5332\n",
      "134/463, train_loss: 0.3906\n",
      "135/463, train_loss: 0.2703\n",
      "136/463, train_loss: 0.2324\n",
      "137/463, train_loss: 0.1277\n",
      "138/463, train_loss: 0.3652\n",
      "139/463, train_loss: 0.2017\n",
      "140/463, train_loss: 0.2759\n",
      "141/463, train_loss: 0.2874\n",
      "142/463, train_loss: 0.2603\n",
      "143/463, train_loss: 0.1064\n",
      "144/463, train_loss: 0.3303\n",
      "145/463, train_loss: 0.6553\n",
      "146/463, train_loss: 0.2401\n",
      "147/463, train_loss: 0.2910\n",
      "148/463, train_loss: 0.6191\n",
      "149/463, train_loss: 0.1549\n",
      "150/463, train_loss: 0.1587\n",
      "151/463, train_loss: 0.2878\n",
      "152/463, train_loss: 0.3335\n",
      "153/463, train_loss: 0.6899\n",
      "154/463, train_loss: 0.1785\n",
      "155/463, train_loss: 0.1783\n",
      "156/463, train_loss: 0.1462\n",
      "157/463, train_loss: 0.2622\n",
      "158/463, train_loss: 0.2581\n",
      "159/463, train_loss: 0.2045\n",
      "160/463, train_loss: 0.9189\n",
      "161/463, train_loss: 0.2803\n",
      "162/463, train_loss: 0.6499\n",
      "163/463, train_loss: 0.1958\n",
      "164/463, train_loss: 0.2380\n",
      "165/463, train_loss: 0.2634\n",
      "166/463, train_loss: 0.3137\n",
      "167/463, train_loss: 0.6714\n",
      "168/463, train_loss: 0.1224\n",
      "169/463, train_loss: 0.1748\n",
      "170/463, train_loss: 0.1830\n",
      "171/463, train_loss: 0.3499\n",
      "172/463, train_loss: 0.5425\n",
      "173/463, train_loss: 0.3662\n",
      "174/463, train_loss: 0.3521\n",
      "175/463, train_loss: 0.3613\n",
      "176/463, train_loss: 0.3240\n",
      "177/463, train_loss: 0.3589\n",
      "178/463, train_loss: 0.2651\n",
      "179/463, train_loss: 0.7002\n",
      "180/463, train_loss: 0.2090\n",
      "181/463, train_loss: 0.3706\n",
      "182/463, train_loss: 0.2477\n",
      "183/463, train_loss: 0.1541\n",
      "184/463, train_loss: 0.1743\n",
      "185/463, train_loss: 0.2184\n",
      "186/463, train_loss: 0.6523\n",
      "187/463, train_loss: 0.2847\n",
      "188/463, train_loss: 0.2317\n",
      "189/463, train_loss: 0.1687\n",
      "190/463, train_loss: 0.1741\n",
      "191/463, train_loss: 0.1799\n",
      "192/463, train_loss: 0.1270\n",
      "193/463, train_loss: 0.3755\n",
      "194/463, train_loss: 0.8652\n",
      "195/463, train_loss: 0.1390\n",
      "196/463, train_loss: 0.1805\n",
      "197/463, train_loss: 0.3240\n",
      "198/463, train_loss: 0.2654\n",
      "199/463, train_loss: 0.4006\n",
      "200/463, train_loss: 0.2286\n",
      "201/463, train_loss: 0.2834\n",
      "202/463, train_loss: 0.1154\n",
      "203/463, train_loss: 0.1355\n",
      "204/463, train_loss: 0.2175\n",
      "205/463, train_loss: 0.2937\n",
      "206/463, train_loss: 0.5918\n",
      "207/463, train_loss: 0.0997\n",
      "208/463, train_loss: 0.1984\n",
      "209/463, train_loss: 0.4856\n",
      "210/463, train_loss: 0.2375\n",
      "211/463, train_loss: 0.2271\n",
      "212/463, train_loss: 0.2324\n",
      "213/463, train_loss: 0.1827\n",
      "214/463, train_loss: 0.2603\n",
      "215/463, train_loss: 0.2949\n",
      "216/463, train_loss: 0.2898\n",
      "217/463, train_loss: 0.1317\n",
      "218/463, train_loss: 0.3882\n",
      "219/463, train_loss: 0.1311\n",
      "220/463, train_loss: 0.3145\n",
      "221/463, train_loss: 0.1858\n",
      "222/463, train_loss: 0.0858\n",
      "223/463, train_loss: 0.3870\n",
      "224/463, train_loss: 0.3235\n",
      "225/463, train_loss: 0.2993\n",
      "226/463, train_loss: 0.2954\n",
      "227/463, train_loss: 0.1591\n",
      "228/463, train_loss: 0.2771\n",
      "229/463, train_loss: 0.1951\n",
      "230/463, train_loss: 0.1914\n",
      "231/463, train_loss: 0.1606\n",
      "232/463, train_loss: 0.3518\n",
      "233/463, train_loss: 0.3218\n",
      "234/463, train_loss: 0.5703\n",
      "235/463, train_loss: 0.1583\n",
      "236/463, train_loss: 0.9575\n",
      "237/463, train_loss: 0.9614\n",
      "238/463, train_loss: 0.2903\n",
      "239/463, train_loss: 0.3955\n",
      "240/463, train_loss: 0.2808\n",
      "241/463, train_loss: 0.4280\n",
      "242/463, train_loss: 0.3154\n",
      "243/463, train_loss: 0.2241\n",
      "244/463, train_loss: 0.5435\n",
      "245/463, train_loss: 0.3123\n",
      "246/463, train_loss: 0.3333\n",
      "247/463, train_loss: 0.4878\n",
      "248/463, train_loss: 0.5972\n",
      "249/463, train_loss: 0.3774\n",
      "250/463, train_loss: 0.3079\n",
      "251/463, train_loss: 0.2446\n",
      "252/463, train_loss: 0.2402\n",
      "253/463, train_loss: 0.1170\n",
      "254/463, train_loss: 0.4341\n",
      "255/463, train_loss: 0.4653\n",
      "256/463, train_loss: 0.2876\n",
      "257/463, train_loss: 0.6055\n",
      "258/463, train_loss: 0.3850\n",
      "259/463, train_loss: 0.1794\n",
      "260/463, train_loss: 0.2805\n",
      "261/463, train_loss: 0.2493\n",
      "262/463, train_loss: 0.2678\n",
      "263/463, train_loss: 0.3875\n",
      "264/463, train_loss: 0.1169\n",
      "265/463, train_loss: 0.1344\n",
      "266/463, train_loss: 0.4541\n",
      "267/463, train_loss: 0.1865\n",
      "268/463, train_loss: 0.2003\n",
      "269/463, train_loss: 0.2502\n",
      "270/463, train_loss: 0.1246\n",
      "271/463, train_loss: 0.1794\n",
      "272/463, train_loss: 0.4189\n",
      "273/463, train_loss: 0.1919\n",
      "274/463, train_loss: 0.1660\n",
      "275/463, train_loss: 0.5093\n",
      "276/463, train_loss: 0.3120\n",
      "277/463, train_loss: 0.3657\n",
      "278/463, train_loss: 0.2856\n",
      "279/463, train_loss: 0.3103\n",
      "280/463, train_loss: 0.3069\n",
      "281/463, train_loss: 0.2002\n",
      "282/463, train_loss: 0.3198\n",
      "283/463, train_loss: 0.1095\n",
      "284/463, train_loss: 0.3464\n",
      "285/463, train_loss: 0.4468\n",
      "286/463, train_loss: 0.1854\n",
      "287/463, train_loss: 0.5742\n",
      "288/463, train_loss: 0.2310\n",
      "289/463, train_loss: 0.1453\n",
      "290/463, train_loss: 0.1449\n",
      "291/463, train_loss: 0.2727\n",
      "292/463, train_loss: 0.3000\n",
      "293/463, train_loss: 0.1631\n",
      "294/463, train_loss: 0.1178\n",
      "295/463, train_loss: 0.4058\n",
      "296/463, train_loss: 0.4382\n",
      "297/463, train_loss: 0.4517\n",
      "298/463, train_loss: 0.2510\n",
      "299/463, train_loss: 0.5381\n",
      "300/463, train_loss: 0.8057\n",
      "301/463, train_loss: 0.4917\n",
      "302/463, train_loss: 0.8535\n",
      "303/463, train_loss: 0.2715\n",
      "304/463, train_loss: 0.4709\n",
      "305/463, train_loss: 0.3284\n",
      "306/463, train_loss: 0.2800\n",
      "307/463, train_loss: 0.2413\n",
      "308/463, train_loss: 0.4790\n",
      "309/463, train_loss: 0.4678\n",
      "310/463, train_loss: 0.2747\n",
      "311/463, train_loss: 0.1438\n",
      "312/463, train_loss: 0.3169\n",
      "313/463, train_loss: 0.2795\n",
      "314/463, train_loss: 0.5918\n",
      "315/463, train_loss: 0.3589\n",
      "316/463, train_loss: 0.8086\n",
      "317/463, train_loss: 0.4678\n",
      "318/463, train_loss: 0.2185\n",
      "319/463, train_loss: 0.1921\n",
      "320/463, train_loss: 0.1881\n",
      "321/463, train_loss: 0.2340\n",
      "322/463, train_loss: 0.1659\n",
      "323/463, train_loss: 0.3601\n",
      "324/463, train_loss: 0.4829\n",
      "325/463, train_loss: 0.4253\n",
      "326/463, train_loss: 0.3848\n",
      "327/463, train_loss: 0.2612\n",
      "328/463, train_loss: 0.3469\n",
      "329/463, train_loss: 0.2242\n",
      "330/463, train_loss: 0.1693\n",
      "331/463, train_loss: 0.1458\n",
      "332/463, train_loss: 0.2104\n",
      "333/463, train_loss: 0.1450\n",
      "334/463, train_loss: 0.1327\n",
      "335/463, train_loss: 0.2517\n",
      "336/463, train_loss: 0.3481\n",
      "337/463, train_loss: 0.4797\n",
      "338/463, train_loss: 0.4209\n",
      "339/463, train_loss: 0.2285\n",
      "340/463, train_loss: 0.4917\n",
      "341/463, train_loss: 0.3589\n",
      "342/463, train_loss: 0.1311\n",
      "343/463, train_loss: 0.4775\n",
      "344/463, train_loss: 0.1604\n",
      "345/463, train_loss: 0.5513\n",
      "346/463, train_loss: 0.1481\n",
      "347/463, train_loss: 0.1912\n",
      "348/463, train_loss: 0.6270\n",
      "349/463, train_loss: 0.4963\n",
      "350/463, train_loss: 0.3701\n",
      "351/463, train_loss: 0.6895\n",
      "352/463, train_loss: 0.1865\n",
      "353/463, train_loss: 0.2551\n",
      "354/463, train_loss: 0.4019\n",
      "355/463, train_loss: 0.2056\n",
      "356/463, train_loss: 0.2583\n",
      "357/463, train_loss: 0.8276\n",
      "358/463, train_loss: 0.1726\n",
      "359/463, train_loss: 0.2888\n",
      "360/463, train_loss: 0.5283\n",
      "361/463, train_loss: 0.3845\n",
      "362/463, train_loss: 0.1909\n",
      "363/463, train_loss: 0.2529\n",
      "364/463, train_loss: 0.1511\n",
      "365/463, train_loss: 0.4136\n",
      "366/463, train_loss: 0.2695\n",
      "367/463, train_loss: 0.2052\n",
      "368/463, train_loss: 0.1963\n",
      "369/463, train_loss: 0.5635\n",
      "370/463, train_loss: 0.4050\n",
      "371/463, train_loss: 0.5327\n",
      "372/463, train_loss: 0.3252\n",
      "373/463, train_loss: 0.0936\n",
      "374/463, train_loss: 0.2390\n",
      "375/463, train_loss: 0.4346\n",
      "376/463, train_loss: 0.4072\n",
      "377/463, train_loss: 0.1388\n",
      "378/463, train_loss: 0.3235\n",
      "379/463, train_loss: 0.2198\n",
      "380/463, train_loss: 0.4092\n",
      "381/463, train_loss: 0.8149\n",
      "382/463, train_loss: 0.3057\n",
      "383/463, train_loss: 0.3789\n",
      "384/463, train_loss: 0.1882\n",
      "385/463, train_loss: 0.4944\n",
      "386/463, train_loss: 0.3191\n",
      "387/463, train_loss: 0.2544\n",
      "388/463, train_loss: 0.5811\n",
      "389/463, train_loss: 0.7671\n",
      "390/463, train_loss: 0.2700\n",
      "391/463, train_loss: 0.2803\n",
      "392/463, train_loss: 0.2632\n",
      "393/463, train_loss: 0.2156\n",
      "394/463, train_loss: 0.3088\n",
      "395/463, train_loss: 0.1865\n",
      "396/463, train_loss: 0.4185\n",
      "397/463, train_loss: 0.3726\n",
      "398/463, train_loss: 0.3125\n",
      "399/463, train_loss: 0.4490\n",
      "400/463, train_loss: 0.2197\n",
      "401/463, train_loss: 0.2290\n",
      "402/463, train_loss: 0.2114\n",
      "403/463, train_loss: 0.3162\n",
      "404/463, train_loss: 0.1735\n",
      "405/463, train_loss: 0.1365\n",
      "406/463, train_loss: 0.2214\n",
      "407/463, train_loss: 0.1340\n",
      "408/463, train_loss: 0.3984\n",
      "409/463, train_loss: 0.5298\n",
      "410/463, train_loss: 0.1196\n",
      "411/463, train_loss: 0.2046\n",
      "412/463, train_loss: 0.2029\n",
      "413/463, train_loss: 0.2959\n",
      "414/463, train_loss: 0.1343\n",
      "415/463, train_loss: 0.1729\n",
      "416/463, train_loss: 0.1550\n",
      "417/463, train_loss: 0.4775\n",
      "418/463, train_loss: 0.3262\n",
      "419/463, train_loss: 0.2349\n",
      "420/463, train_loss: 0.1874\n",
      "421/463, train_loss: 0.5156\n",
      "422/463, train_loss: 0.1752\n",
      "423/463, train_loss: 0.2896\n",
      "424/463, train_loss: 0.1733\n",
      "425/463, train_loss: 0.1172\n",
      "426/463, train_loss: 0.1692\n",
      "427/463, train_loss: 0.8013\n",
      "428/463, train_loss: 0.4666\n",
      "429/463, train_loss: 0.3647\n",
      "430/463, train_loss: 0.2197\n",
      "431/463, train_loss: 0.1665\n",
      "432/463, train_loss: 0.5981\n",
      "433/463, train_loss: 0.2262\n",
      "434/463, train_loss: 0.0949\n",
      "435/463, train_loss: 0.1465\n",
      "436/463, train_loss: 0.3982\n",
      "437/463, train_loss: 0.2405\n",
      "438/463, train_loss: 0.1682\n",
      "439/463, train_loss: 0.3950\n",
      "440/463, train_loss: 0.3760\n",
      "441/463, train_loss: 0.4863\n",
      "442/463, train_loss: 0.2040\n",
      "443/463, train_loss: 0.3687\n",
      "444/463, train_loss: 0.3562\n",
      "445/463, train_loss: 0.2307\n",
      "446/463, train_loss: 0.6636\n",
      "447/463, train_loss: 0.4399\n",
      "448/463, train_loss: 0.2217\n",
      "449/463, train_loss: 0.2238\n",
      "450/463, train_loss: 0.3501\n",
      "451/463, train_loss: 0.4119\n",
      "452/463, train_loss: 0.1382\n",
      "453/463, train_loss: 0.1840\n",
      "454/463, train_loss: 0.1310\n",
      "455/463, train_loss: 0.2466\n",
      "456/463, train_loss: 0.3977\n",
      "457/463, train_loss: 0.1467\n",
      "458/463, train_loss: 0.1710\n",
      "459/463, train_loss: 0.1318\n",
      "460/463, train_loss: 0.2717\n",
      "461/463, train_loss: 0.5181\n",
      "462/463, train_loss: 0.4233\n",
      "463/463, train_loss: 0.5044\n",
      "epoch 5 average loss: 0.3294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 15:15:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 15:15:19 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mixed/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630.nii.gz\n",
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.42260990047989244, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.42260990047989244, 'AP_IoU_0.10_MaxDet_100': 0.4590834851681006, 'nodule_AP_IoU_0.10_MaxDet_100': 0.4590834851681006, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8319088419278463, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8319088419278463, 'AR_IoU_0.10_MaxDet_100': 0.9230769276618958, 'nodule_AR_IoU_0.10_MaxDet_100': 0.9230769276618958}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 15:35:01 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 15:35:05 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 5 current metric: 0.6592 best metric: 0.6592 at epoch 5\n",
      "----------\n",
      "epoch 6/100\n",
      "1/463, train_loss: 0.2617\n",
      "2/463, train_loss: 0.2764\n",
      "3/463, train_loss: 0.1809\n",
      "4/463, train_loss: 0.4146\n",
      "5/463, train_loss: 0.1318\n",
      "6/463, train_loss: 0.1697\n",
      "7/463, train_loss: 0.1082\n",
      "8/463, train_loss: 0.6504\n",
      "9/463, train_loss: 0.1910\n",
      "10/463, train_loss: 0.2358\n",
      "11/463, train_loss: 0.3750\n",
      "12/463, train_loss: 0.2354\n",
      "13/463, train_loss: 0.2471\n",
      "14/463, train_loss: 0.2485\n",
      "15/463, train_loss: 0.6294\n",
      "16/463, train_loss: 0.2227\n",
      "17/463, train_loss: 0.1980\n",
      "18/463, train_loss: 0.1166\n",
      "19/463, train_loss: 0.3577\n",
      "20/463, train_loss: 0.2189\n",
      "21/463, train_loss: 0.7593\n",
      "22/463, train_loss: 0.1434\n",
      "23/463, train_loss: 0.2935\n",
      "24/463, train_loss: 0.7910\n",
      "25/463, train_loss: 0.2280\n",
      "26/463, train_loss: 0.4465\n",
      "27/463, train_loss: 0.2419\n",
      "28/463, train_loss: 0.2856\n",
      "29/463, train_loss: 0.2356\n",
      "30/463, train_loss: 0.5688\n",
      "31/463, train_loss: 0.2472\n",
      "32/463, train_loss: 0.2157\n",
      "33/463, train_loss: 0.3325\n",
      "34/463, train_loss: 0.5239\n",
      "35/463, train_loss: 0.3032\n",
      "36/463, train_loss: 0.5356\n",
      "37/463, train_loss: 0.3018\n",
      "38/463, train_loss: 0.2288\n",
      "39/463, train_loss: 0.2749\n",
      "40/463, train_loss: 0.3362\n",
      "41/463, train_loss: 0.2217\n",
      "42/463, train_loss: 0.6572\n",
      "43/463, train_loss: 0.1633\n",
      "44/463, train_loss: 0.1865\n",
      "45/463, train_loss: 0.1851\n",
      "46/463, train_loss: 0.4160\n",
      "47/463, train_loss: 0.3486\n",
      "48/463, train_loss: 0.1636\n",
      "49/463, train_loss: 0.1987\n",
      "50/463, train_loss: 0.4622\n",
      "51/463, train_loss: 0.3423\n",
      "52/463, train_loss: 0.8979\n",
      "53/463, train_loss: 0.2710\n",
      "54/463, train_loss: 0.4561\n",
      "55/463, train_loss: 0.3396\n",
      "56/463, train_loss: 0.2554\n",
      "57/463, train_loss: 0.8525\n",
      "58/463, train_loss: 0.3716\n",
      "59/463, train_loss: 0.3000\n",
      "60/463, train_loss: 0.1809\n",
      "61/463, train_loss: 0.4380\n",
      "62/463, train_loss: 0.2827\n",
      "63/463, train_loss: 0.1525\n",
      "64/463, train_loss: 0.5190\n",
      "65/463, train_loss: 0.2258\n",
      "66/463, train_loss: 0.5898\n",
      "67/463, train_loss: 0.7461\n",
      "68/463, train_loss: 0.3452\n",
      "69/463, train_loss: 0.2334\n",
      "70/463, train_loss: 0.9971\n",
      "71/463, train_loss: 0.1503\n",
      "72/463, train_loss: 0.3936\n",
      "73/463, train_loss: 0.4788\n",
      "74/463, train_loss: 0.2142\n",
      "75/463, train_loss: 0.2178\n",
      "76/463, train_loss: 0.1404\n",
      "77/463, train_loss: 0.3096\n",
      "78/463, train_loss: 0.2490\n",
      "79/463, train_loss: 0.6006\n",
      "80/463, train_loss: 0.1906\n",
      "81/463, train_loss: 0.2930\n",
      "82/463, train_loss: 0.1239\n",
      "83/463, train_loss: 0.3794\n",
      "84/463, train_loss: 0.3711\n",
      "85/463, train_loss: 0.2720\n",
      "86/463, train_loss: 0.2031\n",
      "87/463, train_loss: 0.1600\n",
      "88/463, train_loss: 0.1479\n",
      "89/463, train_loss: 0.2725\n",
      "90/463, train_loss: 0.1553\n",
      "91/463, train_loss: 0.4023\n",
      "92/463, train_loss: 0.1743\n",
      "93/463, train_loss: 0.4634\n",
      "94/463, train_loss: 0.4497\n",
      "95/463, train_loss: 0.3101\n",
      "96/463, train_loss: 0.2681\n",
      "97/463, train_loss: 0.4546\n",
      "98/463, train_loss: 0.2394\n",
      "99/463, train_loss: 0.2871\n",
      "100/463, train_loss: 0.3840\n",
      "101/463, train_loss: 0.1465\n",
      "102/463, train_loss: 0.3796\n",
      "103/463, train_loss: 0.4829\n",
      "104/463, train_loss: 0.1160\n",
      "105/463, train_loss: 0.7036\n",
      "106/463, train_loss: 0.1989\n",
      "107/463, train_loss: 0.1932\n",
      "108/463, train_loss: 0.4067\n",
      "109/463, train_loss: 0.6592\n",
      "110/463, train_loss: 0.2096\n",
      "111/463, train_loss: 0.1307\n",
      "112/463, train_loss: 0.3347\n",
      "113/463, train_loss: 0.2500\n",
      "114/463, train_loss: 0.3833\n",
      "115/463, train_loss: 0.4836\n",
      "116/463, train_loss: 0.5454\n",
      "117/463, train_loss: 0.1340\n",
      "118/463, train_loss: 0.2617\n",
      "119/463, train_loss: 0.4929\n",
      "120/463, train_loss: 0.3794\n",
      "121/463, train_loss: 0.7246\n",
      "122/463, train_loss: 0.2903\n",
      "123/463, train_loss: 0.2119\n",
      "124/463, train_loss: 0.2271\n",
      "125/463, train_loss: 0.2537\n",
      "126/463, train_loss: 0.2455\n",
      "127/463, train_loss: 0.3213\n",
      "128/463, train_loss: 0.2048\n",
      "129/463, train_loss: 0.2686\n",
      "130/463, train_loss: 0.7231\n",
      "131/463, train_loss: 0.1343\n",
      "132/463, train_loss: 0.2603\n",
      "133/463, train_loss: 0.4209\n",
      "134/463, train_loss: 0.1780\n",
      "135/463, train_loss: 0.3833\n",
      "136/463, train_loss: 0.5029\n",
      "137/463, train_loss: 0.2122\n",
      "138/463, train_loss: 0.2920\n",
      "139/463, train_loss: 0.1816\n",
      "140/463, train_loss: 0.1665\n",
      "141/463, train_loss: 0.5322\n",
      "142/463, train_loss: 0.3496\n",
      "143/463, train_loss: 0.3691\n",
      "144/463, train_loss: 0.1835\n",
      "145/463, train_loss: 0.3594\n",
      "146/463, train_loss: 0.4932\n",
      "147/463, train_loss: 0.1877\n",
      "148/463, train_loss: 0.2031\n",
      "149/463, train_loss: 0.3313\n",
      "150/463, train_loss: 0.1937\n",
      "151/463, train_loss: 0.2491\n",
      "152/463, train_loss: 0.1300\n",
      "153/463, train_loss: 0.2473\n",
      "154/463, train_loss: 0.4873\n",
      "155/463, train_loss: 0.2327\n",
      "156/463, train_loss: 0.2441\n",
      "157/463, train_loss: 0.1381\n",
      "158/463, train_loss: 0.2900\n",
      "159/463, train_loss: 0.1902\n",
      "160/463, train_loss: 0.6196\n",
      "161/463, train_loss: 0.1384\n",
      "162/463, train_loss: 0.1174\n",
      "163/463, train_loss: 0.1567\n",
      "164/463, train_loss: 0.1671\n",
      "165/463, train_loss: 0.1594\n",
      "166/463, train_loss: 0.4788\n",
      "167/463, train_loss: 0.1096\n",
      "168/463, train_loss: 0.2408\n",
      "169/463, train_loss: 0.0886\n",
      "170/463, train_loss: 0.2251\n",
      "171/463, train_loss: 0.2139\n",
      "172/463, train_loss: 0.7275\n",
      "173/463, train_loss: 0.1824\n",
      "174/463, train_loss: 1.0039\n",
      "175/463, train_loss: 0.7920\n",
      "176/463, train_loss: 0.1061\n",
      "177/463, train_loss: 0.1975\n",
      "178/463, train_loss: 0.2053\n",
      "179/463, train_loss: 0.2725\n",
      "180/463, train_loss: 0.1410\n",
      "181/463, train_loss: 0.2061\n",
      "182/463, train_loss: 0.3750\n",
      "183/463, train_loss: 0.2854\n",
      "184/463, train_loss: 0.7490\n",
      "185/463, train_loss: 0.1527\n",
      "186/463, train_loss: 0.1317\n",
      "187/463, train_loss: 0.5620\n",
      "188/463, train_loss: 0.1779\n",
      "189/463, train_loss: 0.1910\n",
      "190/463, train_loss: 0.5049\n",
      "191/463, train_loss: 0.2097\n",
      "192/463, train_loss: 0.7148\n",
      "193/463, train_loss: 0.3271\n",
      "194/463, train_loss: 0.3521\n",
      "195/463, train_loss: 0.1777\n",
      "196/463, train_loss: 0.1864\n",
      "197/463, train_loss: 0.1438\n",
      "198/463, train_loss: 0.4189\n",
      "199/463, train_loss: 0.5610\n",
      "200/463, train_loss: 0.3628\n",
      "201/463, train_loss: 0.1175\n",
      "202/463, train_loss: 0.1247\n",
      "203/463, train_loss: 0.1558\n",
      "204/463, train_loss: 0.1647\n",
      "205/463, train_loss: 0.1995\n",
      "206/463, train_loss: 0.2307\n",
      "207/463, train_loss: 0.2229\n",
      "208/463, train_loss: 0.1827\n",
      "209/463, train_loss: 0.5117\n",
      "210/463, train_loss: 0.2368\n",
      "211/463, train_loss: 0.3508\n",
      "212/463, train_loss: 0.1206\n",
      "213/463, train_loss: 0.1504\n",
      "214/463, train_loss: 0.1381\n",
      "215/463, train_loss: 0.1346\n",
      "216/463, train_loss: 0.2764\n",
      "217/463, train_loss: 0.3523\n",
      "218/463, train_loss: 0.1396\n",
      "219/463, train_loss: 0.1533\n",
      "220/463, train_loss: 0.2810\n",
      "221/463, train_loss: 0.1538\n",
      "222/463, train_loss: 0.6255\n",
      "223/463, train_loss: 0.4556\n",
      "224/463, train_loss: 0.1962\n",
      "225/463, train_loss: 0.5459\n",
      "226/463, train_loss: 0.2971\n",
      "227/463, train_loss: 0.2373\n",
      "228/463, train_loss: 0.1796\n",
      "229/463, train_loss: 0.4253\n",
      "230/463, train_loss: 0.3481\n",
      "231/463, train_loss: 0.1587\n",
      "232/463, train_loss: 0.4929\n",
      "233/463, train_loss: 0.4219\n",
      "234/463, train_loss: 0.9785\n",
      "235/463, train_loss: 0.5117\n",
      "236/463, train_loss: 0.2458\n",
      "237/463, train_loss: 0.3813\n",
      "238/463, train_loss: 0.4292\n",
      "239/463, train_loss: 0.1631\n",
      "240/463, train_loss: 0.5537\n",
      "241/463, train_loss: 0.3452\n",
      "242/463, train_loss: 0.3008\n",
      "243/463, train_loss: 0.1865\n",
      "244/463, train_loss: 0.2286\n",
      "245/463, train_loss: 0.6104\n",
      "246/463, train_loss: 0.2146\n",
      "247/463, train_loss: 0.3743\n",
      "248/463, train_loss: 0.2207\n",
      "249/463, train_loss: 0.2177\n",
      "250/463, train_loss: 0.2131\n",
      "251/463, train_loss: 0.3667\n",
      "252/463, train_loss: 0.4954\n",
      "253/463, train_loss: 0.3638\n",
      "254/463, train_loss: 0.3169\n",
      "255/463, train_loss: 0.3335\n",
      "256/463, train_loss: 0.1741\n",
      "257/463, train_loss: 0.0950\n",
      "258/463, train_loss: 0.2715\n",
      "259/463, train_loss: 0.1788\n",
      "260/463, train_loss: 0.2888\n",
      "261/463, train_loss: 0.1998\n",
      "262/463, train_loss: 0.3606\n",
      "263/463, train_loss: 0.1906\n",
      "264/463, train_loss: 0.9238\n",
      "265/463, train_loss: 0.2180\n",
      "266/463, train_loss: 0.1925\n",
      "267/463, train_loss: 0.4136\n",
      "268/463, train_loss: 0.3237\n",
      "269/463, train_loss: 0.4172\n",
      "270/463, train_loss: 0.4431\n",
      "271/463, train_loss: 0.2148\n",
      "272/463, train_loss: 0.1338\n",
      "273/463, train_loss: 0.4241\n",
      "274/463, train_loss: 0.3137\n",
      "275/463, train_loss: 0.3647\n",
      "276/463, train_loss: 0.1221\n",
      "277/463, train_loss: 0.3213\n",
      "278/463, train_loss: 0.2610\n",
      "279/463, train_loss: 0.1687\n",
      "280/463, train_loss: 0.1292\n",
      "281/463, train_loss: 0.4370\n",
      "282/463, train_loss: 0.1569\n",
      "283/463, train_loss: 0.2617\n",
      "284/463, train_loss: 0.3765\n",
      "285/463, train_loss: 0.2566\n",
      "286/463, train_loss: 0.3896\n",
      "287/463, train_loss: 0.3608\n",
      "288/463, train_loss: 0.2137\n",
      "289/463, train_loss: 0.8975\n",
      "290/463, train_loss: 0.1842\n",
      "291/463, train_loss: 0.1982\n",
      "292/463, train_loss: 0.2625\n",
      "293/463, train_loss: 0.2484\n",
      "294/463, train_loss: 0.4131\n",
      "295/463, train_loss: 0.3020\n",
      "296/463, train_loss: 0.2200\n",
      "297/463, train_loss: 0.4990\n",
      "298/463, train_loss: 0.1460\n",
      "299/463, train_loss: 0.2942\n",
      "300/463, train_loss: 0.6396\n",
      "301/463, train_loss: 0.2583\n",
      "302/463, train_loss: 0.1575\n",
      "303/463, train_loss: 0.1479\n",
      "304/463, train_loss: 0.1650\n",
      "305/463, train_loss: 0.6177\n",
      "306/463, train_loss: 0.6138\n",
      "307/463, train_loss: 0.3848\n",
      "308/463, train_loss: 0.1893\n",
      "309/463, train_loss: 0.7773\n",
      "310/463, train_loss: 0.2006\n",
      "311/463, train_loss: 0.1189\n",
      "312/463, train_loss: 0.2103\n",
      "313/463, train_loss: 0.4485\n",
      "314/463, train_loss: 0.7085\n",
      "315/463, train_loss: 0.1705\n",
      "316/463, train_loss: 0.2942\n",
      "317/463, train_loss: 0.2607\n",
      "318/463, train_loss: 0.2330\n",
      "319/463, train_loss: 0.1667\n",
      "320/463, train_loss: 0.1436\n",
      "321/463, train_loss: 0.1538\n",
      "322/463, train_loss: 0.2842\n",
      "323/463, train_loss: 0.7578\n",
      "324/463, train_loss: 0.1537\n",
      "325/463, train_loss: 0.2676\n",
      "326/463, train_loss: 0.4294\n",
      "327/463, train_loss: 0.2559\n",
      "328/463, train_loss: 0.1733\n",
      "329/463, train_loss: 0.1748\n",
      "330/463, train_loss: 0.2532\n",
      "331/463, train_loss: 0.4761\n",
      "332/463, train_loss: 0.4255\n",
      "333/463, train_loss: 0.1421\n",
      "334/463, train_loss: 0.3054\n",
      "335/463, train_loss: 0.2406\n",
      "336/463, train_loss: 0.6548\n",
      "337/463, train_loss: 0.1470\n",
      "338/463, train_loss: 0.2544\n",
      "339/463, train_loss: 0.2036\n",
      "340/463, train_loss: 0.5645\n",
      "341/463, train_loss: 0.2194\n",
      "342/463, train_loss: 0.3474\n",
      "343/463, train_loss: 0.1958\n",
      "344/463, train_loss: 0.0365\n",
      "345/463, train_loss: 0.3989\n",
      "346/463, train_loss: 0.4780\n",
      "347/463, train_loss: 0.3674\n",
      "348/463, train_loss: 0.2825\n",
      "349/463, train_loss: 0.4458\n",
      "350/463, train_loss: 0.3940\n",
      "351/463, train_loss: 0.1731\n",
      "352/463, train_loss: 0.7285\n",
      "353/463, train_loss: 0.2006\n",
      "354/463, train_loss: 0.2478\n",
      "355/463, train_loss: 0.3298\n",
      "356/463, train_loss: 0.3032\n",
      "357/463, train_loss: 0.3845\n",
      "358/463, train_loss: 0.2869\n",
      "359/463, train_loss: 0.2742\n",
      "360/463, train_loss: 0.1890\n",
      "361/463, train_loss: 0.6006\n",
      "362/463, train_loss: 0.0861\n",
      "363/463, train_loss: 0.0850\n",
      "364/463, train_loss: 0.2854\n",
      "365/463, train_loss: 0.1210\n",
      "366/463, train_loss: 0.1750\n",
      "367/463, train_loss: 0.2783\n",
      "368/463, train_loss: 0.3032\n",
      "369/463, train_loss: 0.3218\n",
      "370/463, train_loss: 0.2705\n",
      "371/463, train_loss: 0.3110\n",
      "372/463, train_loss: 0.4277\n",
      "373/463, train_loss: 0.1927\n",
      "374/463, train_loss: 0.2386\n",
      "375/463, train_loss: 0.3684\n",
      "376/463, train_loss: 0.2520\n",
      "377/463, train_loss: 0.2847\n",
      "378/463, train_loss: 1.2979\n",
      "379/463, train_loss: 0.1562\n",
      "380/463, train_loss: 0.2383\n",
      "381/463, train_loss: 0.2068\n",
      "382/463, train_loss: 0.4861\n",
      "383/463, train_loss: 0.1315\n",
      "384/463, train_loss: 0.3579\n",
      "385/463, train_loss: 0.3733\n",
      "386/463, train_loss: 0.1805\n",
      "387/463, train_loss: 0.9180\n",
      "388/463, train_loss: 0.1274\n",
      "389/463, train_loss: 0.2239\n",
      "390/463, train_loss: 0.3042\n",
      "391/463, train_loss: 0.4917\n",
      "392/463, train_loss: 0.5298\n",
      "393/463, train_loss: 0.5283\n",
      "394/463, train_loss: 0.4175\n",
      "395/463, train_loss: 0.2856\n",
      "396/463, train_loss: 0.2593\n",
      "397/463, train_loss: 0.4922\n",
      "398/463, train_loss: 0.2045\n",
      "399/463, train_loss: 0.1353\n",
      "400/463, train_loss: 0.2769\n",
      "401/463, train_loss: 0.1633\n",
      "402/463, train_loss: 0.2764\n",
      "403/463, train_loss: 0.4014\n",
      "404/463, train_loss: 0.4995\n",
      "405/463, train_loss: 0.1532\n",
      "406/463, train_loss: 0.3772\n",
      "407/463, train_loss: 0.2729\n",
      "408/463, train_loss: 0.2308\n",
      "409/463, train_loss: 0.2410\n",
      "410/463, train_loss: 0.1417\n",
      "411/463, train_loss: 0.2292\n",
      "412/463, train_loss: 0.3428\n",
      "413/463, train_loss: 0.1698\n",
      "414/463, train_loss: 0.4739\n",
      "415/463, train_loss: 0.1350\n",
      "416/463, train_loss: 0.2446\n",
      "417/463, train_loss: 0.3491\n",
      "418/463, train_loss: 0.2742\n",
      "419/463, train_loss: 0.2983\n",
      "420/463, train_loss: 0.2421\n",
      "421/463, train_loss: 0.1707\n",
      "422/463, train_loss: 0.2603\n",
      "423/463, train_loss: 0.3540\n",
      "424/463, train_loss: 0.1566\n",
      "425/463, train_loss: 0.2844\n",
      "426/463, train_loss: 0.2240\n",
      "427/463, train_loss: 0.2615\n",
      "428/463, train_loss: 0.1558\n",
      "429/463, train_loss: 0.2037\n",
      "430/463, train_loss: 0.1495\n",
      "431/463, train_loss: 0.1450\n",
      "432/463, train_loss: 0.2612\n",
      "433/463, train_loss: 0.2542\n",
      "434/463, train_loss: 0.5728\n",
      "435/463, train_loss: 0.2786\n",
      "436/463, train_loss: 0.1949\n",
      "437/463, train_loss: 0.1138\n",
      "438/463, train_loss: 0.2659\n",
      "439/463, train_loss: 0.1642\n",
      "440/463, train_loss: 0.3303\n",
      "441/463, train_loss: 0.2153\n",
      "442/463, train_loss: 0.2017\n",
      "443/463, train_loss: 0.1462\n",
      "444/463, train_loss: 0.6323\n",
      "445/463, train_loss: 0.6006\n",
      "446/463, train_loss: 0.1992\n",
      "447/463, train_loss: 0.5430\n",
      "448/463, train_loss: 0.1617\n",
      "449/463, train_loss: 0.6113\n",
      "450/463, train_loss: 0.1904\n",
      "451/463, train_loss: 0.1599\n",
      "452/463, train_loss: 0.2544\n",
      "453/463, train_loss: 0.4458\n",
      "454/463, train_loss: 0.2405\n",
      "455/463, train_loss: 0.1622\n",
      "456/463, train_loss: 0.1814\n",
      "457/463, train_loss: 0.1204\n",
      "458/463, train_loss: 0.2202\n",
      "459/463, train_loss: 0.8496\n",
      "460/463, train_loss: 0.2937\n",
      "461/463, train_loss: 0.1953\n",
      "462/463, train_loss: 0.1755\n",
      "463/463, train_loss: 0.2189\n",
      "epoch 6 average loss: 0.3134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 18:16:58 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 18:17:02 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 7/100\n",
      "1/463, train_loss: 0.2593\n",
      "2/463, train_loss: 0.4163\n",
      "3/463, train_loss: 0.7388\n",
      "4/463, train_loss: 0.3716\n",
      "5/463, train_loss: 0.1744\n",
      "6/463, train_loss: 0.5435\n",
      "7/463, train_loss: 0.6245\n",
      "8/463, train_loss: 0.1903\n",
      "9/463, train_loss: 0.2142\n",
      "10/463, train_loss: 0.1908\n",
      "11/463, train_loss: 0.2554\n",
      "12/463, train_loss: 0.2233\n",
      "13/463, train_loss: 0.5679\n",
      "14/463, train_loss: 0.1456\n",
      "15/463, train_loss: 0.3750\n",
      "16/463, train_loss: 0.6099\n",
      "17/463, train_loss: 0.5825\n",
      "18/463, train_loss: 0.1598\n",
      "19/463, train_loss: 0.1578\n",
      "20/463, train_loss: 0.4365\n",
      "21/463, train_loss: 0.4399\n",
      "22/463, train_loss: 0.2407\n",
      "23/463, train_loss: 0.3030\n",
      "24/463, train_loss: 0.7012\n",
      "25/463, train_loss: 0.4370\n",
      "26/463, train_loss: 0.2025\n",
      "27/463, train_loss: 0.4539\n",
      "28/463, train_loss: 0.1472\n",
      "29/463, train_loss: 0.2930\n",
      "30/463, train_loss: 0.3269\n",
      "31/463, train_loss: 0.2302\n",
      "32/463, train_loss: 0.3457\n",
      "33/463, train_loss: 0.2054\n",
      "34/463, train_loss: 0.3618\n",
      "35/463, train_loss: 0.1207\n",
      "36/463, train_loss: 0.2046\n",
      "37/463, train_loss: 0.1184\n",
      "38/463, train_loss: 0.5625\n",
      "39/463, train_loss: 0.2032\n",
      "40/463, train_loss: 0.5332\n",
      "41/463, train_loss: 0.2148\n",
      "42/463, train_loss: 0.2153\n",
      "43/463, train_loss: 0.2334\n",
      "44/463, train_loss: 0.3389\n",
      "45/463, train_loss: 0.1075\n",
      "46/463, train_loss: 0.2769\n",
      "47/463, train_loss: 0.9590\n",
      "48/463, train_loss: 0.1814\n",
      "49/463, train_loss: 0.0942\n",
      "50/463, train_loss: 0.2893\n",
      "51/463, train_loss: 0.2052\n",
      "52/463, train_loss: 0.1918\n",
      "53/463, train_loss: 0.2646\n",
      "54/463, train_loss: 0.2632\n",
      "55/463, train_loss: 0.8086\n",
      "56/463, train_loss: 0.2935\n",
      "57/463, train_loss: 0.2115\n",
      "58/463, train_loss: 0.3916\n",
      "59/463, train_loss: 0.3999\n",
      "60/463, train_loss: 0.3818\n",
      "61/463, train_loss: 0.2871\n",
      "62/463, train_loss: 0.3208\n",
      "63/463, train_loss: 0.2969\n",
      "64/463, train_loss: 0.5723\n",
      "65/463, train_loss: 0.1227\n",
      "66/463, train_loss: 0.1960\n",
      "67/463, train_loss: 0.2827\n",
      "68/463, train_loss: 0.4561\n",
      "69/463, train_loss: 0.2617\n",
      "70/463, train_loss: 0.1294\n",
      "71/463, train_loss: 0.1852\n",
      "72/463, train_loss: 0.4482\n",
      "73/463, train_loss: 0.2186\n",
      "74/463, train_loss: 0.3608\n",
      "75/463, train_loss: 0.2308\n",
      "76/463, train_loss: 0.3440\n",
      "77/463, train_loss: 0.6704\n",
      "78/463, train_loss: 0.2299\n",
      "79/463, train_loss: 0.2388\n",
      "80/463, train_loss: 0.2822\n",
      "81/463, train_loss: 0.4189\n",
      "82/463, train_loss: 0.1769\n",
      "83/463, train_loss: 0.1805\n",
      "84/463, train_loss: 0.2878\n",
      "85/463, train_loss: 0.3442\n",
      "86/463, train_loss: 0.1420\n",
      "87/463, train_loss: 0.1501\n",
      "88/463, train_loss: 0.6187\n",
      "89/463, train_loss: 0.7310\n",
      "90/463, train_loss: 0.2159\n",
      "91/463, train_loss: 0.2286\n",
      "92/463, train_loss: 0.0856\n",
      "93/463, train_loss: 0.1753\n",
      "94/463, train_loss: 0.2382\n",
      "95/463, train_loss: 1.0420\n",
      "96/463, train_loss: 0.0709\n",
      "97/463, train_loss: 0.3479\n",
      "98/463, train_loss: 0.1366\n",
      "99/463, train_loss: 0.1638\n",
      "100/463, train_loss: 0.1813\n",
      "101/463, train_loss: 0.2590\n",
      "102/463, train_loss: 0.5933\n",
      "103/463, train_loss: 0.1335\n",
      "104/463, train_loss: 0.1454\n",
      "105/463, train_loss: 0.2267\n",
      "106/463, train_loss: 0.4934\n",
      "107/463, train_loss: 0.1826\n",
      "108/463, train_loss: 0.6885\n",
      "109/463, train_loss: 0.1017\n",
      "110/463, train_loss: 0.4385\n",
      "111/463, train_loss: 0.3376\n",
      "112/463, train_loss: 0.1847\n",
      "113/463, train_loss: 0.1946\n",
      "114/463, train_loss: 0.3027\n",
      "115/463, train_loss: 0.3867\n",
      "116/463, train_loss: 0.4314\n",
      "117/463, train_loss: 0.1903\n",
      "118/463, train_loss: 0.4814\n",
      "119/463, train_loss: 0.2629\n",
      "120/463, train_loss: 0.1609\n",
      "121/463, train_loss: 0.1707\n",
      "122/463, train_loss: 0.4321\n",
      "123/463, train_loss: 0.2236\n",
      "124/463, train_loss: 0.2056\n",
      "125/463, train_loss: 0.2571\n",
      "126/463, train_loss: 0.3452\n",
      "127/463, train_loss: 0.5605\n",
      "128/463, train_loss: 0.3098\n",
      "129/463, train_loss: 0.3330\n",
      "130/463, train_loss: 0.2917\n",
      "131/463, train_loss: 0.6978\n",
      "132/463, train_loss: 0.1809\n",
      "133/463, train_loss: 0.4009\n",
      "134/463, train_loss: 0.4324\n",
      "135/463, train_loss: 0.4263\n",
      "136/463, train_loss: 0.1102\n",
      "137/463, train_loss: 0.1970\n",
      "138/463, train_loss: 0.1486\n",
      "139/463, train_loss: 0.2864\n",
      "140/463, train_loss: 0.7930\n",
      "141/463, train_loss: 0.1967\n",
      "142/463, train_loss: 0.1729\n",
      "143/463, train_loss: 0.2362\n",
      "144/463, train_loss: 0.3857\n",
      "145/463, train_loss: 0.2803\n",
      "146/463, train_loss: 0.2081\n",
      "147/463, train_loss: 0.1545\n",
      "148/463, train_loss: 0.0911\n",
      "149/463, train_loss: 0.3943\n",
      "150/463, train_loss: 0.2559\n",
      "151/463, train_loss: 0.2529\n",
      "152/463, train_loss: 1.2070\n",
      "153/463, train_loss: 0.4795\n",
      "154/463, train_loss: 0.1520\n",
      "155/463, train_loss: 0.1482\n",
      "156/463, train_loss: 0.4595\n",
      "157/463, train_loss: 0.2639\n",
      "158/463, train_loss: 0.2759\n",
      "159/463, train_loss: 0.1389\n",
      "160/463, train_loss: 0.1876\n",
      "161/463, train_loss: 0.3442\n",
      "162/463, train_loss: 0.3982\n",
      "163/463, train_loss: 0.1858\n",
      "164/463, train_loss: 0.1750\n",
      "165/463, train_loss: 0.2395\n",
      "166/463, train_loss: 0.2119\n",
      "167/463, train_loss: 0.2473\n",
      "168/463, train_loss: 0.1045\n",
      "169/463, train_loss: 0.2004\n",
      "170/463, train_loss: 0.1160\n",
      "171/463, train_loss: 0.3147\n",
      "172/463, train_loss: 0.3936\n",
      "173/463, train_loss: 0.0956\n",
      "174/463, train_loss: 0.1460\n",
      "175/463, train_loss: 0.4609\n",
      "176/463, train_loss: 0.2422\n",
      "177/463, train_loss: 0.1573\n",
      "178/463, train_loss: 0.3103\n",
      "179/463, train_loss: 0.2725\n",
      "180/463, train_loss: 0.3542\n",
      "181/463, train_loss: 0.1047\n",
      "182/463, train_loss: 0.2189\n",
      "183/463, train_loss: 0.1133\n",
      "184/463, train_loss: 0.2261\n",
      "185/463, train_loss: 0.1417\n",
      "186/463, train_loss: 0.2397\n",
      "187/463, train_loss: 0.3198\n",
      "188/463, train_loss: 0.3027\n",
      "189/463, train_loss: 0.1283\n",
      "190/463, train_loss: 0.1105\n",
      "191/463, train_loss: 0.1650\n",
      "192/463, train_loss: 0.0991\n",
      "193/463, train_loss: 0.1797\n",
      "194/463, train_loss: 0.0484\n",
      "195/463, train_loss: 0.1030\n",
      "196/463, train_loss: 1.0664\n",
      "197/463, train_loss: 0.3391\n",
      "198/463, train_loss: 0.1211\n",
      "199/463, train_loss: 0.5840\n",
      "200/463, train_loss: 0.4722\n",
      "201/463, train_loss: 0.1079\n",
      "202/463, train_loss: 0.2136\n",
      "203/463, train_loss: 0.2137\n",
      "204/463, train_loss: 0.4302\n",
      "205/463, train_loss: 0.4639\n",
      "206/463, train_loss: 0.6025\n",
      "207/463, train_loss: 0.3325\n",
      "208/463, train_loss: 0.2258\n",
      "209/463, train_loss: 0.3059\n",
      "210/463, train_loss: 0.2247\n",
      "211/463, train_loss: 0.1941\n",
      "212/463, train_loss: 0.3545\n",
      "213/463, train_loss: 0.1449\n",
      "214/463, train_loss: 0.4143\n",
      "215/463, train_loss: 0.4111\n",
      "216/463, train_loss: 0.4370\n",
      "217/463, train_loss: 0.6904\n",
      "218/463, train_loss: 0.3130\n",
      "219/463, train_loss: 0.4695\n",
      "220/463, train_loss: 0.2412\n",
      "221/463, train_loss: 0.1549\n",
      "222/463, train_loss: 0.2095\n",
      "223/463, train_loss: 0.3135\n",
      "224/463, train_loss: 0.3340\n",
      "225/463, train_loss: 0.1360\n",
      "226/463, train_loss: 0.3408\n",
      "227/463, train_loss: 0.2480\n",
      "228/463, train_loss: 0.0942\n",
      "229/463, train_loss: 0.1606\n",
      "230/463, train_loss: 0.6958\n",
      "231/463, train_loss: 0.1565\n",
      "232/463, train_loss: 0.3911\n",
      "233/463, train_loss: 0.2944\n",
      "234/463, train_loss: 0.2930\n",
      "235/463, train_loss: 0.2661\n",
      "236/463, train_loss: 0.2159\n",
      "237/463, train_loss: 0.1853\n",
      "238/463, train_loss: 0.3926\n",
      "239/463, train_loss: 0.3911\n",
      "240/463, train_loss: 0.1748\n",
      "241/463, train_loss: 0.1907\n",
      "242/463, train_loss: 0.2927\n",
      "243/463, train_loss: 0.5537\n",
      "244/463, train_loss: 0.1180\n",
      "245/463, train_loss: 0.1004\n",
      "246/463, train_loss: 0.2639\n",
      "247/463, train_loss: 0.5063\n",
      "248/463, train_loss: 0.1180\n",
      "249/463, train_loss: 0.2585\n",
      "250/463, train_loss: 0.2004\n",
      "251/463, train_loss: 0.3955\n",
      "252/463, train_loss: 0.2151\n",
      "253/463, train_loss: 0.2186\n",
      "254/463, train_loss: 0.3601\n",
      "255/463, train_loss: 0.1675\n",
      "256/463, train_loss: 0.3240\n",
      "257/463, train_loss: 0.2454\n",
      "258/463, train_loss: 0.2036\n",
      "259/463, train_loss: 0.1794\n",
      "260/463, train_loss: 0.1155\n",
      "261/463, train_loss: 0.2966\n",
      "262/463, train_loss: 0.1278\n",
      "263/463, train_loss: 0.1333\n",
      "264/463, train_loss: 0.2715\n",
      "265/463, train_loss: 0.1270\n",
      "266/463, train_loss: 0.8174\n",
      "267/463, train_loss: 0.1716\n",
      "268/463, train_loss: 0.4812\n",
      "269/463, train_loss: 0.1082\n",
      "270/463, train_loss: 0.1975\n",
      "271/463, train_loss: 0.1545\n",
      "272/463, train_loss: 0.4219\n",
      "273/463, train_loss: 0.0694\n",
      "274/463, train_loss: 0.3540\n",
      "275/463, train_loss: 0.0991\n",
      "276/463, train_loss: 0.6587\n",
      "277/463, train_loss: 0.3252\n",
      "278/463, train_loss: 0.2532\n",
      "279/463, train_loss: 0.1096\n",
      "280/463, train_loss: 0.1213\n",
      "281/463, train_loss: 0.2812\n",
      "282/463, train_loss: 0.3496\n",
      "283/463, train_loss: 0.1266\n",
      "284/463, train_loss: 0.3057\n",
      "285/463, train_loss: 0.4504\n",
      "286/463, train_loss: 0.8022\n",
      "287/463, train_loss: 0.1281\n",
      "288/463, train_loss: 0.5303\n",
      "289/463, train_loss: 0.2905\n",
      "290/463, train_loss: 0.1648\n",
      "291/463, train_loss: 0.4258\n",
      "292/463, train_loss: 0.1365\n",
      "293/463, train_loss: 0.2549\n",
      "294/463, train_loss: 0.2581\n",
      "295/463, train_loss: 0.3210\n",
      "296/463, train_loss: 0.2776\n",
      "297/463, train_loss: 0.1809\n",
      "298/463, train_loss: 0.3459\n",
      "299/463, train_loss: 0.1577\n",
      "300/463, train_loss: 0.2194\n",
      "301/463, train_loss: 0.4258\n",
      "302/463, train_loss: 0.2031\n",
      "303/463, train_loss: 0.1597\n",
      "304/463, train_loss: 0.0890\n",
      "305/463, train_loss: 0.1724\n",
      "306/463, train_loss: 0.1567\n",
      "307/463, train_loss: 0.3782\n",
      "308/463, train_loss: 0.1775\n",
      "309/463, train_loss: 0.4138\n",
      "310/463, train_loss: 0.1370\n",
      "311/463, train_loss: 0.3030\n",
      "312/463, train_loss: 0.8882\n",
      "313/463, train_loss: 0.3330\n",
      "314/463, train_loss: 0.1692\n",
      "315/463, train_loss: 0.1831\n",
      "316/463, train_loss: 0.1863\n",
      "317/463, train_loss: 0.8223\n",
      "318/463, train_loss: 0.2263\n",
      "319/463, train_loss: 0.3093\n",
      "320/463, train_loss: 0.1597\n",
      "321/463, train_loss: 0.1887\n",
      "322/463, train_loss: 0.2800\n",
      "323/463, train_loss: 0.3135\n",
      "324/463, train_loss: 0.1553\n",
      "325/463, train_loss: 0.3735\n",
      "326/463, train_loss: 0.4932\n",
      "327/463, train_loss: 0.1915\n",
      "328/463, train_loss: 0.2039\n",
      "329/463, train_loss: 0.1511\n",
      "330/463, train_loss: 0.1918\n",
      "331/463, train_loss: 0.2454\n",
      "332/463, train_loss: 0.1952\n",
      "333/463, train_loss: 0.3518\n",
      "334/463, train_loss: 0.1465\n",
      "335/463, train_loss: 0.9980\n",
      "336/463, train_loss: 0.1536\n",
      "337/463, train_loss: 0.2896\n",
      "338/463, train_loss: 0.2834\n",
      "339/463, train_loss: 0.1027\n",
      "340/463, train_loss: 0.1010\n",
      "341/463, train_loss: 0.1142\n",
      "342/463, train_loss: 0.3303\n",
      "343/463, train_loss: 0.1653\n",
      "344/463, train_loss: 0.2192\n",
      "345/463, train_loss: 0.3701\n",
      "346/463, train_loss: 0.3457\n",
      "347/463, train_loss: 0.2393\n",
      "348/463, train_loss: 0.1207\n",
      "349/463, train_loss: 0.4180\n",
      "350/463, train_loss: 0.1130\n",
      "351/463, train_loss: 0.2505\n",
      "352/463, train_loss: 0.1907\n",
      "353/463, train_loss: 0.1681\n",
      "354/463, train_loss: 0.2175\n",
      "355/463, train_loss: 0.2546\n",
      "356/463, train_loss: 0.3354\n",
      "357/463, train_loss: 0.5864\n",
      "358/463, train_loss: 0.2319\n",
      "359/463, train_loss: 0.2776\n",
      "360/463, train_loss: 0.4014\n",
      "361/463, train_loss: 0.2976\n",
      "362/463, train_loss: 0.3894\n",
      "363/463, train_loss: 0.1847\n",
      "364/463, train_loss: 0.2632\n",
      "365/463, train_loss: 0.1414\n",
      "366/463, train_loss: 0.1400\n",
      "367/463, train_loss: 0.4814\n",
      "368/463, train_loss: 0.1383\n",
      "369/463, train_loss: 0.2585\n",
      "370/463, train_loss: 0.3901\n",
      "371/463, train_loss: 0.1907\n",
      "372/463, train_loss: 0.3516\n",
      "373/463, train_loss: 0.3352\n",
      "374/463, train_loss: 0.4956\n",
      "375/463, train_loss: 0.3472\n",
      "376/463, train_loss: 0.5884\n",
      "377/463, train_loss: 0.2327\n",
      "378/463, train_loss: 0.5400\n",
      "379/463, train_loss: 0.3479\n",
      "380/463, train_loss: 0.1479\n",
      "381/463, train_loss: 0.7095\n",
      "382/463, train_loss: 0.5522\n",
      "383/463, train_loss: 0.2563\n",
      "384/463, train_loss: 0.2317\n",
      "385/463, train_loss: 0.1797\n",
      "386/463, train_loss: 0.2656\n",
      "387/463, train_loss: 0.3345\n",
      "388/463, train_loss: 0.3779\n",
      "389/463, train_loss: 0.2925\n",
      "390/463, train_loss: 0.2322\n",
      "391/463, train_loss: 0.2510\n",
      "392/463, train_loss: 0.2578\n",
      "393/463, train_loss: 0.1769\n",
      "394/463, train_loss: 0.1780\n",
      "395/463, train_loss: 0.1614\n",
      "396/463, train_loss: 0.1653\n",
      "397/463, train_loss: 0.4927\n",
      "398/463, train_loss: 0.1724\n",
      "399/463, train_loss: 0.2368\n",
      "400/463, train_loss: 0.5396\n",
      "401/463, train_loss: 0.8770\n",
      "402/463, train_loss: 0.1870\n",
      "403/463, train_loss: 0.1482\n",
      "404/463, train_loss: 0.4417\n",
      "405/463, train_loss: 0.3030\n",
      "406/463, train_loss: 0.2964\n",
      "407/463, train_loss: 0.2949\n",
      "408/463, train_loss: 0.3254\n",
      "409/463, train_loss: 0.1382\n",
      "410/463, train_loss: 0.3162\n",
      "411/463, train_loss: 0.1505\n",
      "412/463, train_loss: 0.1852\n",
      "413/463, train_loss: 0.1741\n",
      "414/463, train_loss: 0.3352\n",
      "415/463, train_loss: 0.5142\n",
      "416/463, train_loss: 0.1239\n",
      "417/463, train_loss: 0.3367\n",
      "418/463, train_loss: 0.1940\n",
      "419/463, train_loss: 0.4521\n",
      "420/463, train_loss: 0.2361\n",
      "421/463, train_loss: 0.2341\n",
      "422/463, train_loss: 0.2012\n",
      "423/463, train_loss: 0.2905\n",
      "424/463, train_loss: 0.2766\n",
      "425/463, train_loss: 0.1027\n",
      "426/463, train_loss: 0.1415\n",
      "427/463, train_loss: 0.1201\n",
      "428/463, train_loss: 0.2263\n",
      "429/463, train_loss: 0.2039\n",
      "430/463, train_loss: 0.2566\n",
      "431/463, train_loss: 0.2004\n",
      "432/463, train_loss: 0.3240\n",
      "433/463, train_loss: 0.2905\n",
      "434/463, train_loss: 0.2192\n",
      "435/463, train_loss: 0.1333\n",
      "436/463, train_loss: 0.1289\n",
      "437/463, train_loss: 0.3030\n",
      "438/463, train_loss: 0.1066\n",
      "439/463, train_loss: 0.1644\n",
      "440/463, train_loss: 0.1570\n",
      "441/463, train_loss: 0.1348\n",
      "442/463, train_loss: 0.5190\n",
      "443/463, train_loss: 0.5254\n",
      "444/463, train_loss: 0.8955\n",
      "445/463, train_loss: 0.2429\n",
      "446/463, train_loss: 0.2236\n",
      "447/463, train_loss: 0.2405\n",
      "448/463, train_loss: 0.2085\n",
      "449/463, train_loss: 0.0755\n",
      "450/463, train_loss: 0.1406\n",
      "451/463, train_loss: 0.4058\n",
      "452/463, train_loss: 0.0856\n",
      "453/463, train_loss: 1.0371\n",
      "454/463, train_loss: 0.1499\n",
      "455/463, train_loss: 0.5532\n",
      "456/463, train_loss: 0.0924\n",
      "457/463, train_loss: 0.1432\n",
      "458/463, train_loss: 0.1626\n",
      "459/463, train_loss: 0.1343\n",
      "460/463, train_loss: 0.2529\n",
      "461/463, train_loss: 0.1788\n",
      "462/463, train_loss: 0.1071\n",
      "463/463, train_loss: 0.1697\n",
      "epoch 7 average loss: 0.2924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 20:29:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 20:29:30 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 8/100\n",
      "1/463, train_loss: 0.1360\n",
      "2/463, train_loss: 0.1965\n",
      "3/463, train_loss: 0.0698\n",
      "4/463, train_loss: 0.1440\n",
      "5/463, train_loss: 0.0904\n",
      "6/463, train_loss: 0.3535\n",
      "7/463, train_loss: 0.0881\n",
      "8/463, train_loss: 0.2869\n",
      "9/463, train_loss: 0.1713\n",
      "10/463, train_loss: 0.1891\n",
      "11/463, train_loss: 0.5044\n",
      "12/463, train_loss: 0.2019\n",
      "13/463, train_loss: 0.1428\n",
      "14/463, train_loss: 0.2424\n",
      "15/463, train_loss: 0.3401\n",
      "16/463, train_loss: 0.2090\n",
      "17/463, train_loss: 0.1375\n",
      "18/463, train_loss: 0.2544\n",
      "19/463, train_loss: 0.2849\n",
      "20/463, train_loss: 0.1791\n",
      "21/463, train_loss: 0.2319\n",
      "22/463, train_loss: 0.1130\n",
      "23/463, train_loss: 0.2966\n",
      "24/463, train_loss: 0.5542\n",
      "25/463, train_loss: 0.0917\n",
      "26/463, train_loss: 0.1635\n",
      "27/463, train_loss: 0.1694\n",
      "28/463, train_loss: 0.0764\n",
      "29/463, train_loss: 0.3838\n",
      "30/463, train_loss: 0.2529\n",
      "31/463, train_loss: 0.1294\n",
      "32/463, train_loss: 0.2695\n",
      "33/463, train_loss: 0.1182\n",
      "34/463, train_loss: 0.1241\n",
      "35/463, train_loss: 0.1841\n",
      "36/463, train_loss: 0.4807\n",
      "37/463, train_loss: 0.2800\n",
      "38/463, train_loss: 0.1328\n",
      "39/463, train_loss: 0.1183\n",
      "40/463, train_loss: 0.1132\n",
      "41/463, train_loss: 0.2930\n",
      "42/463, train_loss: 0.1555\n",
      "43/463, train_loss: 0.6729\n",
      "44/463, train_loss: 0.3889\n",
      "45/463, train_loss: 0.2800\n",
      "46/463, train_loss: 0.2732\n",
      "47/463, train_loss: 0.4937\n",
      "48/463, train_loss: 0.2854\n",
      "49/463, train_loss: 0.1740\n",
      "50/463, train_loss: 0.2878\n",
      "51/463, train_loss: 0.1432\n",
      "52/463, train_loss: 0.3506\n",
      "53/463, train_loss: 0.2668\n",
      "54/463, train_loss: 0.1241\n",
      "55/463, train_loss: 0.2957\n",
      "56/463, train_loss: 0.2159\n",
      "57/463, train_loss: 0.3723\n",
      "58/463, train_loss: 0.7285\n",
      "59/463, train_loss: 0.1439\n",
      "60/463, train_loss: 0.2612\n",
      "61/463, train_loss: 0.1033\n",
      "62/463, train_loss: 0.1173\n",
      "63/463, train_loss: 0.1633\n",
      "64/463, train_loss: 0.2284\n",
      "65/463, train_loss: 0.1006\n",
      "66/463, train_loss: 0.2751\n",
      "67/463, train_loss: 0.7910\n",
      "68/463, train_loss: 0.2097\n",
      "69/463, train_loss: 0.2173\n",
      "70/463, train_loss: 0.1379\n",
      "71/463, train_loss: 0.4919\n",
      "72/463, train_loss: 0.2115\n",
      "73/463, train_loss: 0.2986\n",
      "74/463, train_loss: 0.1936\n",
      "75/463, train_loss: 0.3757\n",
      "76/463, train_loss: 0.1990\n",
      "77/463, train_loss: 0.2090\n",
      "78/463, train_loss: 0.0986\n",
      "79/463, train_loss: 0.2742\n",
      "80/463, train_loss: 0.4756\n",
      "81/463, train_loss: 0.3577\n",
      "82/463, train_loss: 0.1843\n",
      "83/463, train_loss: 0.2184\n",
      "84/463, train_loss: 0.6465\n",
      "85/463, train_loss: 0.5161\n",
      "86/463, train_loss: 0.6558\n",
      "87/463, train_loss: 0.5522\n",
      "88/463, train_loss: 0.1628\n",
      "89/463, train_loss: 0.1616\n",
      "90/463, train_loss: 0.1405\n",
      "91/463, train_loss: 0.4497\n",
      "92/463, train_loss: 0.1711\n",
      "93/463, train_loss: 0.2756\n",
      "94/463, train_loss: 0.4458\n",
      "95/463, train_loss: 0.3628\n",
      "96/463, train_loss: 0.2235\n",
      "97/463, train_loss: 0.2510\n",
      "98/463, train_loss: 0.2510\n",
      "99/463, train_loss: 0.1670\n",
      "100/463, train_loss: 0.5005\n",
      "101/463, train_loss: 0.3848\n",
      "102/463, train_loss: 0.1423\n",
      "103/463, train_loss: 1.1035\n",
      "104/463, train_loss: 0.4678\n",
      "105/463, train_loss: 0.1732\n",
      "106/463, train_loss: 0.4868\n",
      "107/463, train_loss: 0.2683\n",
      "108/463, train_loss: 0.2786\n",
      "109/463, train_loss: 0.4558\n",
      "110/463, train_loss: 0.2478\n",
      "111/463, train_loss: 0.1545\n",
      "112/463, train_loss: 0.2646\n",
      "113/463, train_loss: 0.5840\n",
      "114/463, train_loss: 0.3308\n",
      "115/463, train_loss: 0.1919\n",
      "116/463, train_loss: 0.1316\n",
      "117/463, train_loss: 0.1343\n",
      "118/463, train_loss: 0.2900\n",
      "119/463, train_loss: 0.3174\n",
      "120/463, train_loss: 0.2527\n",
      "121/463, train_loss: 0.4434\n",
      "122/463, train_loss: 0.2900\n",
      "123/463, train_loss: 0.2600\n",
      "124/463, train_loss: 0.2252\n",
      "125/463, train_loss: 0.2418\n",
      "126/463, train_loss: 0.2520\n",
      "127/463, train_loss: 0.1813\n",
      "128/463, train_loss: 0.1785\n",
      "129/463, train_loss: 0.1479\n",
      "130/463, train_loss: 0.5659\n",
      "131/463, train_loss: 0.2686\n",
      "132/463, train_loss: 0.2134\n",
      "133/463, train_loss: 0.2179\n",
      "134/463, train_loss: 0.3616\n",
      "135/463, train_loss: 0.2983\n",
      "136/463, train_loss: 0.2051\n",
      "137/463, train_loss: 0.1464\n",
      "138/463, train_loss: 0.1562\n",
      "139/463, train_loss: 0.1377\n",
      "140/463, train_loss: 0.2306\n",
      "141/463, train_loss: 0.3374\n",
      "142/463, train_loss: 0.3838\n",
      "143/463, train_loss: 0.2703\n",
      "144/463, train_loss: 0.2443\n",
      "145/463, train_loss: 0.1252\n",
      "146/463, train_loss: 0.0682\n",
      "147/463, train_loss: 0.1980\n",
      "148/463, train_loss: 0.1323\n",
      "149/463, train_loss: 0.1721\n",
      "150/463, train_loss: 0.1137\n",
      "151/463, train_loss: 0.1042\n",
      "152/463, train_loss: 0.0730\n",
      "153/463, train_loss: 0.1443\n",
      "154/463, train_loss: 0.1733\n",
      "155/463, train_loss: 0.2126\n",
      "156/463, train_loss: 0.2402\n",
      "157/463, train_loss: 0.5801\n",
      "158/463, train_loss: 0.2881\n",
      "159/463, train_loss: 0.4719\n",
      "160/463, train_loss: 0.5493\n",
      "161/463, train_loss: 0.1311\n",
      "162/463, train_loss: 0.2205\n",
      "163/463, train_loss: 0.1835\n",
      "164/463, train_loss: 0.5732\n",
      "165/463, train_loss: 0.1400\n",
      "166/463, train_loss: 0.3623\n",
      "167/463, train_loss: 0.1289\n",
      "168/463, train_loss: 0.2002\n",
      "169/463, train_loss: 0.1234\n",
      "170/463, train_loss: 0.1168\n",
      "171/463, train_loss: 0.2339\n",
      "172/463, train_loss: 0.7583\n",
      "173/463, train_loss: 0.1070\n",
      "174/463, train_loss: 0.1650\n",
      "175/463, train_loss: 0.1215\n",
      "176/463, train_loss: 0.1744\n",
      "177/463, train_loss: 0.1921\n",
      "178/463, train_loss: 0.1746\n",
      "179/463, train_loss: 0.1445\n",
      "180/463, train_loss: 0.2356\n",
      "181/463, train_loss: 0.0835\n",
      "182/463, train_loss: 0.2388\n",
      "183/463, train_loss: 0.1263\n",
      "184/463, train_loss: 0.4082\n",
      "185/463, train_loss: 0.4009\n",
      "186/463, train_loss: 0.4819\n",
      "187/463, train_loss: 0.1001\n",
      "188/463, train_loss: 0.5000\n",
      "189/463, train_loss: 0.2761\n",
      "190/463, train_loss: 0.2729\n",
      "191/463, train_loss: 0.2678\n",
      "192/463, train_loss: 0.2124\n",
      "193/463, train_loss: 0.7998\n",
      "194/463, train_loss: 0.3149\n",
      "195/463, train_loss: 0.2153\n",
      "196/463, train_loss: 0.5190\n",
      "197/463, train_loss: 0.5068\n",
      "198/463, train_loss: 0.5098\n",
      "199/463, train_loss: 0.5063\n",
      "200/463, train_loss: 0.3159\n",
      "201/463, train_loss: 0.2598\n",
      "202/463, train_loss: 0.2424\n",
      "203/463, train_loss: 0.1548\n",
      "204/463, train_loss: 0.2935\n",
      "205/463, train_loss: 0.3247\n",
      "206/463, train_loss: 0.3301\n",
      "207/463, train_loss: 0.3088\n",
      "208/463, train_loss: 0.2095\n",
      "209/463, train_loss: 0.2722\n",
      "210/463, train_loss: 0.1918\n",
      "211/463, train_loss: 0.1046\n",
      "212/463, train_loss: 0.4287\n",
      "213/463, train_loss: 0.2595\n",
      "214/463, train_loss: 0.2712\n",
      "215/463, train_loss: 0.1846\n",
      "216/463, train_loss: 0.2751\n",
      "217/463, train_loss: 0.4944\n",
      "218/463, train_loss: 0.2449\n",
      "219/463, train_loss: 0.1340\n",
      "220/463, train_loss: 0.1626\n",
      "221/463, train_loss: 0.2654\n",
      "222/463, train_loss: 0.2365\n",
      "223/463, train_loss: 0.3936\n",
      "224/463, train_loss: 0.3796\n",
      "225/463, train_loss: 0.2793\n",
      "226/463, train_loss: 0.2808\n",
      "227/463, train_loss: 0.2632\n",
      "228/463, train_loss: 0.1556\n",
      "229/463, train_loss: 0.5098\n",
      "230/463, train_loss: 0.3223\n",
      "231/463, train_loss: 0.3926\n",
      "232/463, train_loss: 0.6074\n",
      "233/463, train_loss: 0.1427\n",
      "234/463, train_loss: 0.1971\n",
      "235/463, train_loss: 0.1907\n",
      "236/463, train_loss: 0.3662\n",
      "237/463, train_loss: 0.1414\n",
      "238/463, train_loss: 0.1112\n",
      "239/463, train_loss: 0.1505\n",
      "240/463, train_loss: 0.1335\n",
      "241/463, train_loss: 0.5786\n",
      "242/463, train_loss: 0.2295\n",
      "243/463, train_loss: 0.1074\n",
      "244/463, train_loss: 0.2795\n",
      "245/463, train_loss: 0.1407\n",
      "246/463, train_loss: 0.2493\n",
      "247/463, train_loss: 0.1129\n",
      "248/463, train_loss: 0.2343\n",
      "249/463, train_loss: 0.4421\n",
      "250/463, train_loss: 0.1765\n",
      "251/463, train_loss: 0.4038\n",
      "252/463, train_loss: 0.1182\n",
      "253/463, train_loss: 0.4905\n",
      "254/463, train_loss: 0.1896\n",
      "255/463, train_loss: 0.0925\n",
      "256/463, train_loss: 0.3708\n",
      "257/463, train_loss: 0.4802\n",
      "258/463, train_loss: 0.1986\n",
      "259/463, train_loss: 0.4785\n",
      "260/463, train_loss: 0.1204\n",
      "261/463, train_loss: 0.1409\n",
      "262/463, train_loss: 0.1283\n",
      "263/463, train_loss: 0.1049\n",
      "264/463, train_loss: 0.1558\n",
      "265/463, train_loss: 0.3430\n",
      "266/463, train_loss: 0.1307\n",
      "267/463, train_loss: 0.5244\n",
      "268/463, train_loss: 0.2935\n",
      "269/463, train_loss: 0.2041\n",
      "270/463, train_loss: 0.4329\n",
      "271/463, train_loss: 0.3950\n",
      "272/463, train_loss: 0.1173\n",
      "273/463, train_loss: 0.3584\n",
      "274/463, train_loss: 0.7490\n",
      "275/463, train_loss: 0.1855\n",
      "276/463, train_loss: 0.1630\n",
      "277/463, train_loss: 0.7832\n",
      "278/463, train_loss: 0.2225\n",
      "279/463, train_loss: 0.2795\n",
      "280/463, train_loss: 0.4990\n",
      "281/463, train_loss: 0.1404\n",
      "282/463, train_loss: 0.1776\n",
      "283/463, train_loss: 0.1893\n",
      "284/463, train_loss: 0.1288\n",
      "285/463, train_loss: 0.6182\n",
      "286/463, train_loss: 0.2512\n",
      "287/463, train_loss: 0.1316\n",
      "288/463, train_loss: 0.5391\n",
      "289/463, train_loss: 0.2455\n",
      "290/463, train_loss: 0.3857\n",
      "291/463, train_loss: 0.1686\n",
      "292/463, train_loss: 0.2827\n",
      "293/463, train_loss: 0.3015\n",
      "294/463, train_loss: 0.2280\n",
      "295/463, train_loss: 0.1899\n",
      "296/463, train_loss: 0.1227\n",
      "297/463, train_loss: 0.9980\n",
      "298/463, train_loss: 0.1423\n",
      "299/463, train_loss: 0.3933\n",
      "300/463, train_loss: 0.1477\n",
      "301/463, train_loss: 0.2275\n",
      "302/463, train_loss: 0.1282\n",
      "303/463, train_loss: 0.6206\n",
      "304/463, train_loss: 0.2122\n",
      "305/463, train_loss: 0.3906\n",
      "306/463, train_loss: 0.4912\n",
      "307/463, train_loss: 0.2603\n",
      "308/463, train_loss: 0.2314\n",
      "309/463, train_loss: 0.2783\n",
      "310/463, train_loss: 0.4126\n",
      "311/463, train_loss: 0.1052\n",
      "312/463, train_loss: 0.1222\n",
      "313/463, train_loss: 0.1864\n",
      "314/463, train_loss: 0.4070\n",
      "315/463, train_loss: 0.6860\n",
      "316/463, train_loss: 0.1089\n",
      "317/463, train_loss: 0.2341\n",
      "318/463, train_loss: 0.1528\n",
      "319/463, train_loss: 0.1498\n",
      "320/463, train_loss: 0.1947\n",
      "321/463, train_loss: 0.6792\n",
      "322/463, train_loss: 0.1719\n",
      "323/463, train_loss: 0.1328\n",
      "324/463, train_loss: 0.2190\n",
      "325/463, train_loss: 0.1359\n",
      "326/463, train_loss: 0.2568\n",
      "327/463, train_loss: 0.5132\n",
      "328/463, train_loss: 0.4243\n",
      "329/463, train_loss: 0.8560\n",
      "330/463, train_loss: 0.1031\n",
      "331/463, train_loss: 0.2810\n",
      "332/463, train_loss: 0.4375\n",
      "333/463, train_loss: 0.4180\n",
      "334/463, train_loss: 0.2451\n",
      "335/463, train_loss: 0.1519\n",
      "336/463, train_loss: 0.1986\n",
      "337/463, train_loss: 0.1951\n",
      "338/463, train_loss: 0.2424\n",
      "339/463, train_loss: 0.3838\n",
      "340/463, train_loss: 0.4370\n",
      "341/463, train_loss: 0.3257\n",
      "342/463, train_loss: 0.5059\n",
      "343/463, train_loss: 0.1218\n",
      "344/463, train_loss: 0.3975\n",
      "345/463, train_loss: 0.5146\n",
      "346/463, train_loss: 0.2598\n",
      "347/463, train_loss: 0.3950\n",
      "348/463, train_loss: 0.2185\n",
      "349/463, train_loss: 0.3799\n",
      "350/463, train_loss: 0.3335\n",
      "351/463, train_loss: 0.1140\n",
      "352/463, train_loss: 0.2065\n",
      "353/463, train_loss: 0.4058\n",
      "354/463, train_loss: 0.1268\n",
      "355/463, train_loss: 0.1638\n",
      "356/463, train_loss: 0.3452\n",
      "357/463, train_loss: 0.1201\n",
      "358/463, train_loss: 0.2131\n",
      "359/463, train_loss: 0.1021\n",
      "360/463, train_loss: 0.1685\n",
      "361/463, train_loss: 0.1385\n",
      "362/463, train_loss: 0.1324\n",
      "363/463, train_loss: 0.1065\n",
      "364/463, train_loss: 0.2913\n",
      "365/463, train_loss: 0.2423\n",
      "366/463, train_loss: 0.0955\n",
      "367/463, train_loss: 0.2084\n",
      "368/463, train_loss: 0.1165\n",
      "369/463, train_loss: 0.1567\n",
      "370/463, train_loss: 0.1812\n",
      "371/463, train_loss: 0.1160\n",
      "372/463, train_loss: 0.2515\n",
      "373/463, train_loss: 0.0893\n",
      "374/463, train_loss: 0.1130\n",
      "375/463, train_loss: 0.0978\n",
      "376/463, train_loss: 0.4482\n",
      "377/463, train_loss: 0.1794\n",
      "378/463, train_loss: 0.2239\n",
      "379/463, train_loss: 0.5854\n",
      "380/463, train_loss: 1.1826\n",
      "381/463, train_loss: 0.1829\n",
      "382/463, train_loss: 0.3984\n",
      "383/463, train_loss: 0.2629\n",
      "384/463, train_loss: 0.1370\n",
      "385/463, train_loss: 0.8647\n",
      "386/463, train_loss: 0.1693\n",
      "387/463, train_loss: 0.4233\n",
      "388/463, train_loss: 0.1741\n",
      "389/463, train_loss: 0.3071\n",
      "390/463, train_loss: 0.1599\n",
      "391/463, train_loss: 0.2595\n",
      "392/463, train_loss: 0.4111\n",
      "393/463, train_loss: 0.2260\n",
      "394/463, train_loss: 0.2036\n",
      "395/463, train_loss: 0.2487\n",
      "396/463, train_loss: 0.1052\n",
      "397/463, train_loss: 0.3010\n",
      "398/463, train_loss: 0.1140\n",
      "399/463, train_loss: 0.3203\n",
      "400/463, train_loss: 0.1753\n",
      "401/463, train_loss: 0.2793\n",
      "402/463, train_loss: 0.2661\n",
      "403/463, train_loss: 0.7725\n",
      "404/463, train_loss: 0.6182\n",
      "405/463, train_loss: 0.2749\n",
      "406/463, train_loss: 0.2051\n",
      "407/463, train_loss: 0.5332\n",
      "408/463, train_loss: 0.4531\n",
      "409/463, train_loss: 0.1844\n",
      "410/463, train_loss: 0.1479\n",
      "411/463, train_loss: 0.6040\n",
      "412/463, train_loss: 0.4580\n",
      "413/463, train_loss: 0.2588\n",
      "414/463, train_loss: 0.6064\n",
      "415/463, train_loss: 0.1162\n",
      "416/463, train_loss: 0.4131\n",
      "417/463, train_loss: 0.3237\n",
      "418/463, train_loss: 0.1459\n",
      "419/463, train_loss: 0.1616\n",
      "420/463, train_loss: 0.1459\n",
      "421/463, train_loss: 0.2258\n",
      "422/463, train_loss: 0.1758\n",
      "423/463, train_loss: 0.1343\n",
      "424/463, train_loss: 0.6440\n",
      "425/463, train_loss: 0.1395\n",
      "426/463, train_loss: 0.5576\n",
      "427/463, train_loss: 0.1425\n",
      "428/463, train_loss: 0.1171\n",
      "429/463, train_loss: 0.1616\n",
      "430/463, train_loss: 0.2474\n",
      "431/463, train_loss: 0.1802\n",
      "432/463, train_loss: 0.1229\n",
      "433/463, train_loss: 0.6797\n",
      "434/463, train_loss: 0.6895\n",
      "435/463, train_loss: 0.5439\n",
      "436/463, train_loss: 0.3481\n",
      "437/463, train_loss: 0.6372\n",
      "438/463, train_loss: 0.4180\n",
      "439/463, train_loss: 0.2279\n",
      "440/463, train_loss: 0.2671\n",
      "441/463, train_loss: 0.1368\n",
      "442/463, train_loss: 0.3816\n",
      "443/463, train_loss: 0.2939\n",
      "444/463, train_loss: 0.2117\n",
      "445/463, train_loss: 0.0653\n",
      "446/463, train_loss: 0.3560\n",
      "447/463, train_loss: 0.1731\n",
      "448/463, train_loss: 0.2136\n",
      "449/463, train_loss: 0.4131\n",
      "450/463, train_loss: 0.4282\n",
      "451/463, train_loss: 0.3989\n",
      "452/463, train_loss: 0.3647\n",
      "453/463, train_loss: 0.2421\n",
      "454/463, train_loss: 0.1581\n",
      "455/463, train_loss: 0.1292\n",
      "456/463, train_loss: 0.1975\n",
      "457/463, train_loss: 0.1454\n",
      "458/463, train_loss: 0.1599\n",
      "459/463, train_loss: 0.1638\n",
      "460/463, train_loss: 0.3730\n",
      "461/463, train_loss: 0.1780\n",
      "462/463, train_loss: 0.1763\n",
      "463/463, train_loss: 0.3564\n",
      "epoch 8 average loss: 0.2810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/20 22:42:13 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/20 22:42:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 9/100\n",
      "1/463, train_loss: 0.4058\n",
      "2/463, train_loss: 0.3936\n",
      "3/463, train_loss: 0.1482\n",
      "4/463, train_loss: 0.5518\n",
      "5/463, train_loss: 0.2219\n",
      "6/463, train_loss: 0.0900\n",
      "7/463, train_loss: 0.1813\n",
      "8/463, train_loss: 0.3467\n",
      "9/463, train_loss: 0.1892\n",
      "10/463, train_loss: 0.1700\n",
      "11/463, train_loss: 0.1586\n",
      "12/463, train_loss: 0.2512\n",
      "13/463, train_loss: 0.3354\n",
      "14/463, train_loss: 0.3442\n",
      "15/463, train_loss: 0.1948\n",
      "16/463, train_loss: 0.8799\n",
      "17/463, train_loss: 0.0871\n",
      "18/463, train_loss: 0.8193\n",
      "19/463, train_loss: 0.3311\n",
      "20/463, train_loss: 0.1960\n",
      "21/463, train_loss: 0.2437\n",
      "22/463, train_loss: 0.1693\n",
      "23/463, train_loss: 0.2715\n",
      "24/463, train_loss: 0.1467\n",
      "25/463, train_loss: 0.2769\n",
      "26/463, train_loss: 0.2333\n",
      "27/463, train_loss: 0.3354\n",
      "28/463, train_loss: 0.1581\n",
      "29/463, train_loss: 0.6113\n",
      "30/463, train_loss: 0.1760\n",
      "31/463, train_loss: 0.3259\n",
      "32/463, train_loss: 0.2727\n",
      "33/463, train_loss: 0.5376\n",
      "34/463, train_loss: 0.1765\n",
      "35/463, train_loss: 0.5352\n",
      "36/463, train_loss: 0.1180\n",
      "37/463, train_loss: 0.2515\n",
      "38/463, train_loss: 0.5591\n",
      "39/463, train_loss: 0.2188\n",
      "40/463, train_loss: 0.4060\n",
      "41/463, train_loss: 0.2146\n",
      "42/463, train_loss: 0.1166\n",
      "43/463, train_loss: 0.1637\n",
      "44/463, train_loss: 0.4243\n",
      "45/463, train_loss: 0.1630\n",
      "46/463, train_loss: 0.1087\n",
      "47/463, train_loss: 0.3057\n",
      "48/463, train_loss: 0.1755\n",
      "49/463, train_loss: 0.2057\n",
      "50/463, train_loss: 0.1641\n",
      "51/463, train_loss: 0.1049\n",
      "52/463, train_loss: 0.1318\n",
      "53/463, train_loss: 0.1506\n",
      "54/463, train_loss: 0.1842\n",
      "55/463, train_loss: 0.2091\n",
      "56/463, train_loss: 0.1873\n",
      "57/463, train_loss: 0.4973\n",
      "58/463, train_loss: 0.1186\n",
      "59/463, train_loss: 0.1434\n",
      "60/463, train_loss: 0.2339\n",
      "61/463, train_loss: 0.1766\n",
      "62/463, train_loss: 0.2169\n",
      "63/463, train_loss: 0.1355\n",
      "64/463, train_loss: 0.1676\n",
      "65/463, train_loss: 0.1649\n",
      "66/463, train_loss: 0.1421\n",
      "67/463, train_loss: 0.1130\n",
      "68/463, train_loss: 0.2485\n",
      "69/463, train_loss: 0.1978\n",
      "70/463, train_loss: 0.1346\n",
      "71/463, train_loss: 0.1498\n",
      "72/463, train_loss: 0.5229\n",
      "73/463, train_loss: 0.2194\n",
      "74/463, train_loss: 0.4663\n",
      "75/463, train_loss: 0.2510\n",
      "76/463, train_loss: 0.2094\n",
      "77/463, train_loss: 0.2070\n",
      "78/463, train_loss: 0.0898\n",
      "79/463, train_loss: 0.4944\n",
      "80/463, train_loss: 0.1475\n",
      "81/463, train_loss: 0.1449\n",
      "82/463, train_loss: 0.5068\n",
      "83/463, train_loss: 0.4585\n",
      "84/463, train_loss: 0.1885\n",
      "85/463, train_loss: 0.1348\n",
      "86/463, train_loss: 0.1290\n",
      "87/463, train_loss: 0.1511\n",
      "88/463, train_loss: 0.1971\n",
      "89/463, train_loss: 0.2426\n",
      "90/463, train_loss: 0.5005\n",
      "91/463, train_loss: 0.1414\n",
      "92/463, train_loss: 0.7500\n",
      "93/463, train_loss: 0.2483\n",
      "94/463, train_loss: 0.4819\n",
      "95/463, train_loss: 0.1335\n",
      "96/463, train_loss: 0.0812\n",
      "97/463, train_loss: 0.1415\n",
      "98/463, train_loss: 0.3071\n",
      "99/463, train_loss: 0.1375\n",
      "100/463, train_loss: 0.7344\n",
      "101/463, train_loss: 0.2830\n",
      "102/463, train_loss: 0.1246\n",
      "103/463, train_loss: 0.2690\n",
      "104/463, train_loss: 0.1011\n",
      "105/463, train_loss: 0.4199\n",
      "106/463, train_loss: 0.2673\n",
      "107/463, train_loss: 0.1339\n",
      "108/463, train_loss: 0.1338\n",
      "109/463, train_loss: 0.4148\n",
      "110/463, train_loss: 0.1453\n",
      "111/463, train_loss: 0.3582\n",
      "112/463, train_loss: 0.1992\n",
      "113/463, train_loss: 0.5142\n",
      "114/463, train_loss: 0.1338\n",
      "115/463, train_loss: 0.3174\n",
      "116/463, train_loss: 0.3276\n",
      "117/463, train_loss: 0.8374\n",
      "118/463, train_loss: 0.2396\n",
      "119/463, train_loss: 0.5508\n",
      "120/463, train_loss: 0.1453\n",
      "121/463, train_loss: 0.1393\n",
      "122/463, train_loss: 0.5278\n",
      "123/463, train_loss: 0.1287\n",
      "124/463, train_loss: 0.2603\n",
      "125/463, train_loss: 0.2556\n",
      "126/463, train_loss: 0.4497\n",
      "127/463, train_loss: 0.6777\n",
      "128/463, train_loss: 0.3623\n",
      "129/463, train_loss: 0.4712\n",
      "130/463, train_loss: 0.2854\n",
      "131/463, train_loss: 0.1970\n",
      "132/463, train_loss: 0.1982\n",
      "133/463, train_loss: 0.2781\n",
      "134/463, train_loss: 0.2146\n",
      "135/463, train_loss: 0.1561\n",
      "136/463, train_loss: 0.5396\n",
      "137/463, train_loss: 0.1877\n",
      "138/463, train_loss: 0.1709\n",
      "139/463, train_loss: 0.1154\n",
      "140/463, train_loss: 0.1176\n",
      "141/463, train_loss: 0.2759\n",
      "142/463, train_loss: 0.2922\n",
      "143/463, train_loss: 0.3030\n",
      "144/463, train_loss: 0.1140\n",
      "145/463, train_loss: 0.4128\n",
      "146/463, train_loss: 0.3101\n",
      "147/463, train_loss: 0.1642\n",
      "148/463, train_loss: 0.2822\n",
      "149/463, train_loss: 0.2554\n",
      "150/463, train_loss: 0.3147\n",
      "151/463, train_loss: 0.4080\n",
      "152/463, train_loss: 0.2288\n",
      "153/463, train_loss: 0.3215\n",
      "154/463, train_loss: 0.1406\n",
      "155/463, train_loss: 0.2920\n",
      "156/463, train_loss: 0.4453\n",
      "157/463, train_loss: 0.1444\n",
      "158/463, train_loss: 0.1567\n",
      "159/463, train_loss: 0.1609\n",
      "160/463, train_loss: 0.3140\n",
      "161/463, train_loss: 0.2197\n",
      "162/463, train_loss: 0.5894\n",
      "163/463, train_loss: 0.5664\n",
      "164/463, train_loss: 0.2086\n",
      "165/463, train_loss: 0.4878\n",
      "166/463, train_loss: 0.1960\n",
      "167/463, train_loss: 0.5269\n",
      "168/463, train_loss: 0.3674\n",
      "169/463, train_loss: 0.1239\n",
      "170/463, train_loss: 0.0943\n",
      "171/463, train_loss: 0.4771\n",
      "172/463, train_loss: 0.1589\n",
      "173/463, train_loss: 0.2744\n",
      "174/463, train_loss: 0.8984\n",
      "175/463, train_loss: 0.0828\n",
      "176/463, train_loss: 0.1533\n",
      "177/463, train_loss: 0.1644\n",
      "178/463, train_loss: 0.3472\n",
      "179/463, train_loss: 0.4536\n",
      "180/463, train_loss: 0.1448\n",
      "181/463, train_loss: 0.1823\n",
      "182/463, train_loss: 0.1538\n",
      "183/463, train_loss: 0.1123\n",
      "184/463, train_loss: 0.9922\n",
      "185/463, train_loss: 0.2000\n",
      "186/463, train_loss: 0.1791\n",
      "187/463, train_loss: 0.2052\n",
      "188/463, train_loss: 0.4817\n",
      "189/463, train_loss: 0.1166\n",
      "190/463, train_loss: 0.1479\n",
      "191/463, train_loss: 0.1626\n",
      "192/463, train_loss: 0.1523\n",
      "193/463, train_loss: 0.0741\n",
      "194/463, train_loss: 0.1379\n",
      "195/463, train_loss: 0.1077\n",
      "196/463, train_loss: 0.2440\n",
      "197/463, train_loss: 0.2759\n",
      "198/463, train_loss: 0.3225\n",
      "199/463, train_loss: 0.2205\n",
      "200/463, train_loss: 0.4814\n",
      "201/463, train_loss: 0.2070\n",
      "202/463, train_loss: 0.6362\n",
      "203/463, train_loss: 0.1278\n",
      "204/463, train_loss: 0.2463\n",
      "205/463, train_loss: 0.3691\n",
      "206/463, train_loss: 0.3174\n",
      "207/463, train_loss: 0.2136\n",
      "208/463, train_loss: 0.3794\n",
      "209/463, train_loss: 0.1594\n",
      "210/463, train_loss: 0.3413\n",
      "211/463, train_loss: 0.1832\n",
      "212/463, train_loss: 0.2300\n",
      "213/463, train_loss: 0.0985\n",
      "214/463, train_loss: 0.0941\n",
      "215/463, train_loss: 0.1116\n",
      "216/463, train_loss: 0.1831\n",
      "217/463, train_loss: 0.3616\n",
      "218/463, train_loss: 0.2773\n",
      "219/463, train_loss: 0.2014\n",
      "220/463, train_loss: 0.2434\n",
      "221/463, train_loss: 0.1655\n",
      "222/463, train_loss: 0.1102\n",
      "223/463, train_loss: 0.1471\n",
      "224/463, train_loss: 0.1371\n",
      "225/463, train_loss: 0.0645\n",
      "226/463, train_loss: 0.1532\n",
      "227/463, train_loss: 0.3972\n",
      "228/463, train_loss: 0.5679\n",
      "229/463, train_loss: 0.2122\n",
      "230/463, train_loss: 0.5791\n",
      "231/463, train_loss: 0.5806\n",
      "232/463, train_loss: 0.1658\n",
      "233/463, train_loss: 0.0970\n",
      "234/463, train_loss: 0.6675\n",
      "235/463, train_loss: 0.5303\n",
      "236/463, train_loss: 0.1500\n",
      "237/463, train_loss: 0.2798\n",
      "238/463, train_loss: 0.2258\n",
      "239/463, train_loss: 0.2791\n",
      "240/463, train_loss: 0.1982\n",
      "241/463, train_loss: 0.3350\n",
      "242/463, train_loss: 0.6211\n",
      "243/463, train_loss: 0.4189\n",
      "244/463, train_loss: 0.1533\n",
      "245/463, train_loss: 0.1427\n",
      "246/463, train_loss: 0.1149\n",
      "247/463, train_loss: 0.3276\n",
      "248/463, train_loss: 0.1249\n",
      "249/463, train_loss: 0.1896\n",
      "250/463, train_loss: 0.5942\n",
      "251/463, train_loss: 0.1362\n",
      "252/463, train_loss: 0.3877\n",
      "253/463, train_loss: 0.1081\n",
      "254/463, train_loss: 0.6797\n",
      "255/463, train_loss: 0.2322\n",
      "256/463, train_loss: 0.1092\n",
      "257/463, train_loss: 0.3315\n",
      "258/463, train_loss: 0.4092\n",
      "259/463, train_loss: 0.3647\n",
      "260/463, train_loss: 0.2988\n",
      "261/463, train_loss: 0.0978\n",
      "262/463, train_loss: 0.4365\n",
      "263/463, train_loss: 0.2468\n",
      "264/463, train_loss: 0.2510\n",
      "265/463, train_loss: 0.3813\n",
      "266/463, train_loss: 0.1743\n",
      "267/463, train_loss: 0.1331\n",
      "268/463, train_loss: 0.1785\n",
      "269/463, train_loss: 0.1863\n",
      "270/463, train_loss: 0.4351\n",
      "271/463, train_loss: 0.2295\n",
      "272/463, train_loss: 1.0732\n",
      "273/463, train_loss: 0.1287\n",
      "274/463, train_loss: 0.3359\n",
      "275/463, train_loss: 0.1752\n",
      "276/463, train_loss: 0.4316\n",
      "277/463, train_loss: 0.1733\n",
      "278/463, train_loss: 0.2454\n",
      "279/463, train_loss: 0.4624\n",
      "280/463, train_loss: 0.2690\n",
      "281/463, train_loss: 0.6113\n",
      "282/463, train_loss: 0.1354\n",
      "283/463, train_loss: 0.2686\n",
      "284/463, train_loss: 0.1646\n",
      "285/463, train_loss: 0.2014\n",
      "286/463, train_loss: 0.3787\n",
      "287/463, train_loss: 0.1554\n",
      "288/463, train_loss: 0.4043\n",
      "289/463, train_loss: 0.4258\n",
      "290/463, train_loss: 0.0477\n",
      "291/463, train_loss: 0.7607\n",
      "292/463, train_loss: 0.4958\n",
      "293/463, train_loss: 0.1316\n",
      "294/463, train_loss: 0.2629\n",
      "295/463, train_loss: 0.1973\n",
      "296/463, train_loss: 0.7373\n",
      "297/463, train_loss: 0.2236\n",
      "298/463, train_loss: 0.2808\n",
      "299/463, train_loss: 0.2671\n",
      "300/463, train_loss: 0.1516\n",
      "301/463, train_loss: 0.1661\n",
      "302/463, train_loss: 0.2661\n",
      "303/463, train_loss: 0.1648\n",
      "304/463, train_loss: 0.1707\n",
      "305/463, train_loss: 0.2124\n",
      "306/463, train_loss: 0.2112\n",
      "307/463, train_loss: 0.0936\n",
      "308/463, train_loss: 0.3176\n",
      "309/463, train_loss: 0.1377\n",
      "310/463, train_loss: 0.2554\n",
      "311/463, train_loss: 0.2188\n",
      "312/463, train_loss: 0.1899\n",
      "313/463, train_loss: 0.1157\n",
      "314/463, train_loss: 0.2434\n",
      "315/463, train_loss: 0.2371\n",
      "316/463, train_loss: 0.1842\n",
      "317/463, train_loss: 0.1819\n",
      "318/463, train_loss: 0.3252\n",
      "319/463, train_loss: 0.0840\n",
      "320/463, train_loss: 0.1195\n",
      "321/463, train_loss: 0.1676\n",
      "322/463, train_loss: 0.3157\n",
      "323/463, train_loss: 0.1736\n",
      "324/463, train_loss: 0.6230\n",
      "325/463, train_loss: 0.1367\n",
      "326/463, train_loss: 0.0981\n",
      "327/463, train_loss: 0.1168\n",
      "328/463, train_loss: 0.2034\n",
      "329/463, train_loss: 0.3472\n",
      "330/463, train_loss: 0.1270\n",
      "331/463, train_loss: 0.1619\n",
      "332/463, train_loss: 0.3950\n",
      "333/463, train_loss: 0.6182\n",
      "334/463, train_loss: 0.1382\n",
      "335/463, train_loss: 0.2683\n",
      "336/463, train_loss: 0.0575\n",
      "337/463, train_loss: 0.3545\n",
      "338/463, train_loss: 0.2969\n",
      "339/463, train_loss: 0.2966\n",
      "340/463, train_loss: 0.1632\n",
      "341/463, train_loss: 0.1009\n",
      "342/463, train_loss: 0.4268\n",
      "343/463, train_loss: 0.2413\n",
      "344/463, train_loss: 0.2170\n",
      "345/463, train_loss: 0.5459\n",
      "346/463, train_loss: 0.1506\n",
      "347/463, train_loss: 0.3010\n",
      "348/463, train_loss: 0.9907\n",
      "349/463, train_loss: 0.1859\n",
      "350/463, train_loss: 0.3433\n",
      "351/463, train_loss: 0.1945\n",
      "352/463, train_loss: 0.1354\n",
      "353/463, train_loss: 0.1631\n",
      "354/463, train_loss: 0.5205\n",
      "355/463, train_loss: 0.2191\n",
      "356/463, train_loss: 0.2966\n",
      "357/463, train_loss: 0.4421\n",
      "358/463, train_loss: 0.1288\n",
      "359/463, train_loss: 0.4741\n",
      "360/463, train_loss: 0.1838\n",
      "361/463, train_loss: 0.1077\n",
      "362/463, train_loss: 0.5020\n",
      "363/463, train_loss: 0.3496\n",
      "364/463, train_loss: 0.1978\n",
      "365/463, train_loss: 0.0995\n",
      "366/463, train_loss: 0.1602\n",
      "367/463, train_loss: 0.2859\n",
      "368/463, train_loss: 0.1071\n",
      "369/463, train_loss: 0.7329\n",
      "370/463, train_loss: 0.1995\n",
      "371/463, train_loss: 0.1995\n",
      "372/463, train_loss: 0.2222\n",
      "373/463, train_loss: 0.1660\n",
      "374/463, train_loss: 0.4700\n",
      "375/463, train_loss: 0.5859\n",
      "376/463, train_loss: 0.2137\n",
      "377/463, train_loss: 0.1398\n",
      "378/463, train_loss: 0.1826\n",
      "379/463, train_loss: 0.2389\n",
      "380/463, train_loss: 0.1504\n",
      "381/463, train_loss: 0.4463\n",
      "382/463, train_loss: 0.1296\n",
      "383/463, train_loss: 0.1102\n",
      "384/463, train_loss: 0.4395\n",
      "385/463, train_loss: 0.3445\n",
      "386/463, train_loss: 0.1995\n",
      "387/463, train_loss: 0.2627\n",
      "388/463, train_loss: 0.5386\n",
      "389/463, train_loss: 0.1530\n",
      "390/463, train_loss: 0.1858\n",
      "391/463, train_loss: 0.3513\n",
      "392/463, train_loss: 0.4644\n",
      "393/463, train_loss: 0.2551\n",
      "394/463, train_loss: 0.1044\n",
      "395/463, train_loss: 0.2637\n",
      "396/463, train_loss: 0.0887\n",
      "397/463, train_loss: 0.5938\n",
      "398/463, train_loss: 0.4939\n",
      "399/463, train_loss: 0.6221\n",
      "400/463, train_loss: 0.2097\n",
      "401/463, train_loss: 0.3574\n",
      "402/463, train_loss: 0.3042\n",
      "403/463, train_loss: 0.4502\n",
      "404/463, train_loss: 0.2349\n",
      "405/463, train_loss: 0.4058\n",
      "406/463, train_loss: 0.4297\n",
      "407/463, train_loss: 0.4045\n",
      "408/463, train_loss: 0.2146\n",
      "409/463, train_loss: 0.1229\n",
      "410/463, train_loss: 0.1404\n",
      "411/463, train_loss: 0.4924\n",
      "412/463, train_loss: 0.2249\n",
      "413/463, train_loss: 0.1804\n",
      "414/463, train_loss: 0.1609\n",
      "415/463, train_loss: 0.4360\n",
      "416/463, train_loss: 0.1899\n",
      "417/463, train_loss: 0.1761\n",
      "418/463, train_loss: 0.2205\n",
      "419/463, train_loss: 0.2627\n",
      "420/463, train_loss: 0.3591\n",
      "421/463, train_loss: 0.2930\n",
      "422/463, train_loss: 0.3706\n",
      "423/463, train_loss: 0.9717\n",
      "424/463, train_loss: 0.1329\n",
      "425/463, train_loss: 0.1316\n",
      "426/463, train_loss: 0.5801\n",
      "427/463, train_loss: 0.5127\n",
      "428/463, train_loss: 0.2297\n",
      "429/463, train_loss: 0.1008\n",
      "430/463, train_loss: 0.4084\n",
      "431/463, train_loss: 0.1445\n",
      "432/463, train_loss: 0.1753\n",
      "433/463, train_loss: 0.3069\n",
      "434/463, train_loss: 0.5186\n",
      "435/463, train_loss: 0.4109\n",
      "436/463, train_loss: 0.3271\n",
      "437/463, train_loss: 0.3318\n",
      "438/463, train_loss: 0.2317\n",
      "439/463, train_loss: 0.5430\n",
      "440/463, train_loss: 0.2284\n",
      "441/463, train_loss: 0.1287\n",
      "442/463, train_loss: 0.1284\n",
      "443/463, train_loss: 0.1650\n",
      "444/463, train_loss: 0.1709\n",
      "445/463, train_loss: 0.3364\n",
      "446/463, train_loss: 0.1760\n",
      "447/463, train_loss: 0.1184\n",
      "448/463, train_loss: 0.1292\n",
      "449/463, train_loss: 0.5308\n",
      "450/463, train_loss: 0.3394\n",
      "451/463, train_loss: 0.8188\n",
      "452/463, train_loss: 0.2271\n",
      "453/463, train_loss: 0.1770\n",
      "454/463, train_loss: 0.4490\n",
      "455/463, train_loss: 0.1182\n",
      "456/463, train_loss: 0.1965\n",
      "457/463, train_loss: 0.3748\n",
      "458/463, train_loss: 0.3384\n",
      "459/463, train_loss: 0.6753\n",
      "460/463, train_loss: 0.1747\n",
      "461/463, train_loss: 0.1431\n",
      "462/463, train_loss: 0.1451\n",
      "463/463, train_loss: 0.3853\n",
      "epoch 9 average loss: 0.2810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 00:55:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 00:55:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 00:55:17 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 10/100\n",
      "1/463, train_loss: 0.5845\n",
      "2/463, train_loss: 0.0815\n",
      "3/463, train_loss: 0.2394\n",
      "4/463, train_loss: 0.3394\n",
      "5/463, train_loss: 0.1449\n",
      "6/463, train_loss: 0.2874\n",
      "7/463, train_loss: 0.1575\n",
      "8/463, train_loss: 0.3606\n",
      "9/463, train_loss: 0.2224\n",
      "10/463, train_loss: 0.1187\n",
      "11/463, train_loss: 0.3755\n",
      "12/463, train_loss: 0.2964\n",
      "13/463, train_loss: 0.2383\n",
      "14/463, train_loss: 0.4438\n",
      "15/463, train_loss: 0.2812\n",
      "16/463, train_loss: 0.5254\n",
      "17/463, train_loss: 0.1993\n",
      "18/463, train_loss: 0.1970\n",
      "19/463, train_loss: 0.2515\n",
      "20/463, train_loss: 0.1666\n",
      "21/463, train_loss: 0.1101\n",
      "22/463, train_loss: 0.1631\n",
      "23/463, train_loss: 0.2108\n",
      "24/463, train_loss: 0.1726\n",
      "25/463, train_loss: 0.1061\n",
      "26/463, train_loss: 0.1483\n",
      "27/463, train_loss: 0.2554\n",
      "28/463, train_loss: 0.1392\n",
      "29/463, train_loss: 0.5625\n",
      "30/463, train_loss: 0.2382\n",
      "31/463, train_loss: 0.1947\n",
      "32/463, train_loss: 0.2517\n",
      "33/463, train_loss: 0.2250\n",
      "34/463, train_loss: 0.3813\n",
      "35/463, train_loss: 0.5342\n",
      "36/463, train_loss: 0.1460\n",
      "37/463, train_loss: 0.1682\n",
      "38/463, train_loss: 0.1793\n",
      "39/463, train_loss: 0.3159\n",
      "40/463, train_loss: 0.1812\n",
      "41/463, train_loss: 0.4922\n",
      "42/463, train_loss: 0.2211\n",
      "43/463, train_loss: 0.1423\n",
      "44/463, train_loss: 0.1200\n",
      "45/463, train_loss: 0.4580\n",
      "46/463, train_loss: 0.0403\n",
      "47/463, train_loss: 0.1528\n",
      "48/463, train_loss: 0.0731\n",
      "49/463, train_loss: 0.4121\n",
      "50/463, train_loss: 0.1036\n",
      "51/463, train_loss: 0.1555\n",
      "52/463, train_loss: 0.3989\n",
      "53/463, train_loss: 0.5840\n",
      "54/463, train_loss: 0.1300\n",
      "55/463, train_loss: 0.2075\n",
      "56/463, train_loss: 0.2251\n",
      "57/463, train_loss: 0.1223\n",
      "58/463, train_loss: 0.3218\n",
      "59/463, train_loss: 0.2141\n",
      "60/463, train_loss: 0.3145\n",
      "61/463, train_loss: 0.1804\n",
      "62/463, train_loss: 0.2000\n",
      "63/463, train_loss: 0.1311\n",
      "64/463, train_loss: 0.7607\n",
      "65/463, train_loss: 0.2559\n",
      "66/463, train_loss: 0.0815\n",
      "67/463, train_loss: 0.1487\n",
      "68/463, train_loss: 0.3284\n",
      "69/463, train_loss: 0.1414\n",
      "70/463, train_loss: 0.1898\n",
      "71/463, train_loss: 0.1487\n",
      "72/463, train_loss: 0.1544\n",
      "73/463, train_loss: 0.2791\n",
      "74/463, train_loss: 0.8267\n",
      "75/463, train_loss: 0.1433\n",
      "76/463, train_loss: 0.1193\n",
      "77/463, train_loss: 0.1141\n",
      "78/463, train_loss: 0.1929\n",
      "79/463, train_loss: 0.4622\n",
      "80/463, train_loss: 0.1978\n",
      "81/463, train_loss: 0.1998\n",
      "82/463, train_loss: 0.2891\n",
      "83/463, train_loss: 0.1804\n",
      "84/463, train_loss: 0.2473\n",
      "85/463, train_loss: 0.1760\n",
      "86/463, train_loss: 0.1719\n",
      "87/463, train_loss: 0.1490\n",
      "88/463, train_loss: 0.3589\n",
      "89/463, train_loss: 0.2153\n",
      "90/463, train_loss: 0.0212\n",
      "91/463, train_loss: 0.4153\n",
      "92/463, train_loss: 0.1443\n",
      "93/463, train_loss: 0.1233\n",
      "94/463, train_loss: 0.1279\n",
      "95/463, train_loss: 0.5400\n",
      "96/463, train_loss: 0.2263\n",
      "97/463, train_loss: 0.1459\n",
      "98/463, train_loss: 0.1722\n",
      "99/463, train_loss: 0.2411\n",
      "100/463, train_loss: 0.3398\n",
      "101/463, train_loss: 0.3362\n",
      "102/463, train_loss: 0.0884\n",
      "103/463, train_loss: 0.8008\n",
      "104/463, train_loss: 0.1924\n",
      "105/463, train_loss: 0.5229\n",
      "106/463, train_loss: 0.6001\n",
      "107/463, train_loss: 0.1178\n",
      "108/463, train_loss: 0.2729\n",
      "109/463, train_loss: 0.1472\n",
      "110/463, train_loss: 0.3857\n",
      "111/463, train_loss: 0.2219\n",
      "112/463, train_loss: 0.1342\n",
      "113/463, train_loss: 0.2009\n",
      "114/463, train_loss: 0.2206\n",
      "115/463, train_loss: 0.1420\n",
      "116/463, train_loss: 0.1390\n",
      "117/463, train_loss: 0.6685\n",
      "118/463, train_loss: 0.1772\n",
      "119/463, train_loss: 0.2288\n",
      "120/463, train_loss: 0.1149\n",
      "121/463, train_loss: 0.1934\n",
      "122/463, train_loss: 0.3022\n",
      "123/463, train_loss: 0.1475\n",
      "124/463, train_loss: 0.5146\n",
      "125/463, train_loss: 0.1289\n",
      "126/463, train_loss: 0.2622\n",
      "127/463, train_loss: 0.3232\n",
      "128/463, train_loss: 0.4531\n",
      "129/463, train_loss: 0.2729\n",
      "130/463, train_loss: 0.3145\n",
      "131/463, train_loss: 0.2534\n",
      "132/463, train_loss: 0.2517\n",
      "133/463, train_loss: 0.1915\n",
      "134/463, train_loss: 0.2122\n",
      "135/463, train_loss: 0.1467\n",
      "136/463, train_loss: 0.0947\n",
      "137/463, train_loss: 0.4353\n",
      "138/463, train_loss: 0.1202\n",
      "139/463, train_loss: 0.2244\n",
      "140/463, train_loss: 0.2444\n",
      "141/463, train_loss: 0.1158\n",
      "142/463, train_loss: 0.1947\n",
      "143/463, train_loss: 0.4827\n",
      "144/463, train_loss: 0.1904\n",
      "145/463, train_loss: 0.1470\n",
      "146/463, train_loss: 0.0878\n",
      "147/463, train_loss: 0.2234\n",
      "148/463, train_loss: 0.3711\n",
      "149/463, train_loss: 0.1008\n",
      "150/463, train_loss: 0.4592\n",
      "151/463, train_loss: 0.2822\n",
      "152/463, train_loss: 0.2646\n",
      "153/463, train_loss: 0.1843\n",
      "154/463, train_loss: 0.1338\n",
      "155/463, train_loss: 0.3491\n",
      "156/463, train_loss: 0.1090\n",
      "157/463, train_loss: 0.0941\n",
      "158/463, train_loss: 0.5371\n",
      "159/463, train_loss: 0.2039\n",
      "160/463, train_loss: 0.6323\n",
      "161/463, train_loss: 0.5469\n",
      "162/463, train_loss: 0.2664\n",
      "163/463, train_loss: 0.1475\n",
      "164/463, train_loss: 0.3020\n",
      "165/463, train_loss: 0.1379\n",
      "166/463, train_loss: 0.2057\n",
      "167/463, train_loss: 0.1941\n",
      "168/463, train_loss: 0.1587\n",
      "169/463, train_loss: 0.1631\n",
      "170/463, train_loss: 0.2659\n",
      "171/463, train_loss: 0.0673\n",
      "172/463, train_loss: 0.2075\n",
      "173/463, train_loss: 0.1736\n",
      "174/463, train_loss: 0.0804\n",
      "175/463, train_loss: 0.1514\n",
      "176/463, train_loss: 0.2463\n",
      "177/463, train_loss: 0.6260\n",
      "178/463, train_loss: 0.3281\n",
      "179/463, train_loss: 0.2129\n",
      "180/463, train_loss: 0.4585\n",
      "181/463, train_loss: 0.1254\n",
      "182/463, train_loss: 0.3374\n",
      "183/463, train_loss: 0.4526\n",
      "184/463, train_loss: 0.2456\n",
      "185/463, train_loss: 0.1576\n",
      "186/463, train_loss: 0.2524\n",
      "187/463, train_loss: 0.1545\n",
      "188/463, train_loss: 0.1759\n",
      "189/463, train_loss: 0.1423\n",
      "190/463, train_loss: 0.2539\n",
      "191/463, train_loss: 0.1599\n",
      "192/463, train_loss: 0.1262\n",
      "193/463, train_loss: 0.2152\n",
      "194/463, train_loss: 0.1437\n",
      "195/463, train_loss: 0.1140\n",
      "196/463, train_loss: 0.2427\n",
      "197/463, train_loss: 0.2206\n",
      "198/463, train_loss: 0.0669\n",
      "199/463, train_loss: 0.1652\n",
      "200/463, train_loss: 0.2041\n",
      "201/463, train_loss: 0.2502\n",
      "202/463, train_loss: 0.4214\n",
      "203/463, train_loss: 0.2430\n",
      "204/463, train_loss: 0.4294\n",
      "205/463, train_loss: 0.1180\n",
      "206/463, train_loss: 0.1100\n",
      "207/463, train_loss: 0.2452\n",
      "208/463, train_loss: 0.2954\n",
      "209/463, train_loss: 0.2893\n",
      "210/463, train_loss: 0.1132\n",
      "211/463, train_loss: 0.1731\n",
      "212/463, train_loss: 0.4661\n",
      "213/463, train_loss: 0.1628\n",
      "214/463, train_loss: 0.2053\n",
      "215/463, train_loss: 0.5117\n",
      "216/463, train_loss: 0.0979\n",
      "217/463, train_loss: 0.1936\n",
      "218/463, train_loss: 0.3647\n",
      "219/463, train_loss: 0.2017\n",
      "220/463, train_loss: 0.4348\n",
      "221/463, train_loss: 0.5327\n",
      "222/463, train_loss: 0.3550\n",
      "223/463, train_loss: 0.3352\n",
      "224/463, train_loss: 0.3857\n",
      "225/463, train_loss: 0.6338\n",
      "226/463, train_loss: 0.0925\n",
      "227/463, train_loss: 0.1315\n",
      "228/463, train_loss: 0.1305\n",
      "229/463, train_loss: 0.2969\n",
      "230/463, train_loss: 0.1665\n",
      "231/463, train_loss: 0.5361\n",
      "232/463, train_loss: 0.2988\n",
      "233/463, train_loss: 0.3230\n",
      "234/463, train_loss: 0.1697\n",
      "235/463, train_loss: 0.3582\n",
      "236/463, train_loss: 0.1282\n",
      "237/463, train_loss: 0.2150\n",
      "238/463, train_loss: 0.3450\n",
      "239/463, train_loss: 0.2395\n",
      "240/463, train_loss: 0.1260\n",
      "241/463, train_loss: 0.3452\n",
      "242/463, train_loss: 0.2690\n",
      "243/463, train_loss: 0.3677\n",
      "244/463, train_loss: 0.1229\n",
      "245/463, train_loss: 0.1632\n",
      "246/463, train_loss: 0.0787\n",
      "247/463, train_loss: 0.6011\n",
      "248/463, train_loss: 0.1265\n",
      "249/463, train_loss: 0.1436\n",
      "250/463, train_loss: 0.2073\n",
      "251/463, train_loss: 0.1909\n",
      "252/463, train_loss: 0.1641\n",
      "253/463, train_loss: 0.1509\n",
      "254/463, train_loss: 0.1250\n",
      "255/463, train_loss: 0.2810\n",
      "256/463, train_loss: 0.1584\n",
      "257/463, train_loss: 0.1771\n",
      "258/463, train_loss: 0.1077\n",
      "259/463, train_loss: 0.1893\n",
      "260/463, train_loss: 0.2007\n",
      "261/463, train_loss: 0.3889\n",
      "262/463, train_loss: 0.0645\n",
      "263/463, train_loss: 0.2366\n",
      "264/463, train_loss: 0.0834\n",
      "265/463, train_loss: 0.3457\n",
      "266/463, train_loss: 0.8496\n",
      "267/463, train_loss: 0.1707\n",
      "268/463, train_loss: 0.1648\n",
      "269/463, train_loss: 0.7026\n",
      "270/463, train_loss: 0.1006\n",
      "271/463, train_loss: 0.1322\n",
      "272/463, train_loss: 0.1880\n",
      "273/463, train_loss: 0.3472\n",
      "274/463, train_loss: 0.0800\n",
      "275/463, train_loss: 0.3923\n",
      "276/463, train_loss: 0.2106\n",
      "277/463, train_loss: 0.1246\n",
      "278/463, train_loss: 0.2749\n",
      "279/463, train_loss: 0.4844\n",
      "280/463, train_loss: 0.5117\n",
      "281/463, train_loss: 0.1246\n",
      "282/463, train_loss: 0.1315\n",
      "283/463, train_loss: 0.8506\n",
      "284/463, train_loss: 0.2399\n",
      "285/463, train_loss: 0.5913\n",
      "286/463, train_loss: 0.4573\n",
      "287/463, train_loss: 0.4658\n",
      "288/463, train_loss: 0.3689\n",
      "289/463, train_loss: 0.1075\n",
      "290/463, train_loss: 0.1245\n",
      "291/463, train_loss: 0.1396\n",
      "292/463, train_loss: 0.1926\n",
      "293/463, train_loss: 0.7197\n",
      "294/463, train_loss: 0.1794\n",
      "295/463, train_loss: 0.0420\n",
      "296/463, train_loss: 0.1378\n",
      "297/463, train_loss: 0.1283\n",
      "298/463, train_loss: 0.2458\n",
      "299/463, train_loss: 0.1702\n",
      "300/463, train_loss: 0.1665\n",
      "301/463, train_loss: 0.2229\n",
      "302/463, train_loss: 0.1575\n",
      "303/463, train_loss: 0.1112\n",
      "304/463, train_loss: 0.4871\n",
      "305/463, train_loss: 0.7832\n",
      "306/463, train_loss: 0.0641\n",
      "307/463, train_loss: 0.1521\n",
      "308/463, train_loss: 0.1470\n",
      "309/463, train_loss: 0.5923\n",
      "310/463, train_loss: 0.1488\n",
      "311/463, train_loss: 0.1495\n",
      "312/463, train_loss: 0.2168\n",
      "313/463, train_loss: 0.2074\n",
      "314/463, train_loss: 0.0956\n",
      "315/463, train_loss: 0.2283\n",
      "316/463, train_loss: 0.2578\n",
      "317/463, train_loss: 0.1757\n",
      "318/463, train_loss: 0.1282\n",
      "319/463, train_loss: 0.2610\n",
      "320/463, train_loss: 0.7925\n",
      "321/463, train_loss: 0.1327\n",
      "322/463, train_loss: 0.2234\n",
      "323/463, train_loss: 0.2358\n",
      "324/463, train_loss: 0.4058\n",
      "325/463, train_loss: 0.2208\n",
      "326/463, train_loss: 0.1279\n",
      "327/463, train_loss: 0.2795\n",
      "328/463, train_loss: 0.1121\n",
      "329/463, train_loss: 0.3997\n",
      "330/463, train_loss: 0.1920\n",
      "331/463, train_loss: 0.2725\n",
      "332/463, train_loss: 0.1589\n",
      "333/463, train_loss: 0.1223\n",
      "334/463, train_loss: 0.2175\n",
      "335/463, train_loss: 0.2842\n",
      "336/463, train_loss: 0.3872\n",
      "337/463, train_loss: 0.1655\n",
      "338/463, train_loss: 0.2803\n",
      "339/463, train_loss: 0.0959\n",
      "340/463, train_loss: 0.4524\n",
      "341/463, train_loss: 0.1965\n",
      "342/463, train_loss: 0.2812\n",
      "343/463, train_loss: 0.2683\n",
      "344/463, train_loss: 0.1542\n",
      "345/463, train_loss: 0.1181\n",
      "346/463, train_loss: 0.2188\n",
      "347/463, train_loss: 0.4902\n",
      "348/463, train_loss: 0.4346\n",
      "349/463, train_loss: 0.1797\n",
      "350/463, train_loss: 0.0889\n",
      "351/463, train_loss: 0.2333\n",
      "352/463, train_loss: 0.7969\n",
      "353/463, train_loss: 0.1014\n",
      "354/463, train_loss: 0.1757\n",
      "355/463, train_loss: 0.3184\n",
      "356/463, train_loss: 0.4036\n",
      "357/463, train_loss: 0.1346\n",
      "358/463, train_loss: 0.1948\n",
      "359/463, train_loss: 0.4741\n",
      "360/463, train_loss: 0.1743\n",
      "361/463, train_loss: 0.1190\n",
      "362/463, train_loss: 0.5117\n",
      "363/463, train_loss: 0.2383\n",
      "364/463, train_loss: 0.2483\n",
      "365/463, train_loss: 0.1497\n",
      "366/463, train_loss: 0.1776\n",
      "367/463, train_loss: 0.4485\n",
      "368/463, train_loss: 0.1357\n",
      "369/463, train_loss: 0.2363\n",
      "370/463, train_loss: 0.0608\n",
      "371/463, train_loss: 0.1350\n",
      "372/463, train_loss: 0.6455\n",
      "373/463, train_loss: 0.2017\n",
      "374/463, train_loss: 0.4197\n",
      "375/463, train_loss: 0.0906\n",
      "376/463, train_loss: 0.2152\n",
      "377/463, train_loss: 0.3071\n",
      "378/463, train_loss: 0.0836\n",
      "379/463, train_loss: 0.2268\n",
      "380/463, train_loss: 0.1328\n",
      "381/463, train_loss: 0.6895\n",
      "382/463, train_loss: 0.1208\n",
      "383/463, train_loss: 0.3755\n",
      "384/463, train_loss: 0.1633\n",
      "385/463, train_loss: 0.2598\n",
      "386/463, train_loss: 0.1956\n",
      "387/463, train_loss: 0.1221\n",
      "388/463, train_loss: 0.2366\n",
      "389/463, train_loss: 0.2751\n",
      "390/463, train_loss: 0.2974\n",
      "391/463, train_loss: 0.7432\n",
      "392/463, train_loss: 0.1543\n",
      "393/463, train_loss: 0.2054\n",
      "394/463, train_loss: 0.2074\n",
      "395/463, train_loss: 0.5137\n",
      "396/463, train_loss: 0.1960\n",
      "397/463, train_loss: 0.1328\n",
      "398/463, train_loss: 0.2261\n",
      "399/463, train_loss: 0.7144\n",
      "400/463, train_loss: 0.1943\n",
      "401/463, train_loss: 0.4839\n",
      "402/463, train_loss: 0.3274\n",
      "403/463, train_loss: 0.1826\n",
      "404/463, train_loss: 0.1967\n",
      "405/463, train_loss: 0.2441\n",
      "406/463, train_loss: 0.1075\n",
      "407/463, train_loss: 0.1809\n",
      "408/463, train_loss: 0.6216\n",
      "409/463, train_loss: 0.6133\n",
      "410/463, train_loss: 0.1279\n",
      "411/463, train_loss: 0.4436\n",
      "412/463, train_loss: 0.1506\n",
      "413/463, train_loss: 0.1331\n",
      "414/463, train_loss: 0.3948\n",
      "415/463, train_loss: 0.1809\n",
      "416/463, train_loss: 0.1946\n",
      "417/463, train_loss: 0.1550\n",
      "418/463, train_loss: 0.0735\n",
      "419/463, train_loss: 0.4067\n",
      "420/463, train_loss: 0.1392\n",
      "421/463, train_loss: 0.2456\n",
      "422/463, train_loss: 0.1726\n",
      "423/463, train_loss: 0.2837\n",
      "424/463, train_loss: 0.0792\n",
      "425/463, train_loss: 0.3647\n",
      "426/463, train_loss: 0.3059\n",
      "427/463, train_loss: 0.1281\n",
      "428/463, train_loss: 0.2115\n",
      "429/463, train_loss: 0.4121\n",
      "430/463, train_loss: 0.3596\n",
      "431/463, train_loss: 0.2450\n",
      "432/463, train_loss: 0.5498\n",
      "433/463, train_loss: 0.1816\n",
      "434/463, train_loss: 0.1255\n",
      "435/463, train_loss: 0.1411\n",
      "436/463, train_loss: 0.2808\n",
      "437/463, train_loss: 0.7798\n",
      "438/463, train_loss: 0.1571\n",
      "439/463, train_loss: 0.2203\n",
      "440/463, train_loss: 0.2111\n",
      "441/463, train_loss: 0.0793\n",
      "442/463, train_loss: 0.3120\n",
      "443/463, train_loss: 0.2610\n",
      "444/463, train_loss: 0.8687\n",
      "445/463, train_loss: 0.7554\n",
      "446/463, train_loss: 0.1554\n",
      "447/463, train_loss: 0.2251\n",
      "448/463, train_loss: 0.2844\n",
      "449/463, train_loss: 0.2168\n",
      "450/463, train_loss: 0.4792\n",
      "451/463, train_loss: 0.1694\n",
      "452/463, train_loss: 0.2468\n",
      "453/463, train_loss: 0.1954\n",
      "454/463, train_loss: 0.3704\n",
      "455/463, train_loss: 0.4824\n",
      "456/463, train_loss: 0.2111\n",
      "457/463, train_loss: 0.1663\n",
      "458/463, train_loss: 0.2261\n",
      "459/463, train_loss: 0.2305\n",
      "460/463, train_loss: 0.2008\n",
      "461/463, train_loss: 0.1577\n",
      "462/463, train_loss: 0.2756\n",
      "463/463, train_loss: 0.1423\n",
      "epoch 10 average loss: 0.2595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 03:08:25 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 03:08:27 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 03:08:30 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mixed/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630.nii.gz\n",
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.425188533524287, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.425188533524287, 'AP_IoU_0.10_MaxDet_100': 0.48821098820334025, 'nodule_AP_IoU_0.10_MaxDet_100': 0.48821098820334025, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8547008633613586, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8547008633613586, 'AR_IoU_0.10_MaxDet_100': 0.9487179517745972, 'nodule_AR_IoU_0.10_MaxDet_100': 0.9487179517745972}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 03:24:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 03:24:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 03:24:55 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 10 current metric: 0.6792 best metric: 0.6792 at epoch 10\n",
      "----------\n",
      "epoch 11/100\n",
      "1/463, train_loss: 0.1604\n",
      "2/463, train_loss: 0.2529\n",
      "3/463, train_loss: 0.7695\n",
      "4/463, train_loss: 0.1180\n",
      "5/463, train_loss: 0.3320\n",
      "6/463, train_loss: 0.2209\n",
      "7/463, train_loss: 0.4675\n",
      "8/463, train_loss: 0.3359\n",
      "9/463, train_loss: 0.2542\n",
      "10/463, train_loss: 0.1492\n",
      "11/463, train_loss: 0.2642\n",
      "12/463, train_loss: 0.2402\n",
      "13/463, train_loss: 0.1423\n",
      "14/463, train_loss: 0.3516\n",
      "15/463, train_loss: 0.0863\n",
      "16/463, train_loss: 0.3540\n",
      "17/463, train_loss: 0.1093\n",
      "18/463, train_loss: 0.1057\n",
      "19/463, train_loss: 0.1603\n",
      "20/463, train_loss: 0.3843\n",
      "21/463, train_loss: 0.2759\n",
      "22/463, train_loss: 0.2634\n",
      "23/463, train_loss: 0.2595\n",
      "24/463, train_loss: 0.1034\n",
      "25/463, train_loss: 0.2373\n",
      "26/463, train_loss: 0.1198\n",
      "27/463, train_loss: 0.1985\n",
      "28/463, train_loss: 0.3447\n",
      "29/463, train_loss: 0.3247\n",
      "30/463, train_loss: 0.1562\n",
      "31/463, train_loss: 0.1985\n",
      "32/463, train_loss: 0.1399\n",
      "33/463, train_loss: 0.4517\n",
      "34/463, train_loss: 0.2100\n",
      "35/463, train_loss: 0.6221\n",
      "36/463, train_loss: 0.1918\n",
      "37/463, train_loss: 0.4587\n",
      "38/463, train_loss: 0.1912\n",
      "39/463, train_loss: 0.1130\n",
      "40/463, train_loss: 0.2415\n",
      "41/463, train_loss: 0.1190\n",
      "42/463, train_loss: 0.2314\n",
      "43/463, train_loss: 0.2944\n",
      "44/463, train_loss: 0.3525\n",
      "45/463, train_loss: 0.1520\n",
      "46/463, train_loss: 0.7056\n",
      "47/463, train_loss: 0.2471\n",
      "48/463, train_loss: 0.1826\n",
      "49/463, train_loss: 0.1292\n",
      "50/463, train_loss: 0.2013\n",
      "51/463, train_loss: 0.0811\n",
      "52/463, train_loss: 0.2700\n",
      "53/463, train_loss: 0.2759\n",
      "54/463, train_loss: 0.1509\n",
      "55/463, train_loss: 0.1494\n",
      "56/463, train_loss: 0.0378\n",
      "57/463, train_loss: 0.2974\n",
      "58/463, train_loss: 0.2130\n",
      "59/463, train_loss: 0.1628\n",
      "60/463, train_loss: 0.2720\n",
      "61/463, train_loss: 0.1139\n",
      "62/463, train_loss: 0.3574\n",
      "63/463, train_loss: 0.5718\n",
      "64/463, train_loss: 0.2603\n",
      "65/463, train_loss: 0.2725\n",
      "66/463, train_loss: 0.1595\n",
      "67/463, train_loss: 0.1290\n",
      "68/463, train_loss: 0.2178\n",
      "69/463, train_loss: 0.1575\n",
      "70/463, train_loss: 0.2271\n",
      "71/463, train_loss: 0.2061\n",
      "72/463, train_loss: 0.1010\n",
      "73/463, train_loss: 0.3110\n",
      "74/463, train_loss: 0.2134\n",
      "75/463, train_loss: 0.1104\n",
      "76/463, train_loss: 0.1716\n",
      "77/463, train_loss: 0.5459\n",
      "78/463, train_loss: 0.1309\n",
      "79/463, train_loss: 0.2246\n",
      "80/463, train_loss: 0.0900\n",
      "81/463, train_loss: 0.3320\n",
      "82/463, train_loss: 0.2412\n",
      "83/463, train_loss: 0.2219\n",
      "84/463, train_loss: 0.1710\n",
      "85/463, train_loss: 0.2311\n",
      "86/463, train_loss: 0.1147\n",
      "87/463, train_loss: 0.2046\n",
      "88/463, train_loss: 0.3855\n",
      "89/463, train_loss: 0.1396\n",
      "90/463, train_loss: 0.2578\n",
      "91/463, train_loss: 0.1801\n",
      "92/463, train_loss: 0.1205\n",
      "93/463, train_loss: 0.1611\n",
      "94/463, train_loss: 0.1392\n",
      "95/463, train_loss: 0.1865\n",
      "96/463, train_loss: 0.3660\n",
      "97/463, train_loss: 0.0600\n",
      "98/463, train_loss: 0.1904\n",
      "99/463, train_loss: 0.4490\n",
      "100/463, train_loss: 0.2090\n",
      "101/463, train_loss: 0.9058\n",
      "102/463, train_loss: 0.7461\n",
      "103/463, train_loss: 0.2644\n",
      "104/463, train_loss: 0.1345\n",
      "105/463, train_loss: 0.1571\n",
      "106/463, train_loss: 0.1287\n",
      "107/463, train_loss: 0.5059\n",
      "108/463, train_loss: 0.1251\n",
      "109/463, train_loss: 0.3154\n",
      "110/463, train_loss: 0.2256\n",
      "111/463, train_loss: 0.1387\n",
      "112/463, train_loss: 0.0983\n",
      "113/463, train_loss: 0.1763\n",
      "114/463, train_loss: 0.3574\n",
      "115/463, train_loss: 0.3884\n",
      "116/463, train_loss: 0.6104\n",
      "117/463, train_loss: 0.1228\n",
      "118/463, train_loss: 0.1512\n",
      "119/463, train_loss: 0.1602\n",
      "120/463, train_loss: 0.1987\n",
      "121/463, train_loss: 0.2278\n",
      "122/463, train_loss: 0.1562\n",
      "123/463, train_loss: 0.3877\n",
      "124/463, train_loss: 0.1201\n",
      "125/463, train_loss: 0.1367\n",
      "126/463, train_loss: 0.0682\n",
      "127/463, train_loss: 0.1940\n",
      "128/463, train_loss: 0.1509\n",
      "129/463, train_loss: 0.1255\n",
      "130/463, train_loss: 0.1356\n",
      "131/463, train_loss: 0.3115\n",
      "132/463, train_loss: 0.1741\n",
      "133/463, train_loss: 0.1377\n",
      "134/463, train_loss: 0.1069\n",
      "135/463, train_loss: 0.6011\n",
      "136/463, train_loss: 0.1470\n",
      "137/463, train_loss: 0.4751\n",
      "138/463, train_loss: 0.2357\n",
      "139/463, train_loss: 0.2231\n",
      "140/463, train_loss: 0.1487\n",
      "141/463, train_loss: 0.2927\n",
      "142/463, train_loss: 0.3735\n",
      "143/463, train_loss: 0.7520\n",
      "144/463, train_loss: 0.2045\n",
      "145/463, train_loss: 0.3540\n",
      "146/463, train_loss: 0.2443\n",
      "147/463, train_loss: 0.2048\n",
      "148/463, train_loss: 0.1444\n",
      "149/463, train_loss: 0.3901\n",
      "150/463, train_loss: 0.4299\n",
      "151/463, train_loss: 0.2184\n",
      "152/463, train_loss: 0.1511\n",
      "153/463, train_loss: 0.2112\n",
      "154/463, train_loss: 0.0990\n",
      "155/463, train_loss: 0.3538\n",
      "156/463, train_loss: 0.3530\n",
      "157/463, train_loss: 0.2192\n",
      "158/463, train_loss: 0.0941\n",
      "159/463, train_loss: 0.1338\n",
      "160/463, train_loss: 0.4224\n",
      "161/463, train_loss: 0.3804\n",
      "162/463, train_loss: 0.2219\n",
      "163/463, train_loss: 0.3162\n",
      "164/463, train_loss: 0.1158\n",
      "165/463, train_loss: 0.2114\n",
      "166/463, train_loss: 0.1743\n",
      "167/463, train_loss: 0.3530\n",
      "168/463, train_loss: 0.4590\n",
      "169/463, train_loss: 0.3811\n",
      "170/463, train_loss: 0.1362\n",
      "171/463, train_loss: 0.1854\n",
      "172/463, train_loss: 0.1443\n",
      "173/463, train_loss: 0.2966\n",
      "174/463, train_loss: 0.0970\n",
      "175/463, train_loss: 0.5830\n",
      "176/463, train_loss: 0.2222\n",
      "177/463, train_loss: 0.2490\n",
      "178/463, train_loss: 0.7646\n",
      "179/463, train_loss: 0.1331\n",
      "180/463, train_loss: 0.1531\n",
      "181/463, train_loss: 0.3389\n",
      "182/463, train_loss: 0.5830\n",
      "183/463, train_loss: 0.0638\n",
      "184/463, train_loss: 0.1724\n",
      "185/463, train_loss: 0.4363\n",
      "186/463, train_loss: 0.2429\n",
      "187/463, train_loss: 0.6157\n",
      "188/463, train_loss: 0.3481\n",
      "189/463, train_loss: 0.4043\n",
      "190/463, train_loss: 0.1958\n",
      "191/463, train_loss: 0.1921\n",
      "192/463, train_loss: 0.1799\n",
      "193/463, train_loss: 0.2866\n",
      "194/463, train_loss: 0.2771\n",
      "195/463, train_loss: 0.4441\n",
      "196/463, train_loss: 0.5142\n",
      "197/463, train_loss: 0.1890\n",
      "198/463, train_loss: 0.3237\n",
      "199/463, train_loss: 0.1697\n",
      "200/463, train_loss: 0.1144\n",
      "201/463, train_loss: 0.1757\n",
      "202/463, train_loss: 0.2189\n",
      "203/463, train_loss: 0.1558\n",
      "204/463, train_loss: 0.2954\n",
      "205/463, train_loss: 0.1230\n",
      "206/463, train_loss: 0.1361\n",
      "207/463, train_loss: 0.1890\n",
      "208/463, train_loss: 0.0959\n",
      "209/463, train_loss: 0.3945\n",
      "210/463, train_loss: 0.1765\n",
      "211/463, train_loss: 0.2207\n",
      "212/463, train_loss: 0.0903\n",
      "213/463, train_loss: 0.2649\n",
      "214/463, train_loss: 0.1892\n",
      "215/463, train_loss: 0.4644\n",
      "216/463, train_loss: 0.0903\n",
      "217/463, train_loss: 0.2083\n",
      "218/463, train_loss: 0.1272\n",
      "219/463, train_loss: 0.0390\n",
      "220/463, train_loss: 0.4258\n",
      "221/463, train_loss: 0.0836\n",
      "222/463, train_loss: 0.1385\n",
      "223/463, train_loss: 0.3320\n",
      "224/463, train_loss: 0.3076\n",
      "225/463, train_loss: 0.1187\n",
      "226/463, train_loss: 0.1118\n",
      "227/463, train_loss: 0.3889\n",
      "228/463, train_loss: 0.1326\n",
      "229/463, train_loss: 0.2207\n",
      "230/463, train_loss: 0.2524\n",
      "231/463, train_loss: 0.1483\n",
      "232/463, train_loss: 0.3135\n",
      "233/463, train_loss: 0.3789\n",
      "234/463, train_loss: 0.1753\n",
      "235/463, train_loss: 0.1779\n",
      "236/463, train_loss: 0.0718\n",
      "237/463, train_loss: 0.1512\n",
      "238/463, train_loss: 0.4802\n",
      "239/463, train_loss: 0.1771\n",
      "240/463, train_loss: 0.4673\n",
      "241/463, train_loss: 0.1763\n",
      "242/463, train_loss: 0.3455\n",
      "243/463, train_loss: 0.1692\n",
      "244/463, train_loss: 0.1316\n",
      "245/463, train_loss: 0.3975\n",
      "246/463, train_loss: 0.2087\n",
      "247/463, train_loss: 0.2449\n",
      "248/463, train_loss: 0.2581\n",
      "249/463, train_loss: 0.2385\n",
      "250/463, train_loss: 0.2993\n",
      "251/463, train_loss: 0.1119\n",
      "252/463, train_loss: 0.1458\n",
      "253/463, train_loss: 0.1182\n",
      "254/463, train_loss: 0.1389\n",
      "255/463, train_loss: 0.1661\n",
      "256/463, train_loss: 0.1445\n",
      "257/463, train_loss: 0.1154\n",
      "258/463, train_loss: 0.1816\n",
      "259/463, train_loss: 0.1600\n",
      "260/463, train_loss: 0.4460\n",
      "261/463, train_loss: 0.2250\n",
      "262/463, train_loss: 0.4639\n",
      "263/463, train_loss: 0.1621\n",
      "264/463, train_loss: 0.1575\n",
      "265/463, train_loss: 0.2595\n",
      "266/463, train_loss: 0.5127\n",
      "267/463, train_loss: 0.8145\n",
      "268/463, train_loss: 0.2401\n",
      "269/463, train_loss: 0.3049\n",
      "270/463, train_loss: 0.4424\n",
      "271/463, train_loss: 0.4836\n",
      "272/463, train_loss: 0.0617\n",
      "273/463, train_loss: 0.3599\n",
      "274/463, train_loss: 0.1260\n",
      "275/463, train_loss: 0.1580\n",
      "276/463, train_loss: 0.2764\n",
      "277/463, train_loss: 0.0511\n",
      "278/463, train_loss: 0.1318\n",
      "279/463, train_loss: 0.0872\n",
      "280/463, train_loss: 0.1774\n",
      "281/463, train_loss: 0.3171\n",
      "282/463, train_loss: 0.1520\n",
      "283/463, train_loss: 0.3242\n",
      "284/463, train_loss: 0.0865\n",
      "285/463, train_loss: 0.2155\n",
      "286/463, train_loss: 0.1846\n",
      "287/463, train_loss: 0.2275\n",
      "288/463, train_loss: 0.1036\n",
      "289/463, train_loss: 0.2534\n",
      "290/463, train_loss: 0.1833\n",
      "291/463, train_loss: 0.1188\n",
      "292/463, train_loss: 0.3359\n",
      "293/463, train_loss: 0.1748\n",
      "294/463, train_loss: 0.1453\n",
      "295/463, train_loss: 0.0575\n",
      "296/463, train_loss: 0.1328\n",
      "297/463, train_loss: 0.7930\n",
      "298/463, train_loss: 0.2142\n",
      "299/463, train_loss: 0.3210\n",
      "300/463, train_loss: 0.3271\n",
      "301/463, train_loss: 0.4639\n",
      "302/463, train_loss: 0.1450\n",
      "303/463, train_loss: 0.1112\n",
      "304/463, train_loss: 0.5244\n",
      "305/463, train_loss: 0.1753\n",
      "306/463, train_loss: 0.8076\n",
      "307/463, train_loss: 0.2532\n",
      "308/463, train_loss: 0.1212\n",
      "309/463, train_loss: 0.3809\n",
      "310/463, train_loss: 0.3145\n",
      "311/463, train_loss: 0.8452\n",
      "312/463, train_loss: 0.1946\n",
      "313/463, train_loss: 0.1257\n",
      "314/463, train_loss: 0.2341\n",
      "315/463, train_loss: 0.2759\n",
      "316/463, train_loss: 0.3257\n",
      "317/463, train_loss: 0.5005\n",
      "318/463, train_loss: 0.1194\n",
      "319/463, train_loss: 0.3357\n",
      "320/463, train_loss: 0.1462\n",
      "321/463, train_loss: 0.4873\n",
      "322/463, train_loss: 0.3582\n",
      "323/463, train_loss: 0.1635\n",
      "324/463, train_loss: 0.1382\n",
      "325/463, train_loss: 0.2708\n",
      "326/463, train_loss: 0.1185\n",
      "327/463, train_loss: 0.2617\n",
      "328/463, train_loss: 0.2079\n",
      "329/463, train_loss: 0.1541\n",
      "330/463, train_loss: 0.1194\n",
      "331/463, train_loss: 0.1074\n",
      "332/463, train_loss: 0.4236\n",
      "333/463, train_loss: 0.4580\n",
      "334/463, train_loss: 0.3223\n",
      "335/463, train_loss: 0.1311\n",
      "336/463, train_loss: 0.2991\n",
      "337/463, train_loss: 0.1453\n",
      "338/463, train_loss: 0.6558\n",
      "339/463, train_loss: 0.1624\n",
      "340/463, train_loss: 0.1190\n",
      "341/463, train_loss: 0.0767\n",
      "342/463, train_loss: 0.4121\n",
      "343/463, train_loss: 0.1809\n",
      "344/463, train_loss: 0.1564\n",
      "345/463, train_loss: 0.1000\n",
      "346/463, train_loss: 0.4224\n",
      "347/463, train_loss: 0.2415\n",
      "348/463, train_loss: 0.2108\n",
      "349/463, train_loss: 0.1095\n",
      "350/463, train_loss: 0.0885\n",
      "351/463, train_loss: 0.2275\n",
      "352/463, train_loss: 0.1553\n",
      "353/463, train_loss: 0.2280\n",
      "354/463, train_loss: 0.2205\n",
      "355/463, train_loss: 0.4438\n",
      "356/463, train_loss: 0.1564\n",
      "357/463, train_loss: 0.9121\n",
      "358/463, train_loss: 1.0342\n",
      "359/463, train_loss: 0.2773\n",
      "360/463, train_loss: 0.2722\n",
      "361/463, train_loss: 0.2485\n",
      "362/463, train_loss: 0.7856\n",
      "363/463, train_loss: 0.2649\n",
      "364/463, train_loss: 0.1096\n",
      "365/463, train_loss: 0.3669\n",
      "366/463, train_loss: 0.3765\n",
      "367/463, train_loss: 0.1851\n",
      "368/463, train_loss: 0.2810\n",
      "369/463, train_loss: 0.2529\n",
      "370/463, train_loss: 0.1593\n",
      "371/463, train_loss: 0.1561\n",
      "372/463, train_loss: 0.1738\n",
      "373/463, train_loss: 0.1399\n",
      "374/463, train_loss: 0.0789\n",
      "375/463, train_loss: 0.7612\n",
      "376/463, train_loss: 0.3472\n",
      "377/463, train_loss: 0.1409\n",
      "378/463, train_loss: 0.2502\n",
      "379/463, train_loss: 0.2092\n",
      "380/463, train_loss: 0.4485\n",
      "381/463, train_loss: 0.3071\n",
      "382/463, train_loss: 0.0961\n",
      "383/463, train_loss: 0.0740\n",
      "384/463, train_loss: 0.3701\n",
      "385/463, train_loss: 0.3901\n",
      "386/463, train_loss: 0.3611\n",
      "387/463, train_loss: 0.2532\n",
      "388/463, train_loss: 0.0940\n",
      "389/463, train_loss: 0.1638\n",
      "390/463, train_loss: 0.3333\n",
      "391/463, train_loss: 0.4617\n",
      "392/463, train_loss: 0.1857\n",
      "393/463, train_loss: 0.1422\n",
      "394/463, train_loss: 0.4995\n",
      "395/463, train_loss: 0.4116\n",
      "396/463, train_loss: 0.0964\n",
      "397/463, train_loss: 0.2275\n",
      "398/463, train_loss: 0.6357\n",
      "399/463, train_loss: 0.2600\n",
      "400/463, train_loss: 0.4094\n",
      "401/463, train_loss: 0.4922\n",
      "402/463, train_loss: 0.1278\n",
      "403/463, train_loss: 0.1212\n",
      "404/463, train_loss: 0.3530\n",
      "405/463, train_loss: 0.2122\n",
      "406/463, train_loss: 0.2019\n",
      "407/463, train_loss: 0.1505\n",
      "408/463, train_loss: 0.2235\n",
      "409/463, train_loss: 0.2100\n",
      "410/463, train_loss: 0.2101\n",
      "411/463, train_loss: 0.1108\n",
      "412/463, train_loss: 0.1721\n",
      "413/463, train_loss: 0.4153\n",
      "414/463, train_loss: 0.4646\n",
      "415/463, train_loss: 0.2283\n",
      "416/463, train_loss: 0.3337\n",
      "417/463, train_loss: 0.2030\n",
      "418/463, train_loss: 0.1354\n",
      "419/463, train_loss: 0.1812\n",
      "420/463, train_loss: 0.3215\n",
      "421/463, train_loss: 0.1719\n",
      "422/463, train_loss: 0.0581\n",
      "423/463, train_loss: 0.3413\n",
      "424/463, train_loss: 0.1882\n",
      "425/463, train_loss: 0.1014\n",
      "426/463, train_loss: 0.1959\n",
      "427/463, train_loss: 0.0992\n",
      "428/463, train_loss: 0.1620\n",
      "429/463, train_loss: 0.2712\n",
      "430/463, train_loss: 0.6172\n",
      "431/463, train_loss: 0.4421\n",
      "432/463, train_loss: 0.4146\n",
      "433/463, train_loss: 0.4319\n",
      "434/463, train_loss: 0.2390\n",
      "435/463, train_loss: 0.3062\n",
      "436/463, train_loss: 0.4368\n",
      "437/463, train_loss: 0.2174\n",
      "438/463, train_loss: 0.1111\n",
      "439/463, train_loss: 0.1292\n",
      "440/463, train_loss: 0.3667\n",
      "441/463, train_loss: 0.1060\n",
      "442/463, train_loss: 0.3159\n",
      "443/463, train_loss: 0.1346\n",
      "444/463, train_loss: 0.0984\n",
      "445/463, train_loss: 0.1241\n",
      "446/463, train_loss: 0.2037\n",
      "447/463, train_loss: 0.3240\n",
      "448/463, train_loss: 0.1282\n",
      "449/463, train_loss: 0.3389\n",
      "450/463, train_loss: 0.4104\n",
      "451/463, train_loss: 0.1936\n",
      "452/463, train_loss: 0.1033\n",
      "453/463, train_loss: 0.2534\n",
      "454/463, train_loss: 0.1483\n",
      "455/463, train_loss: 0.1733\n",
      "456/463, train_loss: 0.3652\n",
      "457/463, train_loss: 0.1938\n",
      "458/463, train_loss: 0.0999\n",
      "459/463, train_loss: 0.3669\n",
      "460/463, train_loss: 0.1426\n",
      "461/463, train_loss: 0.3359\n",
      "462/463, train_loss: 0.2180\n",
      "463/463, train_loss: 0.1265\n",
      "epoch 11 average loss: 0.2557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 05:38:04 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 05:38:06 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 05:38:09 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 12/100\n",
      "1/463, train_loss: 0.1010\n",
      "2/463, train_loss: 0.0979\n",
      "3/463, train_loss: 0.3604\n",
      "4/463, train_loss: 0.2109\n",
      "5/463, train_loss: 0.1809\n",
      "6/463, train_loss: 0.1153\n",
      "7/463, train_loss: 0.2842\n",
      "8/463, train_loss: 0.1132\n",
      "9/463, train_loss: 0.1364\n",
      "10/463, train_loss: 0.1654\n",
      "11/463, train_loss: 0.0646\n",
      "12/463, train_loss: 0.4221\n",
      "13/463, train_loss: 0.2010\n",
      "14/463, train_loss: 0.0939\n",
      "15/463, train_loss: 0.2186\n",
      "16/463, train_loss: 0.0900\n",
      "17/463, train_loss: 0.2405\n",
      "18/463, train_loss: 0.1838\n",
      "19/463, train_loss: 0.4746\n",
      "20/463, train_loss: 0.3203\n",
      "21/463, train_loss: 0.2781\n",
      "22/463, train_loss: 0.8701\n",
      "23/463, train_loss: 0.2085\n",
      "24/463, train_loss: 0.1976\n",
      "25/463, train_loss: 0.1497\n",
      "26/463, train_loss: 0.2695\n",
      "27/463, train_loss: 0.3809\n",
      "28/463, train_loss: 0.1648\n",
      "29/463, train_loss: 0.5737\n",
      "30/463, train_loss: 0.5020\n",
      "31/463, train_loss: 0.2783\n",
      "32/463, train_loss: 0.1053\n",
      "33/463, train_loss: 0.1864\n",
      "34/463, train_loss: 0.0814\n",
      "35/463, train_loss: 0.1625\n",
      "36/463, train_loss: 0.6743\n",
      "37/463, train_loss: 0.2059\n",
      "38/463, train_loss: 0.1923\n",
      "39/463, train_loss: 0.0835\n",
      "40/463, train_loss: 0.1699\n",
      "41/463, train_loss: 0.0653\n",
      "42/463, train_loss: 0.2386\n",
      "43/463, train_loss: 0.1475\n",
      "44/463, train_loss: 0.3201\n",
      "45/463, train_loss: 0.1787\n",
      "46/463, train_loss: 0.2820\n",
      "47/463, train_loss: 0.2551\n",
      "48/463, train_loss: 0.1985\n",
      "49/463, train_loss: 0.5259\n",
      "50/463, train_loss: 0.5171\n",
      "51/463, train_loss: 0.9268\n",
      "52/463, train_loss: 0.1732\n",
      "53/463, train_loss: 0.3328\n",
      "54/463, train_loss: 0.1495\n",
      "55/463, train_loss: 0.3855\n",
      "56/463, train_loss: 0.3103\n",
      "57/463, train_loss: 0.1700\n",
      "58/463, train_loss: 0.1573\n",
      "59/463, train_loss: 0.1713\n",
      "60/463, train_loss: 0.1335\n",
      "61/463, train_loss: 0.0992\n",
      "62/463, train_loss: 0.2136\n",
      "63/463, train_loss: 0.2778\n",
      "64/463, train_loss: 0.3279\n",
      "65/463, train_loss: 0.1570\n",
      "66/463, train_loss: 0.2866\n",
      "67/463, train_loss: 0.0938\n",
      "68/463, train_loss: 0.2017\n",
      "69/463, train_loss: 0.1223\n",
      "70/463, train_loss: 0.1034\n",
      "71/463, train_loss: 0.1862\n",
      "72/463, train_loss: 0.0797\n",
      "73/463, train_loss: 0.1744\n",
      "74/463, train_loss: 0.6631\n",
      "75/463, train_loss: 0.6597\n",
      "76/463, train_loss: 0.1451\n",
      "77/463, train_loss: 0.4292\n",
      "78/463, train_loss: 0.4253\n",
      "79/463, train_loss: 0.3779\n",
      "80/463, train_loss: 0.1611\n",
      "81/463, train_loss: 0.2915\n",
      "82/463, train_loss: 0.1560\n",
      "83/463, train_loss: 0.1685\n",
      "84/463, train_loss: 0.1848\n",
      "85/463, train_loss: 0.0609\n",
      "86/463, train_loss: 0.0935\n",
      "87/463, train_loss: 0.1270\n",
      "88/463, train_loss: 0.4751\n",
      "89/463, train_loss: 0.3789\n",
      "90/463, train_loss: 0.1804\n",
      "91/463, train_loss: 0.3396\n",
      "92/463, train_loss: 0.1255\n",
      "93/463, train_loss: 0.0710\n",
      "94/463, train_loss: 0.2113\n",
      "95/463, train_loss: 0.1986\n",
      "96/463, train_loss: 0.1234\n",
      "97/463, train_loss: 0.1512\n",
      "98/463, train_loss: 0.4204\n",
      "99/463, train_loss: 0.2786\n",
      "100/463, train_loss: 0.7520\n",
      "101/463, train_loss: 0.1121\n",
      "102/463, train_loss: 0.1096\n",
      "103/463, train_loss: 0.1350\n",
      "104/463, train_loss: 0.2773\n",
      "105/463, train_loss: 0.3469\n",
      "106/463, train_loss: 0.2957\n",
      "107/463, train_loss: 0.4121\n",
      "108/463, train_loss: 0.2006\n",
      "109/463, train_loss: 0.2974\n",
      "110/463, train_loss: 0.2725\n",
      "111/463, train_loss: 0.2384\n",
      "112/463, train_loss: 0.1697\n",
      "113/463, train_loss: 0.1453\n",
      "114/463, train_loss: 0.2017\n",
      "115/463, train_loss: 0.1804\n",
      "116/463, train_loss: 0.2932\n",
      "117/463, train_loss: 0.0996\n",
      "118/463, train_loss: 0.2202\n",
      "119/463, train_loss: 0.2397\n",
      "120/463, train_loss: 0.1050\n",
      "121/463, train_loss: 0.1925\n",
      "122/463, train_loss: 0.1179\n",
      "123/463, train_loss: 0.1788\n",
      "124/463, train_loss: 0.3110\n",
      "125/463, train_loss: 0.1241\n",
      "126/463, train_loss: 0.2023\n",
      "127/463, train_loss: 0.3420\n",
      "128/463, train_loss: 0.0446\n",
      "129/463, train_loss: 0.1227\n",
      "130/463, train_loss: 0.1978\n",
      "131/463, train_loss: 0.1591\n",
      "132/463, train_loss: 0.4243\n",
      "133/463, train_loss: 0.4504\n",
      "134/463, train_loss: 0.1346\n",
      "135/463, train_loss: 0.1437\n",
      "136/463, train_loss: 0.2437\n",
      "137/463, train_loss: 0.1102\n",
      "138/463, train_loss: 0.3262\n",
      "139/463, train_loss: 0.1907\n",
      "140/463, train_loss: 0.1755\n",
      "141/463, train_loss: 0.2017\n",
      "142/463, train_loss: 0.5479\n",
      "143/463, train_loss: 0.2239\n",
      "144/463, train_loss: 0.7832\n",
      "145/463, train_loss: 0.2822\n",
      "146/463, train_loss: 0.3088\n",
      "147/463, train_loss: 0.2108\n",
      "148/463, train_loss: 0.1614\n",
      "149/463, train_loss: 0.3716\n",
      "150/463, train_loss: 0.1638\n",
      "151/463, train_loss: 0.0757\n",
      "152/463, train_loss: 0.0646\n",
      "153/463, train_loss: 1.0527\n",
      "154/463, train_loss: 0.1890\n",
      "155/463, train_loss: 0.1202\n",
      "156/463, train_loss: 0.1952\n",
      "157/463, train_loss: 0.1054\n",
      "158/463, train_loss: 0.5127\n",
      "159/463, train_loss: 0.1885\n",
      "160/463, train_loss: 0.1522\n",
      "161/463, train_loss: 0.1278\n",
      "162/463, train_loss: 0.1935\n",
      "163/463, train_loss: 0.7139\n",
      "164/463, train_loss: 0.1818\n",
      "165/463, train_loss: 0.4990\n",
      "166/463, train_loss: 0.1848\n",
      "167/463, train_loss: 0.1034\n",
      "168/463, train_loss: 0.2029\n",
      "169/463, train_loss: 0.2935\n",
      "170/463, train_loss: 0.1196\n",
      "171/463, train_loss: 0.1843\n",
      "172/463, train_loss: 0.1294\n",
      "173/463, train_loss: 0.2091\n",
      "174/463, train_loss: 0.1658\n",
      "175/463, train_loss: 0.2332\n",
      "176/463, train_loss: 0.1038\n",
      "177/463, train_loss: 0.2175\n",
      "178/463, train_loss: 0.3271\n",
      "179/463, train_loss: 0.5239\n",
      "180/463, train_loss: 0.3464\n",
      "181/463, train_loss: 0.1501\n",
      "182/463, train_loss: 0.2214\n",
      "183/463, train_loss: 0.2617\n",
      "184/463, train_loss: 0.6089\n",
      "185/463, train_loss: 0.2308\n",
      "186/463, train_loss: 0.3618\n",
      "187/463, train_loss: 0.1797\n",
      "188/463, train_loss: 0.1366\n",
      "189/463, train_loss: 0.1216\n",
      "190/463, train_loss: 0.1519\n",
      "191/463, train_loss: 0.1357\n",
      "192/463, train_loss: 0.2043\n",
      "193/463, train_loss: 0.1283\n",
      "194/463, train_loss: 0.1237\n",
      "195/463, train_loss: 0.1387\n",
      "196/463, train_loss: 0.5356\n",
      "197/463, train_loss: 0.1436\n",
      "198/463, train_loss: 0.2549\n",
      "199/463, train_loss: 0.0775\n",
      "200/463, train_loss: 0.1644\n",
      "201/463, train_loss: 0.1516\n",
      "202/463, train_loss: 0.1678\n",
      "203/463, train_loss: 0.3030\n",
      "204/463, train_loss: 0.1980\n",
      "205/463, train_loss: 0.1746\n",
      "206/463, train_loss: 0.2358\n",
      "207/463, train_loss: 0.1573\n",
      "208/463, train_loss: 0.9570\n",
      "209/463, train_loss: 0.0936\n",
      "210/463, train_loss: 0.3748\n",
      "211/463, train_loss: 0.2256\n",
      "212/463, train_loss: 0.1774\n",
      "213/463, train_loss: 0.0947\n",
      "214/463, train_loss: 0.2773\n",
      "215/463, train_loss: 0.1565\n",
      "216/463, train_loss: 0.1104\n",
      "217/463, train_loss: 0.7754\n",
      "218/463, train_loss: 0.1296\n",
      "219/463, train_loss: 0.2085\n",
      "220/463, train_loss: 0.4258\n",
      "221/463, train_loss: 0.7344\n",
      "222/463, train_loss: 0.1385\n",
      "223/463, train_loss: 0.0853\n",
      "224/463, train_loss: 0.1366\n",
      "225/463, train_loss: 0.0707\n",
      "226/463, train_loss: 0.3757\n",
      "227/463, train_loss: 0.2007\n",
      "228/463, train_loss: 0.1316\n",
      "229/463, train_loss: 0.4639\n",
      "230/463, train_loss: 0.2178\n",
      "231/463, train_loss: 0.2144\n",
      "232/463, train_loss: 0.3093\n",
      "233/463, train_loss: 0.1746\n",
      "234/463, train_loss: 0.3687\n",
      "235/463, train_loss: 0.1479\n",
      "236/463, train_loss: 0.1724\n",
      "237/463, train_loss: 0.0926\n",
      "238/463, train_loss: 0.2145\n",
      "239/463, train_loss: 0.1431\n",
      "240/463, train_loss: 0.2183\n",
      "241/463, train_loss: 0.2446\n",
      "242/463, train_loss: 0.1306\n",
      "243/463, train_loss: 0.1636\n",
      "244/463, train_loss: 0.1528\n",
      "245/463, train_loss: 0.4937\n",
      "246/463, train_loss: 0.4731\n",
      "247/463, train_loss: 0.1210\n",
      "248/463, train_loss: 0.2112\n",
      "249/463, train_loss: 0.3645\n",
      "250/463, train_loss: 0.2297\n",
      "251/463, train_loss: 0.1849\n",
      "252/463, train_loss: 0.1631\n",
      "253/463, train_loss: 0.2405\n",
      "254/463, train_loss: 0.1074\n",
      "255/463, train_loss: 0.4163\n",
      "256/463, train_loss: 0.4912\n",
      "257/463, train_loss: 0.1724\n",
      "258/463, train_loss: 0.1001\n",
      "259/463, train_loss: 0.1742\n",
      "260/463, train_loss: 0.2605\n",
      "261/463, train_loss: 0.1819\n",
      "262/463, train_loss: 0.3047\n",
      "263/463, train_loss: 0.2286\n",
      "264/463, train_loss: 0.0343\n",
      "265/463, train_loss: 0.4541\n",
      "266/463, train_loss: 0.4084\n",
      "267/463, train_loss: 0.1262\n",
      "268/463, train_loss: 0.1782\n",
      "269/463, train_loss: 0.2627\n",
      "270/463, train_loss: 0.1625\n",
      "271/463, train_loss: 0.3433\n",
      "272/463, train_loss: 0.1311\n",
      "273/463, train_loss: 0.2678\n",
      "274/463, train_loss: 0.4944\n",
      "275/463, train_loss: 0.2607\n",
      "276/463, train_loss: 0.9414\n",
      "277/463, train_loss: 0.3652\n",
      "278/463, train_loss: 0.2314\n",
      "279/463, train_loss: 0.3838\n",
      "280/463, train_loss: 0.2384\n",
      "281/463, train_loss: 0.1106\n",
      "282/463, train_loss: 0.2292\n",
      "283/463, train_loss: 0.1864\n",
      "284/463, train_loss: 0.1244\n",
      "285/463, train_loss: 0.2208\n",
      "286/463, train_loss: 0.1772\n",
      "287/463, train_loss: 0.3838\n",
      "288/463, train_loss: 0.4226\n",
      "289/463, train_loss: 0.6123\n",
      "290/463, train_loss: 0.2061\n",
      "291/463, train_loss: 0.4058\n",
      "292/463, train_loss: 0.2778\n",
      "293/463, train_loss: 0.5928\n",
      "294/463, train_loss: 0.4282\n",
      "295/463, train_loss: 0.0922\n",
      "296/463, train_loss: 0.2935\n",
      "297/463, train_loss: 0.3611\n",
      "298/463, train_loss: 0.2910\n",
      "299/463, train_loss: 0.1310\n",
      "300/463, train_loss: 0.1083\n",
      "301/463, train_loss: 0.6851\n",
      "302/463, train_loss: 0.6338\n",
      "303/463, train_loss: 0.0846\n",
      "304/463, train_loss: 0.2881\n",
      "305/463, train_loss: 0.1396\n",
      "306/463, train_loss: 0.2069\n",
      "307/463, train_loss: 0.2754\n",
      "308/463, train_loss: 0.1824\n",
      "309/463, train_loss: 0.2744\n",
      "310/463, train_loss: 0.2267\n",
      "311/463, train_loss: 0.1748\n",
      "312/463, train_loss: 0.1692\n",
      "313/463, train_loss: 0.1113\n",
      "314/463, train_loss: 0.1451\n",
      "315/463, train_loss: 0.2742\n",
      "316/463, train_loss: 0.1306\n",
      "317/463, train_loss: 0.0613\n",
      "318/463, train_loss: 0.1423\n",
      "319/463, train_loss: 0.3645\n",
      "320/463, train_loss: 0.0660\n",
      "321/463, train_loss: 0.4636\n",
      "322/463, train_loss: 0.1466\n",
      "323/463, train_loss: 0.0656\n",
      "324/463, train_loss: 0.6855\n",
      "325/463, train_loss: 0.0915\n",
      "326/463, train_loss: 0.1387\n",
      "327/463, train_loss: 0.1207\n",
      "328/463, train_loss: 0.2759\n",
      "329/463, train_loss: 0.1533\n",
      "330/463, train_loss: 0.0860\n",
      "331/463, train_loss: 0.1829\n",
      "332/463, train_loss: 0.2031\n",
      "333/463, train_loss: 0.1821\n",
      "334/463, train_loss: 0.0935\n",
      "335/463, train_loss: 0.7285\n",
      "336/463, train_loss: 0.3997\n",
      "337/463, train_loss: 0.1157\n",
      "338/463, train_loss: 0.1390\n",
      "339/463, train_loss: 0.1137\n",
      "340/463, train_loss: 0.0757\n",
      "341/463, train_loss: 0.3838\n",
      "342/463, train_loss: 0.3213\n",
      "343/463, train_loss: 0.2251\n",
      "344/463, train_loss: 0.1183\n",
      "345/463, train_loss: 0.0811\n",
      "346/463, train_loss: 0.1772\n",
      "347/463, train_loss: 0.2939\n",
      "348/463, train_loss: 0.2466\n",
      "349/463, train_loss: 0.2798\n",
      "350/463, train_loss: 0.0881\n",
      "351/463, train_loss: 0.4258\n",
      "352/463, train_loss: 0.1907\n",
      "353/463, train_loss: 0.3176\n",
      "354/463, train_loss: 0.2583\n",
      "355/463, train_loss: 0.2627\n",
      "356/463, train_loss: 0.3801\n",
      "357/463, train_loss: 0.3926\n",
      "358/463, train_loss: 0.1414\n",
      "359/463, train_loss: 0.0927\n",
      "360/463, train_loss: 0.5034\n",
      "361/463, train_loss: 0.0767\n",
      "362/463, train_loss: 0.0986\n",
      "363/463, train_loss: 0.4229\n",
      "364/463, train_loss: 0.4717\n",
      "365/463, train_loss: 0.4333\n",
      "366/463, train_loss: 0.1521\n",
      "367/463, train_loss: 0.1004\n",
      "368/463, train_loss: 0.5869\n",
      "369/463, train_loss: 0.1317\n",
      "370/463, train_loss: 0.2686\n",
      "371/463, train_loss: 0.3345\n",
      "372/463, train_loss: 0.1168\n",
      "373/463, train_loss: 0.1436\n",
      "374/463, train_loss: 0.1124\n",
      "375/463, train_loss: 0.2180\n",
      "376/463, train_loss: 0.4292\n",
      "377/463, train_loss: 0.3503\n",
      "378/463, train_loss: 0.5171\n",
      "379/463, train_loss: 0.1187\n",
      "380/463, train_loss: 0.1215\n",
      "381/463, train_loss: 0.1750\n",
      "382/463, train_loss: 0.3506\n",
      "383/463, train_loss: 0.2444\n",
      "384/463, train_loss: 0.3867\n",
      "385/463, train_loss: 0.1240\n",
      "386/463, train_loss: 0.0876\n",
      "387/463, train_loss: 0.0886\n",
      "388/463, train_loss: 0.1001\n",
      "389/463, train_loss: 0.3635\n",
      "390/463, train_loss: 0.1370\n",
      "391/463, train_loss: 0.4443\n",
      "392/463, train_loss: 0.2715\n",
      "393/463, train_loss: 0.2603\n",
      "394/463, train_loss: 0.3254\n",
      "395/463, train_loss: 0.1256\n",
      "396/463, train_loss: 0.3247\n",
      "397/463, train_loss: 0.4868\n",
      "398/463, train_loss: 0.4453\n",
      "399/463, train_loss: 0.5967\n",
      "400/463, train_loss: 0.4414\n",
      "401/463, train_loss: 0.1974\n",
      "402/463, train_loss: 0.1440\n",
      "403/463, train_loss: 0.1808\n",
      "404/463, train_loss: 0.1544\n",
      "405/463, train_loss: 0.3389\n",
      "406/463, train_loss: 0.1361\n",
      "407/463, train_loss: 0.8105\n",
      "408/463, train_loss: 0.1117\n",
      "409/463, train_loss: 0.2063\n",
      "410/463, train_loss: 0.1608\n",
      "411/463, train_loss: 0.3420\n",
      "412/463, train_loss: 0.3604\n",
      "413/463, train_loss: 0.8223\n",
      "414/463, train_loss: 0.1967\n",
      "415/463, train_loss: 0.2231\n",
      "416/463, train_loss: 0.1040\n",
      "417/463, train_loss: 0.5645\n",
      "418/463, train_loss: 0.2725\n",
      "419/463, train_loss: 0.1455\n",
      "420/463, train_loss: 0.3044\n",
      "421/463, train_loss: 0.2029\n",
      "422/463, train_loss: 0.1523\n",
      "423/463, train_loss: 0.1605\n",
      "424/463, train_loss: 0.1500\n",
      "425/463, train_loss: 0.3259\n",
      "426/463, train_loss: 0.1110\n",
      "427/463, train_loss: 0.1853\n",
      "428/463, train_loss: 0.1436\n",
      "429/463, train_loss: 0.2078\n",
      "430/463, train_loss: 0.1901\n",
      "431/463, train_loss: 0.1815\n",
      "432/463, train_loss: 0.2930\n",
      "433/463, train_loss: 0.1506\n",
      "434/463, train_loss: 0.1870\n",
      "435/463, train_loss: 0.6895\n",
      "436/463, train_loss: 0.1621\n",
      "437/463, train_loss: 0.1907\n",
      "438/463, train_loss: 0.4263\n",
      "439/463, train_loss: 0.1340\n",
      "440/463, train_loss: 0.1473\n",
      "441/463, train_loss: 0.1279\n",
      "442/463, train_loss: 0.1062\n",
      "443/463, train_loss: 0.1868\n",
      "444/463, train_loss: 0.1581\n",
      "445/463, train_loss: 0.1577\n",
      "446/463, train_loss: 0.2144\n",
      "447/463, train_loss: 0.9126\n",
      "448/463, train_loss: 0.3530\n",
      "449/463, train_loss: 0.1945\n",
      "450/463, train_loss: 0.1925\n",
      "451/463, train_loss: 0.1028\n",
      "452/463, train_loss: 0.1589\n",
      "453/463, train_loss: 0.2484\n",
      "454/463, train_loss: 0.3804\n",
      "455/463, train_loss: 0.1697\n",
      "456/463, train_loss: 0.3682\n",
      "457/463, train_loss: 0.2258\n",
      "458/463, train_loss: 0.4319\n",
      "459/463, train_loss: 0.1498\n",
      "460/463, train_loss: 0.1832\n",
      "461/463, train_loss: 0.0909\n",
      "462/463, train_loss: 0.1362\n",
      "463/463, train_loss: 0.1501\n",
      "epoch 12 average loss: 0.2520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 07:50:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 07:50:52 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 07:50:54 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 13/100\n",
      "1/463, train_loss: 0.8159\n",
      "2/463, train_loss: 0.4180\n",
      "3/463, train_loss: 0.3115\n",
      "4/463, train_loss: 0.1733\n",
      "5/463, train_loss: 0.2391\n",
      "6/463, train_loss: 0.2957\n",
      "7/463, train_loss: 0.1293\n",
      "8/463, train_loss: 0.1037\n",
      "9/463, train_loss: 0.1799\n",
      "10/463, train_loss: 0.1987\n",
      "11/463, train_loss: 0.4512\n",
      "12/463, train_loss: 0.2056\n",
      "13/463, train_loss: 0.0746\n",
      "14/463, train_loss: 0.7393\n",
      "15/463, train_loss: 0.2827\n",
      "16/463, train_loss: 0.1243\n",
      "17/463, train_loss: 0.1838\n",
      "18/463, train_loss: 0.1971\n",
      "19/463, train_loss: 0.3125\n",
      "20/463, train_loss: 0.0658\n",
      "21/463, train_loss: 0.4497\n",
      "22/463, train_loss: 0.2866\n",
      "23/463, train_loss: 0.1599\n",
      "24/463, train_loss: 0.2023\n",
      "25/463, train_loss: 0.1404\n",
      "26/463, train_loss: 0.0734\n",
      "27/463, train_loss: 0.1499\n",
      "28/463, train_loss: 0.1129\n",
      "29/463, train_loss: 0.1538\n",
      "30/463, train_loss: 0.1283\n",
      "31/463, train_loss: 0.4019\n",
      "32/463, train_loss: 0.1514\n",
      "33/463, train_loss: 0.1071\n",
      "34/463, train_loss: 0.1125\n",
      "35/463, train_loss: 0.1479\n",
      "36/463, train_loss: 0.1157\n",
      "37/463, train_loss: 0.0458\n",
      "38/463, train_loss: 0.2068\n",
      "39/463, train_loss: 0.1888\n",
      "40/463, train_loss: 0.0172\n",
      "41/463, train_loss: 0.0996\n",
      "42/463, train_loss: 0.1405\n",
      "43/463, train_loss: 0.2981\n",
      "44/463, train_loss: 0.0649\n",
      "45/463, train_loss: 0.1165\n",
      "46/463, train_loss: 0.0775\n",
      "47/463, train_loss: 0.2356\n",
      "48/463, train_loss: 0.1836\n",
      "49/463, train_loss: 0.1648\n",
      "50/463, train_loss: 0.2397\n",
      "51/463, train_loss: 0.0653\n",
      "52/463, train_loss: 0.3694\n",
      "53/463, train_loss: 0.1693\n",
      "54/463, train_loss: 0.1395\n",
      "55/463, train_loss: 0.1553\n",
      "56/463, train_loss: 0.2355\n",
      "57/463, train_loss: 0.4966\n",
      "58/463, train_loss: 0.1028\n",
      "59/463, train_loss: 0.2205\n",
      "60/463, train_loss: 0.2283\n",
      "61/463, train_loss: 0.1650\n",
      "62/463, train_loss: 0.1648\n",
      "63/463, train_loss: 0.4292\n",
      "64/463, train_loss: 0.1826\n",
      "65/463, train_loss: 0.1577\n",
      "66/463, train_loss: 0.2424\n",
      "67/463, train_loss: 0.1379\n",
      "68/463, train_loss: 0.1611\n",
      "69/463, train_loss: 0.3923\n",
      "70/463, train_loss: 0.2312\n",
      "71/463, train_loss: 0.1049\n",
      "72/463, train_loss: 0.2139\n",
      "73/463, train_loss: 0.6323\n",
      "74/463, train_loss: 0.1562\n",
      "75/463, train_loss: 0.0829\n",
      "76/463, train_loss: 0.2391\n",
      "77/463, train_loss: 0.2446\n",
      "78/463, train_loss: 0.1289\n",
      "79/463, train_loss: 0.1942\n",
      "80/463, train_loss: 0.1462\n",
      "81/463, train_loss: 0.2465\n",
      "82/463, train_loss: 0.4973\n",
      "83/463, train_loss: 0.4629\n",
      "84/463, train_loss: 0.1755\n",
      "85/463, train_loss: 0.2935\n",
      "86/463, train_loss: 0.1432\n",
      "87/463, train_loss: 0.1304\n",
      "88/463, train_loss: 0.1321\n",
      "89/463, train_loss: 0.5024\n",
      "90/463, train_loss: 0.2554\n",
      "91/463, train_loss: 0.3330\n",
      "92/463, train_loss: 0.3784\n",
      "93/463, train_loss: 0.2446\n",
      "94/463, train_loss: 0.3621\n",
      "95/463, train_loss: 0.1841\n",
      "96/463, train_loss: 0.2573\n",
      "97/463, train_loss: 0.1621\n",
      "98/463, train_loss: 0.3110\n",
      "99/463, train_loss: 0.1289\n",
      "100/463, train_loss: 0.2009\n",
      "101/463, train_loss: 0.3926\n",
      "102/463, train_loss: 0.2236\n",
      "103/463, train_loss: 0.2827\n",
      "104/463, train_loss: 0.2991\n",
      "105/463, train_loss: 0.2124\n",
      "106/463, train_loss: 0.2048\n",
      "107/463, train_loss: 0.1921\n",
      "108/463, train_loss: 0.1970\n",
      "109/463, train_loss: 0.2047\n",
      "110/463, train_loss: 0.0404\n",
      "111/463, train_loss: 0.2408\n",
      "112/463, train_loss: 0.0823\n",
      "113/463, train_loss: 1.4629\n",
      "114/463, train_loss: 0.5103\n",
      "115/463, train_loss: 0.5459\n",
      "116/463, train_loss: 0.1597\n",
      "117/463, train_loss: 0.1123\n",
      "118/463, train_loss: 0.3169\n",
      "119/463, train_loss: 0.1581\n",
      "120/463, train_loss: 0.2402\n",
      "121/463, train_loss: 0.2312\n",
      "122/463, train_loss: 0.6758\n",
      "123/463, train_loss: 0.3335\n",
      "124/463, train_loss: 0.8257\n",
      "125/463, train_loss: 0.6050\n",
      "126/463, train_loss: 0.1395\n",
      "127/463, train_loss: 0.1785\n",
      "128/463, train_loss: 0.1299\n",
      "129/463, train_loss: 0.1946\n",
      "130/463, train_loss: 0.3464\n",
      "131/463, train_loss: 0.0894\n",
      "132/463, train_loss: 0.1987\n",
      "133/463, train_loss: 0.2942\n",
      "134/463, train_loss: 0.4771\n",
      "135/463, train_loss: 0.1412\n",
      "136/463, train_loss: 0.2644\n",
      "137/463, train_loss: 0.4736\n",
      "138/463, train_loss: 0.3342\n",
      "139/463, train_loss: 0.1260\n",
      "140/463, train_loss: 0.3174\n",
      "141/463, train_loss: 0.2257\n",
      "142/463, train_loss: 0.2605\n",
      "143/463, train_loss: 0.3745\n",
      "144/463, train_loss: 0.1747\n",
      "145/463, train_loss: 0.1271\n",
      "146/463, train_loss: 0.1099\n",
      "147/463, train_loss: 0.7197\n",
      "148/463, train_loss: 0.1887\n",
      "149/463, train_loss: 0.1807\n",
      "150/463, train_loss: 0.2588\n",
      "151/463, train_loss: 0.1328\n",
      "152/463, train_loss: 0.9160\n",
      "153/463, train_loss: 0.1445\n",
      "154/463, train_loss: 0.2220\n",
      "155/463, train_loss: 0.1484\n",
      "156/463, train_loss: 0.1106\n",
      "157/463, train_loss: 0.2039\n",
      "158/463, train_loss: 0.1475\n",
      "159/463, train_loss: 0.0957\n",
      "160/463, train_loss: 0.1692\n",
      "161/463, train_loss: 0.2871\n",
      "162/463, train_loss: 0.1083\n",
      "163/463, train_loss: 0.3206\n",
      "164/463, train_loss: 0.2122\n",
      "165/463, train_loss: 0.1462\n",
      "166/463, train_loss: 0.6016\n",
      "167/463, train_loss: 0.3608\n",
      "168/463, train_loss: 0.2004\n",
      "169/463, train_loss: 0.0897\n",
      "170/463, train_loss: 0.2074\n",
      "171/463, train_loss: 0.1907\n",
      "172/463, train_loss: 0.1670\n",
      "173/463, train_loss: 0.1445\n",
      "174/463, train_loss: 0.4238\n",
      "175/463, train_loss: 0.1117\n",
      "176/463, train_loss: 0.1843\n",
      "177/463, train_loss: 0.0782\n",
      "178/463, train_loss: 0.1033\n",
      "179/463, train_loss: 0.3826\n",
      "180/463, train_loss: 0.1035\n",
      "181/463, train_loss: 0.1372\n",
      "182/463, train_loss: 0.0747\n",
      "183/463, train_loss: 0.1300\n",
      "184/463, train_loss: 0.3716\n",
      "185/463, train_loss: 0.2524\n",
      "186/463, train_loss: 0.2305\n",
      "187/463, train_loss: 0.1096\n",
      "188/463, train_loss: 0.4688\n",
      "189/463, train_loss: 0.0897\n",
      "190/463, train_loss: 0.1206\n",
      "191/463, train_loss: 0.1505\n",
      "192/463, train_loss: 0.3618\n",
      "193/463, train_loss: 0.1365\n",
      "194/463, train_loss: 0.1553\n",
      "195/463, train_loss: 0.1143\n",
      "196/463, train_loss: 0.2495\n",
      "197/463, train_loss: 0.1561\n",
      "198/463, train_loss: 0.4917\n",
      "199/463, train_loss: 0.1863\n",
      "200/463, train_loss: 0.1226\n",
      "201/463, train_loss: 0.0764\n",
      "202/463, train_loss: 0.2306\n",
      "203/463, train_loss: 0.2593\n",
      "204/463, train_loss: 0.1958\n",
      "205/463, train_loss: 0.2542\n",
      "206/463, train_loss: 0.1180\n",
      "207/463, train_loss: 0.2520\n",
      "208/463, train_loss: 0.1953\n",
      "209/463, train_loss: 0.5693\n",
      "210/463, train_loss: 0.2032\n",
      "211/463, train_loss: 0.1631\n",
      "212/463, train_loss: 0.0784\n",
      "213/463, train_loss: 0.1562\n",
      "214/463, train_loss: 0.8271\n",
      "215/463, train_loss: 0.2620\n",
      "216/463, train_loss: 0.1133\n",
      "217/463, train_loss: 0.3115\n",
      "218/463, train_loss: 0.2725\n",
      "219/463, train_loss: 0.1832\n",
      "220/463, train_loss: 0.5625\n",
      "221/463, train_loss: 0.1377\n",
      "222/463, train_loss: 0.3555\n",
      "223/463, train_loss: 0.4102\n",
      "224/463, train_loss: 0.1453\n",
      "225/463, train_loss: 0.3503\n",
      "226/463, train_loss: 0.2722\n",
      "227/463, train_loss: 0.1259\n",
      "228/463, train_loss: 0.2954\n",
      "229/463, train_loss: 0.1702\n",
      "230/463, train_loss: 0.1589\n",
      "231/463, train_loss: 0.7866\n",
      "232/463, train_loss: 0.2578\n",
      "233/463, train_loss: 0.2993\n",
      "234/463, train_loss: 0.1835\n",
      "235/463, train_loss: 0.1886\n",
      "236/463, train_loss: 0.2820\n",
      "237/463, train_loss: 0.3113\n",
      "238/463, train_loss: 0.1326\n",
      "239/463, train_loss: 0.1047\n",
      "240/463, train_loss: 0.2578\n",
      "241/463, train_loss: 0.1097\n",
      "242/463, train_loss: 0.0858\n",
      "243/463, train_loss: 0.1153\n",
      "244/463, train_loss: 0.1185\n",
      "245/463, train_loss: 0.0720\n",
      "246/463, train_loss: 0.2186\n",
      "247/463, train_loss: 0.1073\n",
      "248/463, train_loss: 0.1392\n",
      "249/463, train_loss: 0.2671\n",
      "250/463, train_loss: 0.1034\n",
      "251/463, train_loss: 0.4507\n",
      "252/463, train_loss: 0.1555\n",
      "253/463, train_loss: 0.1246\n",
      "254/463, train_loss: 0.1029\n",
      "255/463, train_loss: 0.0622\n",
      "256/463, train_loss: 0.1975\n",
      "257/463, train_loss: 0.4258\n",
      "258/463, train_loss: 0.0499\n",
      "259/463, train_loss: 0.2358\n",
      "260/463, train_loss: 0.3347\n",
      "261/463, train_loss: 0.2842\n",
      "262/463, train_loss: 0.1862\n",
      "263/463, train_loss: 0.1484\n",
      "264/463, train_loss: 0.3315\n",
      "265/463, train_loss: 0.2590\n",
      "266/463, train_loss: 0.1404\n",
      "267/463, train_loss: 0.4512\n",
      "268/463, train_loss: 0.1819\n",
      "269/463, train_loss: 0.0732\n",
      "270/463, train_loss: 0.1399\n",
      "271/463, train_loss: 0.1941\n",
      "272/463, train_loss: 0.2218\n",
      "273/463, train_loss: 0.2114\n",
      "274/463, train_loss: 0.1436\n",
      "275/463, train_loss: 0.3076\n",
      "276/463, train_loss: 0.1934\n",
      "277/463, train_loss: 0.5098\n",
      "278/463, train_loss: 0.2827\n",
      "279/463, train_loss: 0.2651\n",
      "280/463, train_loss: 0.3704\n",
      "281/463, train_loss: 0.1990\n",
      "282/463, train_loss: 0.2153\n",
      "283/463, train_loss: 0.1624\n",
      "284/463, train_loss: 0.2305\n",
      "285/463, train_loss: 0.2059\n",
      "286/463, train_loss: 0.2200\n",
      "287/463, train_loss: 0.1536\n",
      "288/463, train_loss: 0.1520\n",
      "289/463, train_loss: 1.1689\n",
      "290/463, train_loss: 0.1112\n",
      "291/463, train_loss: 0.3259\n",
      "292/463, train_loss: 0.3174\n",
      "293/463, train_loss: 0.1157\n",
      "294/463, train_loss: 0.4106\n",
      "295/463, train_loss: 0.2727\n",
      "296/463, train_loss: 0.5269\n",
      "297/463, train_loss: 0.7451\n",
      "298/463, train_loss: 0.2200\n",
      "299/463, train_loss: 0.2014\n",
      "300/463, train_loss: 0.1926\n",
      "301/463, train_loss: 0.1685\n",
      "302/463, train_loss: 0.2959\n",
      "303/463, train_loss: 0.1016\n",
      "304/463, train_loss: 0.1433\n",
      "305/463, train_loss: 0.2588\n",
      "306/463, train_loss: 0.2725\n",
      "307/463, train_loss: 0.1426\n",
      "308/463, train_loss: 0.1277\n",
      "309/463, train_loss: 0.1887\n",
      "310/463, train_loss: 0.1407\n",
      "311/463, train_loss: 0.3999\n",
      "312/463, train_loss: 0.1322\n",
      "313/463, train_loss: 0.5522\n",
      "314/463, train_loss: 0.2212\n",
      "315/463, train_loss: 0.0684\n",
      "316/463, train_loss: 0.2427\n",
      "317/463, train_loss: 0.5742\n",
      "318/463, train_loss: 0.3494\n",
      "319/463, train_loss: 0.1156\n",
      "320/463, train_loss: 0.1150\n",
      "321/463, train_loss: 0.1323\n",
      "322/463, train_loss: 0.1287\n",
      "323/463, train_loss: 0.1484\n",
      "324/463, train_loss: 0.2778\n",
      "325/463, train_loss: 0.4805\n",
      "326/463, train_loss: 0.0830\n",
      "327/463, train_loss: 0.1046\n",
      "328/463, train_loss: 0.2881\n",
      "329/463, train_loss: 0.3618\n",
      "330/463, train_loss: 0.1294\n",
      "331/463, train_loss: 0.1946\n",
      "332/463, train_loss: 0.1239\n",
      "333/463, train_loss: 0.2434\n",
      "334/463, train_loss: 0.2854\n",
      "335/463, train_loss: 0.4248\n",
      "336/463, train_loss: 0.0756\n",
      "337/463, train_loss: 0.1810\n",
      "338/463, train_loss: 0.1653\n",
      "339/463, train_loss: 0.0924\n",
      "340/463, train_loss: 0.2119\n",
      "341/463, train_loss: 0.2479\n",
      "342/463, train_loss: 0.1626\n",
      "343/463, train_loss: 0.6396\n",
      "344/463, train_loss: 0.2363\n",
      "345/463, train_loss: 0.8296\n",
      "346/463, train_loss: 0.2905\n",
      "347/463, train_loss: 0.2017\n",
      "348/463, train_loss: 0.3015\n",
      "349/463, train_loss: 0.1689\n",
      "350/463, train_loss: 0.0526\n",
      "351/463, train_loss: 0.1650\n",
      "352/463, train_loss: 0.1688\n",
      "353/463, train_loss: 0.3140\n",
      "354/463, train_loss: 0.1958\n",
      "355/463, train_loss: 0.1364\n",
      "356/463, train_loss: 0.4468\n",
      "357/463, train_loss: 0.1065\n",
      "358/463, train_loss: 0.1376\n",
      "359/463, train_loss: 0.1235\n",
      "360/463, train_loss: 0.1083\n",
      "361/463, train_loss: 0.1543\n",
      "362/463, train_loss: 0.1093\n",
      "363/463, train_loss: 0.0902\n",
      "364/463, train_loss: 0.5024\n",
      "365/463, train_loss: 0.1526\n",
      "366/463, train_loss: 0.3901\n",
      "367/463, train_loss: 0.2445\n",
      "368/463, train_loss: 0.1227\n",
      "369/463, train_loss: 0.4966\n",
      "370/463, train_loss: 0.1301\n",
      "371/463, train_loss: 0.1586\n",
      "372/463, train_loss: 0.0705\n",
      "373/463, train_loss: 0.2010\n",
      "374/463, train_loss: 0.1744\n",
      "375/463, train_loss: 0.2935\n",
      "376/463, train_loss: 0.1411\n",
      "377/463, train_loss: 0.0562\n",
      "378/463, train_loss: 0.1387\n",
      "379/463, train_loss: 0.1127\n",
      "380/463, train_loss: 0.1952\n",
      "381/463, train_loss: 0.6289\n",
      "382/463, train_loss: 0.1658\n",
      "383/463, train_loss: 0.0781\n",
      "384/463, train_loss: 0.1929\n",
      "385/463, train_loss: 0.1189\n",
      "386/463, train_loss: 0.1187\n",
      "387/463, train_loss: 0.4939\n",
      "388/463, train_loss: 0.2520\n",
      "389/463, train_loss: 0.4717\n",
      "390/463, train_loss: 0.5410\n",
      "391/463, train_loss: 0.4482\n",
      "392/463, train_loss: 0.1521\n",
      "393/463, train_loss: 0.5181\n",
      "394/463, train_loss: 0.5166\n",
      "395/463, train_loss: 0.2688\n",
      "396/463, train_loss: 0.1741\n",
      "397/463, train_loss: 0.3931\n",
      "398/463, train_loss: 0.2091\n",
      "399/463, train_loss: 0.2957\n",
      "400/463, train_loss: 0.2024\n",
      "401/463, train_loss: 0.3560\n",
      "402/463, train_loss: 0.1942\n",
      "403/463, train_loss: 0.2109\n",
      "404/463, train_loss: 0.1918\n",
      "405/463, train_loss: 0.2019\n",
      "406/463, train_loss: 0.4263\n",
      "407/463, train_loss: 0.9023\n",
      "408/463, train_loss: 0.5586\n",
      "409/463, train_loss: 0.0958\n",
      "410/463, train_loss: 0.1624\n",
      "411/463, train_loss: 0.2756\n",
      "412/463, train_loss: 0.2102\n",
      "413/463, train_loss: 0.1366\n",
      "414/463, train_loss: 0.2025\n",
      "415/463, train_loss: 0.0873\n",
      "416/463, train_loss: 0.1746\n",
      "417/463, train_loss: 0.1033\n",
      "418/463, train_loss: 0.1475\n",
      "419/463, train_loss: 0.1099\n",
      "420/463, train_loss: 0.1001\n",
      "421/463, train_loss: 0.3853\n",
      "422/463, train_loss: 0.0953\n",
      "423/463, train_loss: 0.1793\n",
      "424/463, train_loss: 0.1018\n",
      "425/463, train_loss: 0.0984\n",
      "426/463, train_loss: 0.1100\n",
      "427/463, train_loss: 0.3369\n",
      "428/463, train_loss: 0.1239\n",
      "429/463, train_loss: 0.1810\n",
      "430/463, train_loss: 0.4973\n",
      "431/463, train_loss: 0.1736\n",
      "432/463, train_loss: 0.5605\n",
      "433/463, train_loss: 0.1077\n",
      "434/463, train_loss: 0.4058\n",
      "435/463, train_loss: 0.1260\n",
      "436/463, train_loss: 0.2798\n",
      "437/463, train_loss: 0.3799\n",
      "438/463, train_loss: 0.4363\n",
      "439/463, train_loss: 0.1224\n",
      "440/463, train_loss: 0.2251\n",
      "441/463, train_loss: 0.1345\n",
      "442/463, train_loss: 0.1437\n",
      "443/463, train_loss: 0.0952\n",
      "444/463, train_loss: 0.3484\n",
      "445/463, train_loss: 0.4873\n",
      "446/463, train_loss: 0.7012\n",
      "447/463, train_loss: 0.2974\n",
      "448/463, train_loss: 0.3164\n",
      "449/463, train_loss: 0.5469\n",
      "450/463, train_loss: 0.0333\n",
      "451/463, train_loss: 0.1909\n",
      "452/463, train_loss: 0.1492\n",
      "453/463, train_loss: 0.1458\n",
      "454/463, train_loss: 0.2812\n",
      "455/463, train_loss: 0.1260\n",
      "456/463, train_loss: 0.4011\n",
      "457/463, train_loss: 0.1694\n",
      "458/463, train_loss: 0.1200\n",
      "459/463, train_loss: 0.0760\n",
      "460/463, train_loss: 0.3083\n",
      "461/463, train_loss: 0.4243\n",
      "462/463, train_loss: 0.4578\n",
      "463/463, train_loss: 0.1539\n",
      "epoch 13 average loss: 0.2446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 10:03:30 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 10:03:33 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 10:03:36 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 14/100\n",
      "1/463, train_loss: 0.1562\n",
      "2/463, train_loss: 0.1212\n",
      "3/463, train_loss: 0.3674\n",
      "4/463, train_loss: 0.0937\n",
      "5/463, train_loss: 0.0961\n",
      "6/463, train_loss: 0.1484\n",
      "7/463, train_loss: 0.4028\n",
      "8/463, train_loss: 0.2583\n",
      "9/463, train_loss: 0.1227\n",
      "10/463, train_loss: 0.0891\n",
      "11/463, train_loss: 0.1821\n",
      "12/463, train_loss: 0.1238\n",
      "13/463, train_loss: 0.2338\n",
      "14/463, train_loss: 0.2695\n",
      "15/463, train_loss: 0.1499\n",
      "16/463, train_loss: 0.2368\n",
      "17/463, train_loss: 0.1594\n",
      "18/463, train_loss: 0.1299\n",
      "19/463, train_loss: 0.3257\n",
      "20/463, train_loss: 0.2434\n",
      "21/463, train_loss: 0.0739\n",
      "22/463, train_loss: 0.3184\n",
      "23/463, train_loss: 0.3450\n",
      "24/463, train_loss: 0.0400\n",
      "25/463, train_loss: 0.1885\n",
      "26/463, train_loss: 0.3057\n",
      "27/463, train_loss: 0.1814\n",
      "28/463, train_loss: 0.0632\n",
      "29/463, train_loss: 0.1195\n",
      "30/463, train_loss: 0.5068\n",
      "31/463, train_loss: 0.1083\n",
      "32/463, train_loss: 0.3779\n",
      "33/463, train_loss: 0.0767\n",
      "34/463, train_loss: 1.0801\n",
      "35/463, train_loss: 0.4580\n",
      "36/463, train_loss: 0.1255\n",
      "37/463, train_loss: 0.1531\n",
      "38/463, train_loss: 0.3032\n",
      "39/463, train_loss: 0.2367\n",
      "40/463, train_loss: 0.2164\n",
      "41/463, train_loss: 0.5249\n",
      "42/463, train_loss: 0.1511\n",
      "43/463, train_loss: 0.1142\n",
      "44/463, train_loss: 0.2546\n",
      "45/463, train_loss: 0.2224\n",
      "46/463, train_loss: 0.2499\n",
      "47/463, train_loss: 0.2654\n",
      "48/463, train_loss: 0.1115\n",
      "49/463, train_loss: 0.5195\n",
      "50/463, train_loss: 0.0864\n",
      "51/463, train_loss: 0.1562\n",
      "52/463, train_loss: 0.1648\n",
      "53/463, train_loss: 0.1995\n",
      "54/463, train_loss: 0.1388\n",
      "55/463, train_loss: 0.2095\n",
      "56/463, train_loss: 0.1263\n",
      "57/463, train_loss: 0.0849\n",
      "58/463, train_loss: 0.2119\n",
      "59/463, train_loss: 0.1003\n",
      "60/463, train_loss: 0.3718\n",
      "61/463, train_loss: 0.4727\n",
      "62/463, train_loss: 0.1615\n",
      "63/463, train_loss: 0.1520\n",
      "64/463, train_loss: 0.2576\n",
      "65/463, train_loss: 0.3486\n",
      "66/463, train_loss: 0.2571\n",
      "67/463, train_loss: 0.2421\n",
      "68/463, train_loss: 0.1392\n",
      "69/463, train_loss: 0.2222\n",
      "70/463, train_loss: 0.1471\n",
      "71/463, train_loss: 0.2061\n",
      "72/463, train_loss: 0.0760\n",
      "73/463, train_loss: 0.4590\n",
      "74/463, train_loss: 0.1588\n",
      "75/463, train_loss: 0.2480\n",
      "76/463, train_loss: 0.4839\n",
      "77/463, train_loss: 0.3660\n",
      "78/463, train_loss: 0.0967\n",
      "79/463, train_loss: 0.1421\n",
      "80/463, train_loss: 0.9092\n",
      "81/463, train_loss: 0.1143\n",
      "82/463, train_loss: 0.2197\n",
      "83/463, train_loss: 0.9209\n",
      "84/463, train_loss: 0.1191\n",
      "85/463, train_loss: 0.3701\n",
      "86/463, train_loss: 0.2109\n",
      "87/463, train_loss: 0.4658\n",
      "88/463, train_loss: 0.1763\n",
      "89/463, train_loss: 0.2395\n",
      "90/463, train_loss: 0.1436\n",
      "91/463, train_loss: 0.3496\n",
      "92/463, train_loss: 0.1135\n",
      "93/463, train_loss: 0.1665\n",
      "94/463, train_loss: 0.1503\n",
      "95/463, train_loss: 0.4922\n",
      "96/463, train_loss: 0.1342\n",
      "97/463, train_loss: 0.1589\n",
      "98/463, train_loss: 0.0856\n",
      "99/463, train_loss: 0.1392\n",
      "100/463, train_loss: 0.1489\n",
      "101/463, train_loss: 0.1042\n",
      "102/463, train_loss: 0.2615\n",
      "103/463, train_loss: 0.1345\n",
      "104/463, train_loss: 0.4028\n",
      "105/463, train_loss: 0.1755\n",
      "106/463, train_loss: 0.1008\n",
      "107/463, train_loss: 0.1765\n",
      "108/463, train_loss: 0.1316\n",
      "109/463, train_loss: 0.1521\n",
      "110/463, train_loss: 0.2979\n",
      "111/463, train_loss: 0.0920\n",
      "112/463, train_loss: 0.0747\n",
      "113/463, train_loss: 0.4380\n",
      "114/463, train_loss: 0.0778\n",
      "115/463, train_loss: 0.1134\n",
      "116/463, train_loss: 0.1353\n",
      "117/463, train_loss: 0.1438\n",
      "118/463, train_loss: 0.3679\n",
      "119/463, train_loss: 0.1360\n",
      "120/463, train_loss: 0.2037\n",
      "121/463, train_loss: 0.1078\n",
      "122/463, train_loss: 0.4897\n",
      "123/463, train_loss: 0.2461\n",
      "124/463, train_loss: 0.1675\n",
      "125/463, train_loss: 0.6743\n",
      "126/463, train_loss: 0.1652\n",
      "127/463, train_loss: 0.2915\n",
      "128/463, train_loss: 0.1433\n",
      "129/463, train_loss: 0.3584\n",
      "130/463, train_loss: 0.3369\n",
      "131/463, train_loss: 0.1387\n",
      "132/463, train_loss: 0.2303\n",
      "133/463, train_loss: 0.1028\n",
      "134/463, train_loss: 0.1464\n",
      "135/463, train_loss: 0.4360\n",
      "136/463, train_loss: 0.1438\n",
      "137/463, train_loss: 0.7051\n",
      "138/463, train_loss: 0.3691\n",
      "139/463, train_loss: 0.1875\n",
      "140/463, train_loss: 0.1089\n",
      "141/463, train_loss: 0.3367\n",
      "142/463, train_loss: 0.1777\n",
      "143/463, train_loss: 0.1716\n",
      "144/463, train_loss: 0.3337\n",
      "145/463, train_loss: 0.1545\n",
      "146/463, train_loss: 0.1515\n",
      "147/463, train_loss: 0.1091\n",
      "148/463, train_loss: 0.2451\n",
      "149/463, train_loss: 0.1232\n",
      "150/463, train_loss: 0.1367\n",
      "151/463, train_loss: 0.1384\n",
      "152/463, train_loss: 0.3145\n",
      "153/463, train_loss: 0.3020\n",
      "154/463, train_loss: 0.1525\n",
      "155/463, train_loss: 0.1377\n",
      "156/463, train_loss: 0.3689\n",
      "157/463, train_loss: 0.1187\n",
      "158/463, train_loss: 0.1300\n",
      "159/463, train_loss: 0.4658\n",
      "160/463, train_loss: 0.1191\n",
      "161/463, train_loss: 0.1648\n",
      "162/463, train_loss: 0.1917\n",
      "163/463, train_loss: 0.1370\n",
      "164/463, train_loss: 0.1316\n",
      "165/463, train_loss: 0.0690\n",
      "166/463, train_loss: 0.2242\n",
      "167/463, train_loss: 0.1078\n",
      "168/463, train_loss: 0.1934\n",
      "169/463, train_loss: 0.0791\n",
      "170/463, train_loss: 0.3623\n",
      "171/463, train_loss: 0.3232\n",
      "172/463, train_loss: 0.5747\n",
      "173/463, train_loss: 0.4185\n",
      "174/463, train_loss: 0.1636\n",
      "175/463, train_loss: 0.0980\n",
      "176/463, train_loss: 0.1548\n",
      "177/463, train_loss: 0.0818\n",
      "178/463, train_loss: 0.3586\n",
      "179/463, train_loss: 0.1504\n",
      "180/463, train_loss: 0.3926\n",
      "181/463, train_loss: 0.3682\n",
      "182/463, train_loss: 0.1195\n",
      "183/463, train_loss: 0.1471\n",
      "184/463, train_loss: 0.2737\n",
      "185/463, train_loss: 0.1041\n",
      "186/463, train_loss: 0.1129\n",
      "187/463, train_loss: 0.5225\n",
      "188/463, train_loss: 0.1549\n",
      "189/463, train_loss: 0.3545\n",
      "190/463, train_loss: 0.2441\n",
      "191/463, train_loss: 0.3115\n",
      "192/463, train_loss: 0.4834\n",
      "193/463, train_loss: 0.2944\n",
      "194/463, train_loss: 0.3518\n",
      "195/463, train_loss: 0.2086\n",
      "196/463, train_loss: 0.1209\n",
      "197/463, train_loss: 0.1372\n",
      "198/463, train_loss: 0.0745\n",
      "199/463, train_loss: 0.4312\n",
      "200/463, train_loss: 0.2983\n",
      "201/463, train_loss: 0.3091\n",
      "202/463, train_loss: 0.1614\n",
      "203/463, train_loss: 0.1169\n",
      "204/463, train_loss: 0.5464\n",
      "205/463, train_loss: 0.1920\n",
      "206/463, train_loss: 0.3743\n",
      "207/463, train_loss: 0.2456\n",
      "208/463, train_loss: 0.1711\n",
      "209/463, train_loss: 0.3350\n",
      "210/463, train_loss: 0.1578\n",
      "211/463, train_loss: 0.1445\n",
      "212/463, train_loss: 0.1816\n",
      "213/463, train_loss: 0.2343\n",
      "214/463, train_loss: 0.0980\n",
      "215/463, train_loss: 0.3865\n",
      "216/463, train_loss: 0.1444\n",
      "217/463, train_loss: 0.6099\n",
      "218/463, train_loss: 0.1909\n",
      "219/463, train_loss: 0.1841\n",
      "220/463, train_loss: 0.0997\n",
      "221/463, train_loss: 0.1208\n",
      "222/463, train_loss: 0.2134\n",
      "223/463, train_loss: 0.4265\n",
      "224/463, train_loss: 0.1406\n",
      "225/463, train_loss: 0.2637\n",
      "226/463, train_loss: 0.1636\n",
      "227/463, train_loss: 0.1040\n",
      "228/463, train_loss: 0.3948\n",
      "229/463, train_loss: 0.4661\n",
      "230/463, train_loss: 0.1506\n",
      "231/463, train_loss: 0.1182\n",
      "232/463, train_loss: 0.2122\n",
      "233/463, train_loss: 0.3003\n",
      "234/463, train_loss: 0.1548\n",
      "235/463, train_loss: 0.1440\n",
      "236/463, train_loss: 0.1063\n",
      "237/463, train_loss: 0.1958\n",
      "238/463, train_loss: 0.1522\n",
      "239/463, train_loss: 0.1062\n",
      "240/463, train_loss: 0.2461\n",
      "241/463, train_loss: 0.0917\n",
      "242/463, train_loss: 0.5488\n",
      "243/463, train_loss: 0.2263\n",
      "244/463, train_loss: 0.1901\n",
      "245/463, train_loss: 0.0830\n",
      "246/463, train_loss: 0.3838\n",
      "247/463, train_loss: 0.1528\n",
      "248/463, train_loss: 0.2144\n",
      "249/463, train_loss: 0.1248\n",
      "250/463, train_loss: 0.1350\n",
      "251/463, train_loss: 0.1161\n",
      "252/463, train_loss: 0.1054\n",
      "253/463, train_loss: 0.1317\n",
      "254/463, train_loss: 0.1377\n",
      "255/463, train_loss: 0.2910\n",
      "256/463, train_loss: 0.2413\n",
      "257/463, train_loss: 0.0961\n",
      "258/463, train_loss: 0.3926\n",
      "259/463, train_loss: 0.0779\n",
      "260/463, train_loss: 0.1158\n",
      "261/463, train_loss: 0.1271\n",
      "262/463, train_loss: 0.0452\n",
      "263/463, train_loss: 0.3613\n",
      "264/463, train_loss: 0.1246\n",
      "265/463, train_loss: 0.2888\n",
      "266/463, train_loss: 0.2329\n",
      "267/463, train_loss: 0.1509\n",
      "268/463, train_loss: 0.3196\n",
      "269/463, train_loss: 0.2805\n",
      "270/463, train_loss: 0.1735\n",
      "271/463, train_loss: 0.1345\n",
      "272/463, train_loss: 0.1406\n",
      "273/463, train_loss: 0.1422\n",
      "274/463, train_loss: 0.3625\n",
      "275/463, train_loss: 0.0701\n",
      "276/463, train_loss: 0.1455\n",
      "277/463, train_loss: 0.3813\n",
      "278/463, train_loss: 0.1228\n",
      "279/463, train_loss: 0.1141\n",
      "280/463, train_loss: 0.1276\n",
      "281/463, train_loss: 0.0981\n",
      "282/463, train_loss: 0.2423\n",
      "283/463, train_loss: 0.3447\n",
      "284/463, train_loss: 0.6025\n",
      "285/463, train_loss: 0.1854\n",
      "286/463, train_loss: 0.0696\n",
      "287/463, train_loss: 0.0937\n",
      "288/463, train_loss: 0.2336\n",
      "289/463, train_loss: 0.0999\n",
      "290/463, train_loss: 0.1851\n",
      "291/463, train_loss: 0.3372\n",
      "292/463, train_loss: 0.4897\n",
      "293/463, train_loss: 0.3894\n",
      "294/463, train_loss: 0.1913\n",
      "295/463, train_loss: 0.4661\n",
      "296/463, train_loss: 0.2115\n",
      "297/463, train_loss: 0.3403\n",
      "298/463, train_loss: 0.3198\n",
      "299/463, train_loss: 0.1412\n",
      "300/463, train_loss: 0.1393\n",
      "301/463, train_loss: 0.1266\n",
      "302/463, train_loss: 0.0914\n",
      "303/463, train_loss: 0.0703\n",
      "304/463, train_loss: 0.1562\n",
      "305/463, train_loss: 0.1885\n",
      "306/463, train_loss: 0.1274\n",
      "307/463, train_loss: 0.1814\n",
      "308/463, train_loss: 0.1050\n",
      "309/463, train_loss: 0.2397\n",
      "310/463, train_loss: 0.4849\n",
      "311/463, train_loss: 0.3301\n",
      "312/463, train_loss: 0.2283\n",
      "313/463, train_loss: 0.3257\n",
      "314/463, train_loss: 0.2246\n",
      "315/463, train_loss: 0.2151\n",
      "316/463, train_loss: 0.1295\n",
      "317/463, train_loss: 0.2257\n",
      "318/463, train_loss: 0.2068\n",
      "319/463, train_loss: 0.2742\n",
      "320/463, train_loss: 0.2720\n",
      "321/463, train_loss: 0.2354\n",
      "322/463, train_loss: 0.3477\n",
      "323/463, train_loss: 0.2744\n",
      "324/463, train_loss: 0.1444\n",
      "325/463, train_loss: 0.1525\n",
      "326/463, train_loss: 0.5259\n",
      "327/463, train_loss: 0.3252\n",
      "328/463, train_loss: 0.2300\n",
      "329/463, train_loss: 0.0754\n",
      "330/463, train_loss: 0.1897\n",
      "331/463, train_loss: 0.1702\n",
      "332/463, train_loss: 0.3057\n",
      "333/463, train_loss: 0.3940\n",
      "334/463, train_loss: 0.0799\n",
      "335/463, train_loss: 0.2499\n",
      "336/463, train_loss: 0.2510\n",
      "337/463, train_loss: 0.2983\n",
      "338/463, train_loss: 0.1390\n",
      "339/463, train_loss: 0.2061\n",
      "340/463, train_loss: 0.2329\n",
      "341/463, train_loss: 0.2969\n",
      "342/463, train_loss: 0.0909\n",
      "343/463, train_loss: 0.0709\n",
      "344/463, train_loss: 0.0628\n",
      "345/463, train_loss: 0.2327\n",
      "346/463, train_loss: 0.4844\n",
      "347/463, train_loss: 0.2317\n",
      "348/463, train_loss: 0.2152\n",
      "349/463, train_loss: 0.4575\n",
      "350/463, train_loss: 0.3091\n",
      "351/463, train_loss: 0.0981\n",
      "352/463, train_loss: 0.1117\n",
      "353/463, train_loss: 0.4141\n",
      "354/463, train_loss: 0.1106\n",
      "355/463, train_loss: 0.1389\n",
      "356/463, train_loss: 0.1166\n",
      "357/463, train_loss: 0.1118\n",
      "358/463, train_loss: 0.8257\n",
      "359/463, train_loss: 0.1587\n",
      "360/463, train_loss: 0.1930\n",
      "361/463, train_loss: 0.1179\n",
      "362/463, train_loss: 0.1460\n",
      "363/463, train_loss: 0.0858\n",
      "364/463, train_loss: 0.3684\n",
      "365/463, train_loss: 0.1512\n",
      "366/463, train_loss: 0.5918\n",
      "367/463, train_loss: 0.0842\n",
      "368/463, train_loss: 0.4053\n",
      "369/463, train_loss: 0.1282\n",
      "370/463, train_loss: 0.0619\n",
      "371/463, train_loss: 0.1904\n",
      "372/463, train_loss: 0.0518\n",
      "373/463, train_loss: 0.2871\n",
      "374/463, train_loss: 0.0681\n",
      "375/463, train_loss: 0.1149\n",
      "376/463, train_loss: 0.2520\n",
      "377/463, train_loss: 0.2195\n",
      "378/463, train_loss: 0.3835\n",
      "379/463, train_loss: 0.2159\n",
      "380/463, train_loss: 0.1521\n",
      "381/463, train_loss: 0.1594\n",
      "382/463, train_loss: 0.1560\n",
      "383/463, train_loss: 0.1339\n",
      "384/463, train_loss: 0.1097\n",
      "385/463, train_loss: 0.3374\n",
      "386/463, train_loss: 0.3208\n",
      "387/463, train_loss: 0.1338\n",
      "388/463, train_loss: 0.5830\n",
      "389/463, train_loss: 0.1117\n",
      "390/463, train_loss: 0.1317\n",
      "391/463, train_loss: 0.0474\n",
      "392/463, train_loss: 0.2406\n",
      "393/463, train_loss: 0.2229\n",
      "394/463, train_loss: 0.1299\n",
      "395/463, train_loss: 0.1195\n",
      "396/463, train_loss: 0.2805\n",
      "397/463, train_loss: 0.4460\n",
      "398/463, train_loss: 0.1444\n",
      "399/463, train_loss: 0.2156\n",
      "400/463, train_loss: 0.2944\n",
      "401/463, train_loss: 0.2996\n",
      "402/463, train_loss: 0.2126\n",
      "403/463, train_loss: 0.6338\n",
      "404/463, train_loss: 0.7666\n",
      "405/463, train_loss: 0.1788\n",
      "406/463, train_loss: 0.2388\n",
      "407/463, train_loss: 0.2115\n",
      "408/463, train_loss: 0.1321\n",
      "409/463, train_loss: 0.1400\n",
      "410/463, train_loss: 0.3018\n",
      "411/463, train_loss: 0.0777\n",
      "412/463, train_loss: 0.2004\n",
      "413/463, train_loss: 0.1719\n",
      "414/463, train_loss: 0.5933\n",
      "415/463, train_loss: 0.1252\n",
      "416/463, train_loss: 0.4419\n",
      "417/463, train_loss: 0.1093\n",
      "418/463, train_loss: 0.3879\n",
      "419/463, train_loss: 0.1705\n",
      "420/463, train_loss: 0.0913\n",
      "421/463, train_loss: 0.3774\n",
      "422/463, train_loss: 0.1659\n",
      "423/463, train_loss: 0.1600\n",
      "424/463, train_loss: 0.1509\n",
      "425/463, train_loss: 0.3423\n",
      "426/463, train_loss: 0.1131\n",
      "427/463, train_loss: 0.1482\n",
      "428/463, train_loss: 0.1752\n",
      "429/463, train_loss: 0.1807\n",
      "430/463, train_loss: 0.1440\n",
      "431/463, train_loss: 0.1085\n",
      "432/463, train_loss: 0.1230\n",
      "433/463, train_loss: 0.1719\n",
      "434/463, train_loss: 0.4165\n",
      "435/463, train_loss: 0.3499\n",
      "436/463, train_loss: 0.1193\n",
      "437/463, train_loss: 0.1489\n",
      "438/463, train_loss: 0.0894\n",
      "439/463, train_loss: 0.0858\n",
      "440/463, train_loss: 0.1141\n",
      "441/463, train_loss: 0.2139\n",
      "442/463, train_loss: 0.2178\n",
      "443/463, train_loss: 0.2717\n",
      "444/463, train_loss: 0.1429\n",
      "445/463, train_loss: 0.0939\n",
      "446/463, train_loss: 0.3784\n",
      "447/463, train_loss: 0.3496\n",
      "448/463, train_loss: 0.2520\n",
      "449/463, train_loss: 0.1403\n",
      "450/463, train_loss: 0.5576\n",
      "451/463, train_loss: 0.2313\n",
      "452/463, train_loss: 0.1395\n",
      "453/463, train_loss: 0.1560\n",
      "454/463, train_loss: 0.1748\n",
      "455/463, train_loss: 0.3374\n",
      "456/463, train_loss: 0.4604\n",
      "457/463, train_loss: 0.3665\n",
      "458/463, train_loss: 0.0932\n",
      "459/463, train_loss: 0.0627\n",
      "460/463, train_loss: 0.0929\n",
      "461/463, train_loss: 0.1132\n",
      "462/463, train_loss: 0.3735\n",
      "463/463, train_loss: 0.3623\n",
      "epoch 14 average loss: 0.2283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 12:16:02 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 12:16:05 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 12:16:08 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 15/100\n",
      "1/463, train_loss: 0.1266\n",
      "2/463, train_loss: 0.1359\n",
      "3/463, train_loss: 0.3665\n",
      "4/463, train_loss: 0.4141\n",
      "5/463, train_loss: 0.0864\n",
      "6/463, train_loss: 0.1405\n",
      "7/463, train_loss: 0.2839\n",
      "8/463, train_loss: 0.3123\n",
      "9/463, train_loss: 0.4480\n",
      "10/463, train_loss: 0.1692\n",
      "11/463, train_loss: 0.1501\n",
      "12/463, train_loss: 0.2554\n",
      "13/463, train_loss: 0.3093\n",
      "14/463, train_loss: 0.1176\n",
      "15/463, train_loss: 0.8687\n",
      "16/463, train_loss: 0.1931\n",
      "17/463, train_loss: 0.1204\n",
      "18/463, train_loss: 0.0941\n",
      "19/463, train_loss: 0.5449\n",
      "20/463, train_loss: 0.1285\n",
      "21/463, train_loss: 0.2146\n",
      "22/463, train_loss: 0.2405\n",
      "23/463, train_loss: 0.1259\n",
      "24/463, train_loss: 0.3164\n",
      "25/463, train_loss: 0.3020\n",
      "26/463, train_loss: 0.0956\n",
      "27/463, train_loss: 0.1624\n",
      "28/463, train_loss: 0.2081\n",
      "29/463, train_loss: 0.2068\n",
      "30/463, train_loss: 0.4116\n",
      "31/463, train_loss: 0.2151\n",
      "32/463, train_loss: 0.2152\n",
      "33/463, train_loss: 0.1346\n",
      "34/463, train_loss: 0.1039\n",
      "35/463, train_loss: 0.1230\n",
      "36/463, train_loss: 0.3716\n",
      "37/463, train_loss: 0.1631\n",
      "38/463, train_loss: 0.0938\n",
      "39/463, train_loss: 0.3652\n",
      "40/463, train_loss: 0.1526\n",
      "41/463, train_loss: 0.1399\n",
      "42/463, train_loss: 0.1223\n",
      "43/463, train_loss: 0.1224\n",
      "44/463, train_loss: 0.2070\n",
      "45/463, train_loss: 0.2229\n",
      "46/463, train_loss: 0.1753\n",
      "47/463, train_loss: 0.1821\n",
      "48/463, train_loss: 0.1736\n",
      "49/463, train_loss: 0.0977\n",
      "50/463, train_loss: 0.1895\n",
      "51/463, train_loss: 0.0994\n",
      "52/463, train_loss: 0.3354\n",
      "53/463, train_loss: 0.1453\n",
      "54/463, train_loss: 0.1521\n",
      "55/463, train_loss: 0.0969\n",
      "56/463, train_loss: 0.6909\n",
      "57/463, train_loss: 0.3472\n",
      "58/463, train_loss: 0.0854\n",
      "59/463, train_loss: 0.1135\n",
      "60/463, train_loss: 0.1088\n",
      "61/463, train_loss: 0.1982\n",
      "62/463, train_loss: 0.1498\n",
      "63/463, train_loss: 0.1230\n",
      "64/463, train_loss: 0.5073\n",
      "65/463, train_loss: 0.4368\n",
      "66/463, train_loss: 0.0856\n",
      "67/463, train_loss: 0.1144\n",
      "68/463, train_loss: 0.4663\n",
      "69/463, train_loss: 0.1238\n",
      "70/463, train_loss: 0.1874\n",
      "71/463, train_loss: 0.1018\n",
      "72/463, train_loss: 0.1047\n",
      "73/463, train_loss: 0.1429\n",
      "74/463, train_loss: 0.1571\n",
      "75/463, train_loss: 0.1943\n",
      "76/463, train_loss: 0.1318\n",
      "77/463, train_loss: 0.2335\n",
      "78/463, train_loss: 0.3909\n",
      "79/463, train_loss: 0.1306\n",
      "80/463, train_loss: 0.0474\n",
      "81/463, train_loss: 0.2856\n",
      "82/463, train_loss: 0.1182\n",
      "83/463, train_loss: 0.0844\n",
      "84/463, train_loss: 0.2588\n",
      "85/463, train_loss: 0.1520\n",
      "86/463, train_loss: 0.4272\n",
      "87/463, train_loss: 0.2156\n",
      "88/463, train_loss: 0.0974\n",
      "89/463, train_loss: 0.0850\n",
      "90/463, train_loss: 0.1003\n",
      "91/463, train_loss: 0.1853\n",
      "92/463, train_loss: 0.1553\n",
      "93/463, train_loss: 0.0464\n",
      "94/463, train_loss: 0.3674\n",
      "95/463, train_loss: 0.3220\n",
      "96/463, train_loss: 0.1062\n",
      "97/463, train_loss: 0.1702\n",
      "98/463, train_loss: 0.0986\n",
      "99/463, train_loss: 0.0966\n",
      "100/463, train_loss: 0.5547\n",
      "101/463, train_loss: 0.2637\n",
      "102/463, train_loss: 0.1577\n",
      "103/463, train_loss: 0.0947\n",
      "104/463, train_loss: 0.0722\n",
      "105/463, train_loss: 0.0948\n",
      "106/463, train_loss: 0.5625\n",
      "107/463, train_loss: 0.2126\n",
      "108/463, train_loss: 0.1499\n",
      "109/463, train_loss: 0.1852\n",
      "110/463, train_loss: 0.1707\n",
      "111/463, train_loss: 0.1616\n",
      "112/463, train_loss: 0.1262\n",
      "113/463, train_loss: 0.1177\n",
      "114/463, train_loss: 0.1011\n",
      "115/463, train_loss: 0.1018\n",
      "116/463, train_loss: 0.0729\n",
      "117/463, train_loss: 0.1940\n",
      "118/463, train_loss: 0.1863\n",
      "119/463, train_loss: 0.5059\n",
      "120/463, train_loss: 0.2705\n",
      "121/463, train_loss: 0.0848\n",
      "122/463, train_loss: 0.2141\n",
      "123/463, train_loss: 0.2278\n",
      "124/463, train_loss: 0.1406\n",
      "125/463, train_loss: 0.4331\n",
      "126/463, train_loss: 0.2922\n",
      "127/463, train_loss: 0.1215\n",
      "128/463, train_loss: 0.4019\n",
      "129/463, train_loss: 0.0759\n",
      "130/463, train_loss: 0.1150\n",
      "131/463, train_loss: 0.5522\n",
      "132/463, train_loss: 0.1118\n",
      "133/463, train_loss: 0.1627\n",
      "134/463, train_loss: 0.0712\n",
      "135/463, train_loss: 0.3899\n",
      "136/463, train_loss: 0.3528\n",
      "137/463, train_loss: 0.0370\n",
      "138/463, train_loss: 0.2303\n",
      "139/463, train_loss: 0.0418\n",
      "140/463, train_loss: 0.2300\n",
      "141/463, train_loss: 0.3508\n",
      "142/463, train_loss: 0.1173\n",
      "143/463, train_loss: 0.1704\n",
      "144/463, train_loss: 0.1317\n",
      "145/463, train_loss: 0.0958\n",
      "146/463, train_loss: 0.1907\n",
      "147/463, train_loss: 0.2146\n",
      "148/463, train_loss: 0.0623\n",
      "149/463, train_loss: 0.2261\n",
      "150/463, train_loss: 0.7554\n",
      "151/463, train_loss: 0.1538\n",
      "152/463, train_loss: 0.0984\n",
      "153/463, train_loss: 0.2363\n",
      "154/463, train_loss: 0.1407\n",
      "155/463, train_loss: 0.1344\n",
      "156/463, train_loss: 0.7173\n",
      "157/463, train_loss: 0.1414\n",
      "158/463, train_loss: 0.1974\n",
      "159/463, train_loss: 0.1689\n",
      "160/463, train_loss: 0.5903\n",
      "161/463, train_loss: 0.1707\n",
      "162/463, train_loss: 0.1664\n",
      "163/463, train_loss: 0.3457\n",
      "164/463, train_loss: 0.2194\n",
      "165/463, train_loss: 0.3872\n",
      "166/463, train_loss: 0.1516\n",
      "167/463, train_loss: 0.0782\n",
      "168/463, train_loss: 0.2391\n",
      "169/463, train_loss: 0.1265\n",
      "170/463, train_loss: 0.0957\n",
      "171/463, train_loss: 0.1458\n",
      "172/463, train_loss: 0.1307\n",
      "173/463, train_loss: 0.1670\n",
      "174/463, train_loss: 0.1826\n",
      "175/463, train_loss: 0.3933\n",
      "176/463, train_loss: 0.1766\n",
      "177/463, train_loss: 0.1776\n",
      "178/463, train_loss: 0.0972\n",
      "179/463, train_loss: 0.4031\n",
      "180/463, train_loss: 0.3025\n",
      "181/463, train_loss: 0.1638\n",
      "182/463, train_loss: 0.0735\n",
      "183/463, train_loss: 0.2036\n",
      "184/463, train_loss: 0.1893\n",
      "185/463, train_loss: 0.1543\n",
      "186/463, train_loss: 0.0858\n",
      "187/463, train_loss: 0.4102\n",
      "188/463, train_loss: 0.3037\n",
      "189/463, train_loss: 0.0658\n",
      "190/463, train_loss: 0.4246\n",
      "191/463, train_loss: 0.1368\n",
      "192/463, train_loss: 0.1162\n",
      "193/463, train_loss: 0.3921\n",
      "194/463, train_loss: 0.1816\n",
      "195/463, train_loss: 0.2454\n",
      "196/463, train_loss: 0.2876\n",
      "197/463, train_loss: 0.0773\n",
      "198/463, train_loss: 0.2134\n",
      "199/463, train_loss: 0.1975\n",
      "200/463, train_loss: 0.2283\n",
      "201/463, train_loss: 0.1927\n",
      "202/463, train_loss: 0.0343\n",
      "203/463, train_loss: 0.3391\n",
      "204/463, train_loss: 0.3696\n",
      "205/463, train_loss: 1.0938\n",
      "206/463, train_loss: 0.3188\n",
      "207/463, train_loss: 0.2198\n",
      "208/463, train_loss: 0.1882\n",
      "209/463, train_loss: 0.2764\n",
      "210/463, train_loss: 0.0725\n",
      "211/463, train_loss: 0.2014\n",
      "212/463, train_loss: 0.1200\n",
      "213/463, train_loss: 0.2871\n",
      "214/463, train_loss: 0.1035\n",
      "215/463, train_loss: 0.1653\n",
      "216/463, train_loss: 0.2458\n",
      "217/463, train_loss: 0.1047\n",
      "218/463, train_loss: 0.2399\n",
      "219/463, train_loss: 0.2113\n",
      "220/463, train_loss: 0.2598\n",
      "221/463, train_loss: 0.0891\n",
      "222/463, train_loss: 0.0667\n",
      "223/463, train_loss: 0.3120\n",
      "224/463, train_loss: 0.1995\n",
      "225/463, train_loss: 0.3977\n",
      "226/463, train_loss: 0.1100\n",
      "227/463, train_loss: 0.0907\n",
      "228/463, train_loss: 0.1516\n",
      "229/463, train_loss: 0.2107\n",
      "230/463, train_loss: 0.7446\n",
      "231/463, train_loss: 0.1086\n",
      "232/463, train_loss: 0.1633\n",
      "233/463, train_loss: 0.4307\n",
      "234/463, train_loss: 0.0765\n",
      "235/463, train_loss: 0.0833\n",
      "236/463, train_loss: 0.2451\n",
      "237/463, train_loss: 0.1674\n",
      "238/463, train_loss: 0.2837\n",
      "239/463, train_loss: 0.0892\n",
      "240/463, train_loss: 0.3879\n",
      "241/463, train_loss: 0.1523\n",
      "242/463, train_loss: 0.6455\n",
      "243/463, train_loss: 0.1832\n",
      "244/463, train_loss: 0.3516\n",
      "245/463, train_loss: 0.3579\n",
      "246/463, train_loss: 0.3521\n",
      "247/463, train_loss: 0.0730\n",
      "248/463, train_loss: 0.5537\n",
      "249/463, train_loss: 0.2815\n",
      "250/463, train_loss: 0.1547\n",
      "251/463, train_loss: 0.3804\n",
      "252/463, train_loss: 0.1089\n",
      "253/463, train_loss: 0.1741\n",
      "254/463, train_loss: 0.1523\n",
      "255/463, train_loss: 0.2023\n",
      "256/463, train_loss: 0.0778\n",
      "257/463, train_loss: 0.0859\n",
      "258/463, train_loss: 0.1757\n",
      "259/463, train_loss: 0.0626\n",
      "260/463, train_loss: 0.0836\n",
      "261/463, train_loss: 0.1084\n",
      "262/463, train_loss: 0.0841\n",
      "263/463, train_loss: 0.0518\n",
      "264/463, train_loss: 0.6367\n",
      "265/463, train_loss: 0.1131\n",
      "266/463, train_loss: 0.1934\n",
      "267/463, train_loss: 0.1946\n",
      "268/463, train_loss: 0.5640\n",
      "269/463, train_loss: 0.0781\n",
      "270/463, train_loss: 0.3096\n",
      "271/463, train_loss: 0.2034\n",
      "272/463, train_loss: 0.2642\n",
      "273/463, train_loss: 0.1587\n",
      "274/463, train_loss: 0.2390\n",
      "275/463, train_loss: 0.2455\n",
      "276/463, train_loss: 0.0492\n",
      "277/463, train_loss: 0.5078\n",
      "278/463, train_loss: 0.3044\n",
      "279/463, train_loss: 0.2749\n",
      "280/463, train_loss: 0.1338\n",
      "281/463, train_loss: 0.1965\n",
      "282/463, train_loss: 0.4785\n",
      "283/463, train_loss: 0.1628\n",
      "284/463, train_loss: 0.4919\n",
      "285/463, train_loss: 0.1735\n",
      "286/463, train_loss: 0.1018\n",
      "287/463, train_loss: 0.3677\n",
      "288/463, train_loss: 0.1860\n",
      "289/463, train_loss: 0.1722\n",
      "290/463, train_loss: 0.1388\n",
      "291/463, train_loss: 0.1270\n",
      "292/463, train_loss: 0.2971\n",
      "293/463, train_loss: 0.0771\n",
      "294/463, train_loss: 0.0979\n",
      "295/463, train_loss: 0.1161\n",
      "296/463, train_loss: 0.3315\n",
      "297/463, train_loss: 0.0717\n",
      "298/463, train_loss: 0.9512\n",
      "299/463, train_loss: 0.1467\n",
      "300/463, train_loss: 0.0915\n",
      "301/463, train_loss: 0.1830\n",
      "302/463, train_loss: 0.0928\n",
      "303/463, train_loss: 0.2468\n",
      "304/463, train_loss: 0.4812\n",
      "305/463, train_loss: 0.1332\n",
      "306/463, train_loss: 0.4707\n",
      "307/463, train_loss: 0.1740\n",
      "308/463, train_loss: 0.2734\n",
      "309/463, train_loss: 0.4287\n",
      "310/463, train_loss: 0.3003\n",
      "311/463, train_loss: 0.0803\n",
      "312/463, train_loss: 0.1489\n",
      "313/463, train_loss: 0.2720\n",
      "314/463, train_loss: 0.1119\n",
      "315/463, train_loss: 0.3230\n",
      "316/463, train_loss: 0.1359\n",
      "317/463, train_loss: 0.4072\n",
      "318/463, train_loss: 0.1797\n",
      "319/463, train_loss: 0.4041\n",
      "320/463, train_loss: 0.2620\n",
      "321/463, train_loss: 0.1367\n",
      "322/463, train_loss: 0.2102\n",
      "323/463, train_loss: 0.3506\n",
      "324/463, train_loss: 0.3281\n",
      "325/463, train_loss: 0.1976\n",
      "326/463, train_loss: 0.1929\n",
      "327/463, train_loss: 0.1410\n",
      "328/463, train_loss: 0.2964\n",
      "329/463, train_loss: 0.6460\n",
      "330/463, train_loss: 0.2573\n",
      "331/463, train_loss: 0.1514\n",
      "332/463, train_loss: 0.1526\n",
      "333/463, train_loss: 0.5234\n",
      "334/463, train_loss: 0.2256\n",
      "335/463, train_loss: 0.1766\n",
      "336/463, train_loss: 0.3462\n",
      "337/463, train_loss: 0.6084\n",
      "338/463, train_loss: 0.1392\n",
      "339/463, train_loss: 0.1353\n",
      "340/463, train_loss: 0.3499\n",
      "341/463, train_loss: 0.5352\n",
      "342/463, train_loss: 0.2170\n",
      "343/463, train_loss: 0.2036\n",
      "344/463, train_loss: 0.0756\n",
      "345/463, train_loss: 0.0865\n",
      "346/463, train_loss: 0.1786\n",
      "347/463, train_loss: 0.1534\n",
      "348/463, train_loss: 0.3799\n",
      "349/463, train_loss: 0.0909\n",
      "350/463, train_loss: 0.0818\n",
      "351/463, train_loss: 0.1580\n",
      "352/463, train_loss: 0.1987\n",
      "353/463, train_loss: 0.1383\n",
      "354/463, train_loss: 0.8042\n",
      "355/463, train_loss: 0.3621\n",
      "356/463, train_loss: 0.2267\n",
      "357/463, train_loss: 0.0975\n",
      "358/463, train_loss: 0.3525\n",
      "359/463, train_loss: 0.1766\n",
      "360/463, train_loss: 0.2427\n",
      "361/463, train_loss: 0.3442\n",
      "362/463, train_loss: 0.1785\n",
      "363/463, train_loss: 0.1566\n",
      "364/463, train_loss: 0.1375\n",
      "365/463, train_loss: 0.1605\n",
      "366/463, train_loss: 0.3286\n",
      "367/463, train_loss: 0.1210\n",
      "368/463, train_loss: 0.2351\n",
      "369/463, train_loss: 0.4221\n",
      "370/463, train_loss: 0.1176\n",
      "371/463, train_loss: 0.1445\n",
      "372/463, train_loss: 0.2617\n",
      "373/463, train_loss: 0.1287\n",
      "374/463, train_loss: 0.2078\n",
      "375/463, train_loss: 0.2893\n",
      "376/463, train_loss: 0.2004\n",
      "377/463, train_loss: 0.1232\n",
      "378/463, train_loss: 0.1277\n",
      "379/463, train_loss: 0.2771\n",
      "380/463, train_loss: 0.1023\n",
      "381/463, train_loss: 0.0874\n",
      "382/463, train_loss: 0.1425\n",
      "383/463, train_loss: 0.1299\n",
      "384/463, train_loss: 0.7051\n",
      "385/463, train_loss: 0.4502\n",
      "386/463, train_loss: 0.1897\n",
      "387/463, train_loss: 0.1227\n",
      "388/463, train_loss: 0.2168\n",
      "389/463, train_loss: 0.1227\n",
      "390/463, train_loss: 0.1069\n",
      "391/463, train_loss: 0.8164\n",
      "392/463, train_loss: 0.1381\n",
      "393/463, train_loss: 0.2517\n",
      "394/463, train_loss: 0.1993\n",
      "395/463, train_loss: 0.1512\n",
      "396/463, train_loss: 0.1749\n",
      "397/463, train_loss: 0.1094\n",
      "398/463, train_loss: 0.1372\n",
      "399/463, train_loss: 0.2288\n",
      "400/463, train_loss: 0.0980\n",
      "401/463, train_loss: 0.1041\n",
      "402/463, train_loss: 0.1389\n",
      "403/463, train_loss: 0.4055\n",
      "404/463, train_loss: 0.1019\n",
      "405/463, train_loss: 0.4097\n",
      "406/463, train_loss: 0.0734\n",
      "407/463, train_loss: 0.1658\n",
      "408/463, train_loss: 0.2383\n",
      "409/463, train_loss: 0.2355\n",
      "410/463, train_loss: 0.1591\n",
      "411/463, train_loss: 0.1505\n",
      "412/463, train_loss: 0.1035\n",
      "413/463, train_loss: 0.2664\n",
      "414/463, train_loss: 0.0925\n",
      "415/463, train_loss: 0.1791\n",
      "416/463, train_loss: 0.1479\n",
      "417/463, train_loss: 0.3596\n",
      "418/463, train_loss: 0.4541\n",
      "419/463, train_loss: 0.2930\n",
      "420/463, train_loss: 0.1136\n",
      "421/463, train_loss: 0.3022\n",
      "422/463, train_loss: 0.2229\n",
      "423/463, train_loss: 0.2030\n",
      "424/463, train_loss: 0.0853\n",
      "425/463, train_loss: 0.0706\n",
      "426/463, train_loss: 0.3477\n",
      "427/463, train_loss: 0.1279\n",
      "428/463, train_loss: 0.4805\n",
      "429/463, train_loss: 0.3091\n",
      "430/463, train_loss: 0.1677\n",
      "431/463, train_loss: 0.3994\n",
      "432/463, train_loss: 0.1553\n",
      "433/463, train_loss: 0.6108\n",
      "434/463, train_loss: 0.1057\n",
      "435/463, train_loss: 0.2478\n",
      "436/463, train_loss: 0.2140\n",
      "437/463, train_loss: 0.1090\n",
      "438/463, train_loss: 0.2239\n",
      "439/463, train_loss: 0.1079\n",
      "440/463, train_loss: 0.2898\n",
      "441/463, train_loss: 0.1791\n",
      "442/463, train_loss: 0.1980\n",
      "443/463, train_loss: 0.2072\n",
      "444/463, train_loss: 0.1526\n",
      "445/463, train_loss: 0.1841\n",
      "446/463, train_loss: 0.1824\n",
      "447/463, train_loss: 0.4873\n",
      "448/463, train_loss: 0.0784\n",
      "449/463, train_loss: 0.7744\n",
      "450/463, train_loss: 0.1216\n",
      "451/463, train_loss: 0.1812\n",
      "452/463, train_loss: 0.1343\n",
      "453/463, train_loss: 0.1338\n",
      "454/463, train_loss: 0.1357\n",
      "455/463, train_loss: 0.0996\n",
      "456/463, train_loss: 0.3889\n",
      "457/463, train_loss: 0.2551\n",
      "458/463, train_loss: 0.1631\n",
      "459/463, train_loss: 0.1052\n",
      "460/463, train_loss: 0.1414\n",
      "461/463, train_loss: 0.0936\n",
      "462/463, train_loss: 0.0908\n",
      "463/463, train_loss: 0.3770\n",
      "epoch 15 average loss: 0.2246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 14:29:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 14:29:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 14:29:17 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mixed/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630.nii.gz\n",
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.5624461183273228, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.5624461183273228, 'AP_IoU_0.10_MaxDet_100': 0.6213164951762941, 'nodule_AP_IoU_0.10_MaxDet_100': 0.6213164951762941, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8803418874740601, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8803418874740601, 'AR_IoU_0.10_MaxDet_100': 0.9230769276618958, 'nodule_AR_IoU_0.10_MaxDet_100': 0.9230769276618958}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 14:45:38 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 14:45:41 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 14:45:43 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current epoch: 15 current metric: 0.7468 best metric: 0.7468 at epoch 15\n",
      "----------\n",
      "epoch 16/100\n",
      "1/463, train_loss: 0.3994\n",
      "2/463, train_loss: 0.2393\n",
      "3/463, train_loss: 0.1597\n",
      "4/463, train_loss: 0.1096\n",
      "5/463, train_loss: 0.0835\n",
      "6/463, train_loss: 0.3064\n",
      "7/463, train_loss: 0.1283\n",
      "8/463, train_loss: 0.1221\n",
      "9/463, train_loss: 0.1230\n",
      "10/463, train_loss: 0.0414\n",
      "11/463, train_loss: 0.1079\n",
      "12/463, train_loss: 0.2241\n",
      "13/463, train_loss: 0.3784\n",
      "14/463, train_loss: 0.0803\n",
      "15/463, train_loss: 0.4583\n",
      "16/463, train_loss: 0.9297\n",
      "17/463, train_loss: 0.2019\n",
      "18/463, train_loss: 0.0880\n",
      "19/463, train_loss: 0.0959\n",
      "20/463, train_loss: 0.1978\n",
      "21/463, train_loss: 0.2834\n",
      "22/463, train_loss: 0.1646\n",
      "23/463, train_loss: 0.1382\n",
      "24/463, train_loss: 0.0855\n",
      "25/463, train_loss: 0.1082\n",
      "26/463, train_loss: 0.2598\n",
      "27/463, train_loss: 0.1271\n",
      "28/463, train_loss: 0.3860\n",
      "29/463, train_loss: 0.1842\n",
      "30/463, train_loss: 0.0809\n",
      "31/463, train_loss: 0.2280\n",
      "32/463, train_loss: 0.1103\n",
      "33/463, train_loss: 0.1852\n",
      "34/463, train_loss: 0.1923\n",
      "35/463, train_loss: 0.1541\n",
      "36/463, train_loss: 0.0563\n",
      "37/463, train_loss: 0.1790\n",
      "38/463, train_loss: 0.3367\n",
      "39/463, train_loss: 0.3604\n",
      "40/463, train_loss: 0.0634\n",
      "41/463, train_loss: 0.4729\n",
      "42/463, train_loss: 0.0962\n",
      "43/463, train_loss: 0.6250\n",
      "44/463, train_loss: 0.3665\n",
      "45/463, train_loss: 0.0491\n",
      "46/463, train_loss: 0.1454\n",
      "47/463, train_loss: 0.1116\n",
      "48/463, train_loss: 0.1611\n",
      "49/463, train_loss: 0.0826\n",
      "50/463, train_loss: 0.1187\n",
      "51/463, train_loss: 0.2668\n",
      "52/463, train_loss: 0.1736\n",
      "53/463, train_loss: 0.2688\n",
      "54/463, train_loss: 0.3054\n",
      "55/463, train_loss: 0.4998\n",
      "56/463, train_loss: 0.0995\n",
      "57/463, train_loss: 0.1987\n",
      "58/463, train_loss: 0.2231\n",
      "59/463, train_loss: 0.1849\n",
      "60/463, train_loss: 0.1716\n",
      "61/463, train_loss: 0.1055\n",
      "62/463, train_loss: 0.0485\n",
      "63/463, train_loss: 0.2151\n",
      "64/463, train_loss: 0.1426\n",
      "65/463, train_loss: 0.3369\n",
      "66/463, train_loss: 0.1082\n",
      "67/463, train_loss: 0.1573\n",
      "68/463, train_loss: 0.2998\n",
      "69/463, train_loss: 0.1405\n",
      "70/463, train_loss: 0.1522\n",
      "71/463, train_loss: 0.1181\n",
      "72/463, train_loss: 0.1296\n",
      "73/463, train_loss: 0.1816\n",
      "74/463, train_loss: 0.1160\n",
      "75/463, train_loss: 0.1025\n",
      "76/463, train_loss: 0.1393\n",
      "77/463, train_loss: 0.5156\n",
      "78/463, train_loss: 0.1028\n",
      "79/463, train_loss: 0.2976\n",
      "80/463, train_loss: 0.1095\n",
      "81/463, train_loss: 0.5015\n",
      "82/463, train_loss: 0.1207\n",
      "83/463, train_loss: 0.4338\n",
      "84/463, train_loss: 0.3503\n",
      "85/463, train_loss: 0.1707\n",
      "86/463, train_loss: 0.1517\n",
      "87/463, train_loss: 0.0932\n",
      "88/463, train_loss: 0.1237\n",
      "89/463, train_loss: 0.4070\n",
      "90/463, train_loss: 0.1418\n",
      "91/463, train_loss: 0.1184\n",
      "92/463, train_loss: 0.1144\n",
      "93/463, train_loss: 0.3237\n",
      "94/463, train_loss: 0.1392\n",
      "95/463, train_loss: 0.1081\n",
      "96/463, train_loss: 0.1163\n",
      "97/463, train_loss: 0.3105\n",
      "98/463, train_loss: 0.2212\n",
      "99/463, train_loss: 0.1173\n",
      "100/463, train_loss: 0.1296\n",
      "101/463, train_loss: 0.1713\n",
      "102/463, train_loss: 0.1135\n",
      "103/463, train_loss: 0.1501\n",
      "104/463, train_loss: 0.1652\n",
      "105/463, train_loss: 0.0950\n",
      "106/463, train_loss: 0.1338\n",
      "107/463, train_loss: 0.0717\n",
      "108/463, train_loss: 0.2959\n",
      "109/463, train_loss: 0.1589\n",
      "110/463, train_loss: 0.2769\n",
      "111/463, train_loss: 0.1829\n",
      "112/463, train_loss: 0.1310\n",
      "113/463, train_loss: 0.1224\n",
      "114/463, train_loss: 0.1260\n",
      "115/463, train_loss: 0.2421\n",
      "116/463, train_loss: 0.0985\n",
      "117/463, train_loss: 0.5010\n",
      "118/463, train_loss: 0.1453\n",
      "119/463, train_loss: 0.0515\n",
      "120/463, train_loss: 0.1394\n",
      "121/463, train_loss: 0.1372\n",
      "122/463, train_loss: 0.1420\n",
      "123/463, train_loss: 0.4443\n",
      "124/463, train_loss: 0.1639\n",
      "125/463, train_loss: 0.1345\n",
      "126/463, train_loss: 0.1119\n",
      "127/463, train_loss: 0.1659\n",
      "128/463, train_loss: 0.3848\n",
      "129/463, train_loss: 0.1211\n",
      "130/463, train_loss: 0.1588\n",
      "131/463, train_loss: 0.5107\n",
      "132/463, train_loss: 0.1376\n",
      "133/463, train_loss: 0.0858\n",
      "134/463, train_loss: 0.0793\n",
      "135/463, train_loss: 0.7427\n",
      "136/463, train_loss: 0.3901\n",
      "137/463, train_loss: 0.2048\n",
      "138/463, train_loss: 0.3098\n",
      "139/463, train_loss: 0.1587\n",
      "140/463, train_loss: 0.3721\n",
      "141/463, train_loss: 0.1252\n",
      "142/463, train_loss: 0.2583\n",
      "143/463, train_loss: 0.0573\n",
      "144/463, train_loss: 0.0774\n",
      "145/463, train_loss: 0.1098\n",
      "146/463, train_loss: 0.1443\n",
      "147/463, train_loss: 0.5391\n",
      "148/463, train_loss: 0.2656\n",
      "149/463, train_loss: 0.1769\n",
      "150/463, train_loss: 0.3965\n",
      "151/463, train_loss: 0.2457\n",
      "152/463, train_loss: 0.3953\n",
      "153/463, train_loss: 0.1183\n",
      "154/463, train_loss: 0.0918\n",
      "155/463, train_loss: 0.1227\n",
      "156/463, train_loss: 0.1562\n",
      "157/463, train_loss: 0.1960\n",
      "158/463, train_loss: 0.2278\n",
      "159/463, train_loss: 0.1387\n",
      "160/463, train_loss: 0.1121\n",
      "161/463, train_loss: 0.1199\n",
      "162/463, train_loss: 0.1539\n",
      "163/463, train_loss: 0.1061\n",
      "164/463, train_loss: 0.3989\n",
      "165/463, train_loss: 0.1327\n",
      "166/463, train_loss: 0.3369\n",
      "167/463, train_loss: 0.5269\n",
      "168/463, train_loss: 0.1337\n",
      "169/463, train_loss: 0.3623\n",
      "170/463, train_loss: 0.0656\n",
      "171/463, train_loss: 0.1167\n",
      "172/463, train_loss: 0.0260\n",
      "173/463, train_loss: 0.2056\n",
      "174/463, train_loss: 0.2075\n",
      "175/463, train_loss: 0.3252\n",
      "176/463, train_loss: 0.1641\n",
      "177/463, train_loss: 0.1356\n",
      "178/463, train_loss: 0.1036\n",
      "179/463, train_loss: 0.0653\n",
      "180/463, train_loss: 0.4424\n",
      "181/463, train_loss: 0.0549\n",
      "182/463, train_loss: 0.2150\n",
      "183/463, train_loss: 0.1349\n",
      "184/463, train_loss: 0.4258\n",
      "185/463, train_loss: 0.1624\n",
      "186/463, train_loss: 0.1278\n",
      "187/463, train_loss: 0.1071\n",
      "188/463, train_loss: 0.4451\n",
      "189/463, train_loss: 0.0764\n",
      "190/463, train_loss: 0.1929\n",
      "191/463, train_loss: 0.0733\n",
      "192/463, train_loss: 0.1241\n",
      "193/463, train_loss: 0.1995\n",
      "194/463, train_loss: 0.1250\n",
      "195/463, train_loss: 0.1562\n",
      "196/463, train_loss: 0.3203\n",
      "197/463, train_loss: 0.4841\n",
      "198/463, train_loss: 0.5205\n",
      "199/463, train_loss: 0.1218\n",
      "200/463, train_loss: 0.3613\n",
      "201/463, train_loss: 0.3562\n",
      "202/463, train_loss: 0.1785\n",
      "203/463, train_loss: 0.0851\n",
      "204/463, train_loss: 0.1328\n",
      "205/463, train_loss: 0.3330\n",
      "206/463, train_loss: 0.1459\n",
      "207/463, train_loss: 0.5132\n",
      "208/463, train_loss: 0.2048\n",
      "209/463, train_loss: 0.3337\n",
      "210/463, train_loss: 0.1697\n",
      "211/463, train_loss: 0.1054\n",
      "212/463, train_loss: 0.0986\n",
      "213/463, train_loss: 0.4146\n",
      "214/463, train_loss: 0.1195\n",
      "215/463, train_loss: 0.2021\n",
      "216/463, train_loss: 0.4585\n",
      "217/463, train_loss: 0.2737\n",
      "218/463, train_loss: 0.1927\n",
      "219/463, train_loss: 0.8496\n",
      "220/463, train_loss: 0.7822\n",
      "221/463, train_loss: 0.0512\n",
      "222/463, train_loss: 0.3394\n",
      "223/463, train_loss: 0.5527\n",
      "224/463, train_loss: 0.1888\n",
      "225/463, train_loss: 0.1117\n",
      "226/463, train_loss: 0.2979\n",
      "227/463, train_loss: 0.1179\n",
      "228/463, train_loss: 0.1727\n",
      "229/463, train_loss: 0.1593\n",
      "230/463, train_loss: 0.2112\n",
      "231/463, train_loss: 0.1190\n",
      "232/463, train_loss: 0.6660\n",
      "233/463, train_loss: 0.1343\n",
      "234/463, train_loss: 0.4443\n",
      "235/463, train_loss: 0.1600\n",
      "236/463, train_loss: 0.2996\n",
      "237/463, train_loss: 1.1797\n",
      "238/463, train_loss: 0.3142\n",
      "239/463, train_loss: 0.1431\n",
      "240/463, train_loss: 0.2386\n",
      "241/463, train_loss: 0.2598\n",
      "242/463, train_loss: 0.1906\n",
      "243/463, train_loss: 0.3816\n",
      "244/463, train_loss: 0.5586\n",
      "245/463, train_loss: 0.0933\n",
      "246/463, train_loss: 0.1103\n",
      "247/463, train_loss: 0.1423\n",
      "248/463, train_loss: 0.1243\n",
      "249/463, train_loss: 0.2491\n",
      "250/463, train_loss: 0.1582\n",
      "251/463, train_loss: 0.0782\n",
      "252/463, train_loss: 0.1719\n",
      "253/463, train_loss: 0.1367\n",
      "254/463, train_loss: 0.0865\n",
      "255/463, train_loss: 0.0730\n",
      "256/463, train_loss: 0.2344\n",
      "257/463, train_loss: 0.1179\n",
      "258/463, train_loss: 0.2639\n",
      "259/463, train_loss: 0.4585\n",
      "260/463, train_loss: 0.1604\n",
      "261/463, train_loss: 0.1362\n",
      "262/463, train_loss: 0.3860\n",
      "263/463, train_loss: 0.6260\n",
      "264/463, train_loss: 0.6021\n",
      "265/463, train_loss: 0.1853\n",
      "266/463, train_loss: 0.0377\n",
      "267/463, train_loss: 0.2944\n",
      "268/463, train_loss: 0.2632\n",
      "269/463, train_loss: 0.1111\n",
      "270/463, train_loss: 0.7637\n",
      "271/463, train_loss: 0.6460\n",
      "272/463, train_loss: 0.2510\n",
      "273/463, train_loss: 0.1630\n",
      "274/463, train_loss: 0.2383\n",
      "275/463, train_loss: 0.2472\n",
      "276/463, train_loss: 0.1627\n",
      "277/463, train_loss: 0.4412\n",
      "278/463, train_loss: 0.2219\n",
      "279/463, train_loss: 0.2191\n",
      "280/463, train_loss: 0.1593\n",
      "281/463, train_loss: 0.1001\n",
      "282/463, train_loss: 0.1395\n",
      "283/463, train_loss: 0.2312\n",
      "284/463, train_loss: 0.1000\n",
      "285/463, train_loss: 0.1982\n",
      "286/463, train_loss: 0.3218\n",
      "287/463, train_loss: 0.4697\n",
      "288/463, train_loss: 0.0963\n",
      "289/463, train_loss: 0.1560\n",
      "290/463, train_loss: 0.1252\n",
      "291/463, train_loss: 0.1482\n",
      "292/463, train_loss: 0.0531\n",
      "293/463, train_loss: 0.1930\n",
      "294/463, train_loss: 0.1307\n",
      "295/463, train_loss: 0.1653\n",
      "296/463, train_loss: 0.0997\n",
      "297/463, train_loss: 0.1636\n",
      "298/463, train_loss: 0.1841\n",
      "299/463, train_loss: 0.2979\n",
      "300/463, train_loss: 0.4626\n",
      "301/463, train_loss: 0.1808\n",
      "302/463, train_loss: 0.1270\n",
      "303/463, train_loss: 0.1864\n",
      "304/463, train_loss: 0.2131\n",
      "305/463, train_loss: 0.1464\n",
      "306/463, train_loss: 0.3679\n",
      "307/463, train_loss: 0.1597\n",
      "308/463, train_loss: 0.0690\n",
      "309/463, train_loss: 0.1053\n",
      "310/463, train_loss: 0.1938\n",
      "311/463, train_loss: 0.0945\n",
      "312/463, train_loss: 0.2734\n",
      "313/463, train_loss: 0.1139\n",
      "314/463, train_loss: 0.3008\n",
      "315/463, train_loss: 0.1196\n",
      "316/463, train_loss: 0.5591\n",
      "317/463, train_loss: 0.1752\n",
      "318/463, train_loss: 0.3047\n",
      "319/463, train_loss: 0.1054\n",
      "320/463, train_loss: 0.1357\n",
      "321/463, train_loss: 0.2720\n",
      "322/463, train_loss: 0.1180\n",
      "323/463, train_loss: 0.1270\n",
      "324/463, train_loss: 0.1349\n",
      "325/463, train_loss: 0.5176\n",
      "326/463, train_loss: 0.1521\n",
      "327/463, train_loss: 0.4043\n",
      "328/463, train_loss: 0.2539\n",
      "329/463, train_loss: 0.2661\n",
      "330/463, train_loss: 0.0695\n",
      "331/463, train_loss: 0.1667\n",
      "332/463, train_loss: 0.1146\n",
      "333/463, train_loss: 0.1963\n",
      "334/463, train_loss: 0.1315\n",
      "335/463, train_loss: 0.1543\n",
      "336/463, train_loss: 0.1572\n",
      "337/463, train_loss: 0.6099\n",
      "338/463, train_loss: 0.2695\n",
      "339/463, train_loss: 0.1455\n",
      "340/463, train_loss: 0.0544\n",
      "341/463, train_loss: 0.2120\n",
      "342/463, train_loss: 0.1262\n",
      "343/463, train_loss: 0.1503\n",
      "344/463, train_loss: 0.3315\n",
      "345/463, train_loss: 0.0786\n",
      "346/463, train_loss: 0.3401\n",
      "347/463, train_loss: 0.1237\n",
      "348/463, train_loss: 0.2034\n",
      "349/463, train_loss: 1.1865\n",
      "350/463, train_loss: 0.0753\n",
      "351/463, train_loss: 0.1447\n",
      "352/463, train_loss: 0.3850\n",
      "353/463, train_loss: 0.0989\n",
      "354/463, train_loss: 0.2247\n",
      "355/463, train_loss: 0.2646\n",
      "356/463, train_loss: 0.2235\n",
      "357/463, train_loss: 0.1539\n",
      "358/463, train_loss: 0.1249\n",
      "359/463, train_loss: 0.1426\n",
      "360/463, train_loss: 0.0806\n",
      "361/463, train_loss: 0.3455\n",
      "362/463, train_loss: 0.3799\n",
      "363/463, train_loss: 0.1420\n",
      "364/463, train_loss: 0.2063\n",
      "365/463, train_loss: 0.1266\n",
      "366/463, train_loss: 0.1649\n",
      "367/463, train_loss: 0.0943\n",
      "368/463, train_loss: 0.2705\n",
      "369/463, train_loss: 0.1090\n",
      "370/463, train_loss: 0.1941\n",
      "371/463, train_loss: 0.2710\n",
      "372/463, train_loss: 0.1316\n",
      "373/463, train_loss: 0.0880\n",
      "374/463, train_loss: 0.1342\n",
      "375/463, train_loss: 0.4167\n",
      "376/463, train_loss: 0.3833\n",
      "377/463, train_loss: 0.1672\n",
      "378/463, train_loss: 0.1593\n",
      "379/463, train_loss: 0.0607\n",
      "380/463, train_loss: 0.1593\n",
      "381/463, train_loss: 0.1931\n",
      "382/463, train_loss: 0.2344\n",
      "383/463, train_loss: 0.3201\n",
      "384/463, train_loss: 0.1180\n",
      "385/463, train_loss: 0.0724\n",
      "386/463, train_loss: 0.0743\n",
      "387/463, train_loss: 0.0648\n",
      "388/463, train_loss: 0.4622\n",
      "389/463, train_loss: 0.2114\n",
      "390/463, train_loss: 0.3882\n",
      "391/463, train_loss: 0.4258\n",
      "392/463, train_loss: 0.1759\n",
      "393/463, train_loss: 0.1434\n",
      "394/463, train_loss: 0.1768\n",
      "395/463, train_loss: 0.1504\n",
      "396/463, train_loss: 0.0760\n",
      "397/463, train_loss: 0.2766\n",
      "398/463, train_loss: 0.1659\n",
      "399/463, train_loss: 0.3799\n",
      "400/463, train_loss: 0.1592\n",
      "401/463, train_loss: 0.1450\n",
      "402/463, train_loss: 0.2852\n",
      "403/463, train_loss: 0.1338\n",
      "404/463, train_loss: 0.1274\n",
      "405/463, train_loss: 0.0818\n",
      "406/463, train_loss: 0.3066\n",
      "407/463, train_loss: 0.2399\n",
      "408/463, train_loss: 0.0181\n",
      "409/463, train_loss: 0.1292\n",
      "410/463, train_loss: 0.4229\n",
      "411/463, train_loss: 0.1625\n",
      "412/463, train_loss: 0.4863\n",
      "413/463, train_loss: 0.1962\n",
      "414/463, train_loss: 0.0954\n",
      "415/463, train_loss: 0.3384\n",
      "416/463, train_loss: 0.2192\n",
      "417/463, train_loss: 0.5317\n",
      "418/463, train_loss: 0.2089\n",
      "419/463, train_loss: 0.1758\n",
      "420/463, train_loss: 0.4805\n",
      "421/463, train_loss: 0.1790\n",
      "422/463, train_loss: 0.2371\n",
      "423/463, train_loss: 0.0881\n",
      "424/463, train_loss: 0.2003\n",
      "425/463, train_loss: 0.1991\n",
      "426/463, train_loss: 0.3696\n",
      "427/463, train_loss: 0.0936\n",
      "428/463, train_loss: 0.1711\n",
      "429/463, train_loss: 0.0743\n",
      "430/463, train_loss: 0.1641\n",
      "431/463, train_loss: 0.4631\n",
      "432/463, train_loss: 0.1213\n",
      "433/463, train_loss: 1.0215\n",
      "434/463, train_loss: 0.1632\n",
      "435/463, train_loss: 0.1824\n",
      "436/463, train_loss: 0.6523\n",
      "437/463, train_loss: 0.0754\n",
      "438/463, train_loss: 0.4309\n",
      "439/463, train_loss: 0.6465\n",
      "440/463, train_loss: 0.1521\n",
      "441/463, train_loss: 0.2197\n",
      "442/463, train_loss: 0.1812\n",
      "443/463, train_loss: 0.0628\n",
      "444/463, train_loss: 0.0994\n",
      "445/463, train_loss: 0.1660\n",
      "446/463, train_loss: 0.2793\n",
      "447/463, train_loss: 0.1658\n",
      "448/463, train_loss: 0.1290\n",
      "449/463, train_loss: 0.1619\n",
      "450/463, train_loss: 0.1008\n",
      "451/463, train_loss: 0.0741\n",
      "452/463, train_loss: 0.2246\n",
      "453/463, train_loss: 0.1018\n",
      "454/463, train_loss: 0.0948\n",
      "455/463, train_loss: 0.1744\n",
      "456/463, train_loss: 0.1763\n",
      "457/463, train_loss: 0.1371\n",
      "458/463, train_loss: 0.0956\n",
      "459/463, train_loss: 0.1100\n",
      "460/463, train_loss: 0.1060\n",
      "461/463, train_loss: 0.1453\n",
      "462/463, train_loss: 0.1644\n",
      "463/463, train_loss: 0.2419\n",
      "epoch 16 average loss: 0.2209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 16:58:32 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 16:58:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 16:58:38 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 17/100\n",
      "1/463, train_loss: 0.1555\n",
      "2/463, train_loss: 0.1669\n",
      "3/463, train_loss: 0.1628\n",
      "4/463, train_loss: 0.1659\n",
      "5/463, train_loss: 0.0816\n",
      "6/463, train_loss: 0.1377\n",
      "7/463, train_loss: 0.2051\n",
      "8/463, train_loss: 0.3145\n",
      "9/463, train_loss: 0.1161\n",
      "10/463, train_loss: 0.1318\n",
      "11/463, train_loss: 0.1071\n",
      "12/463, train_loss: 1.0986\n",
      "13/463, train_loss: 0.2222\n",
      "14/463, train_loss: 0.2096\n",
      "15/463, train_loss: 0.2271\n",
      "16/463, train_loss: 0.0981\n",
      "17/463, train_loss: 0.1511\n",
      "18/463, train_loss: 0.1707\n",
      "19/463, train_loss: 0.0814\n",
      "20/463, train_loss: 0.2834\n",
      "21/463, train_loss: 0.0712\n",
      "22/463, train_loss: 0.0957\n",
      "23/463, train_loss: 0.0922\n",
      "24/463, train_loss: 0.3811\n",
      "25/463, train_loss: 0.1154\n",
      "26/463, train_loss: 0.0682\n",
      "27/463, train_loss: 0.4258\n",
      "28/463, train_loss: 0.6221\n",
      "29/463, train_loss: 0.1324\n",
      "30/463, train_loss: 0.3970\n",
      "31/463, train_loss: 0.1626\n",
      "32/463, train_loss: 0.4165\n",
      "33/463, train_loss: 0.2825\n",
      "34/463, train_loss: 0.1373\n",
      "35/463, train_loss: 0.3401\n",
      "36/463, train_loss: 0.4253\n",
      "37/463, train_loss: 0.3044\n",
      "38/463, train_loss: 0.3809\n",
      "39/463, train_loss: 0.3027\n",
      "40/463, train_loss: 0.2115\n",
      "41/463, train_loss: 0.0544\n",
      "42/463, train_loss: 0.2832\n",
      "43/463, train_loss: 0.3584\n",
      "44/463, train_loss: 0.1660\n",
      "45/463, train_loss: 0.0850\n",
      "46/463, train_loss: 0.2352\n",
      "47/463, train_loss: 0.3208\n",
      "48/463, train_loss: 0.2710\n",
      "49/463, train_loss: 0.2910\n",
      "50/463, train_loss: 0.2664\n",
      "51/463, train_loss: 0.4722\n",
      "52/463, train_loss: 0.1907\n",
      "53/463, train_loss: 0.2145\n",
      "54/463, train_loss: 0.4907\n",
      "55/463, train_loss: 0.1367\n",
      "56/463, train_loss: 0.1582\n",
      "57/463, train_loss: 0.1078\n",
      "58/463, train_loss: 0.1150\n",
      "59/463, train_loss: 0.1252\n",
      "60/463, train_loss: 0.1143\n",
      "61/463, train_loss: 0.2700\n",
      "62/463, train_loss: 0.2031\n",
      "63/463, train_loss: 0.0862\n",
      "64/463, train_loss: 0.2058\n",
      "65/463, train_loss: 0.1240\n",
      "66/463, train_loss: 0.0997\n",
      "67/463, train_loss: 0.1070\n",
      "68/463, train_loss: 0.1335\n",
      "69/463, train_loss: 0.2284\n",
      "70/463, train_loss: 0.0981\n",
      "71/463, train_loss: 0.2664\n",
      "72/463, train_loss: 0.5020\n",
      "73/463, train_loss: 0.1926\n",
      "74/463, train_loss: 0.0929\n",
      "75/463, train_loss: 0.2382\n",
      "76/463, train_loss: 0.1237\n",
      "77/463, train_loss: 0.3813\n",
      "78/463, train_loss: 0.1210\n",
      "79/463, train_loss: 0.2306\n",
      "80/463, train_loss: 0.2009\n",
      "81/463, train_loss: 0.3611\n",
      "82/463, train_loss: 0.1343\n",
      "83/463, train_loss: 0.7808\n",
      "84/463, train_loss: 0.4624\n",
      "85/463, train_loss: 0.1643\n",
      "86/463, train_loss: 0.1578\n",
      "87/463, train_loss: 0.3994\n",
      "88/463, train_loss: 0.2327\n",
      "89/463, train_loss: 0.1661\n",
      "90/463, train_loss: 0.0679\n",
      "91/463, train_loss: 0.1106\n",
      "92/463, train_loss: 0.0784\n",
      "93/463, train_loss: 0.0943\n",
      "94/463, train_loss: 0.3274\n",
      "95/463, train_loss: 0.0768\n",
      "96/463, train_loss: 0.0791\n",
      "97/463, train_loss: 0.0692\n",
      "98/463, train_loss: 0.2666\n",
      "99/463, train_loss: 0.2896\n",
      "100/463, train_loss: 0.2817\n",
      "101/463, train_loss: 0.6846\n",
      "102/463, train_loss: 0.1847\n",
      "103/463, train_loss: 0.1135\n",
      "104/463, train_loss: 0.5293\n",
      "105/463, train_loss: 0.0819\n",
      "106/463, train_loss: 0.1038\n",
      "107/463, train_loss: 0.4856\n",
      "108/463, train_loss: 0.1310\n",
      "109/463, train_loss: 0.1904\n",
      "110/463, train_loss: 0.3000\n",
      "111/463, train_loss: 0.2340\n",
      "112/463, train_loss: 0.2942\n",
      "113/463, train_loss: 0.0842\n",
      "114/463, train_loss: 0.1217\n",
      "115/463, train_loss: 0.1561\n",
      "116/463, train_loss: 0.2491\n",
      "117/463, train_loss: 0.6631\n",
      "118/463, train_loss: 0.1450\n",
      "119/463, train_loss: 0.0906\n",
      "120/463, train_loss: 0.2072\n",
      "121/463, train_loss: 0.1619\n",
      "122/463, train_loss: 0.1052\n",
      "123/463, train_loss: 0.1183\n",
      "124/463, train_loss: 0.1071\n",
      "125/463, train_loss: 0.5225\n",
      "126/463, train_loss: 0.1354\n",
      "127/463, train_loss: 0.1100\n",
      "128/463, train_loss: 0.3459\n",
      "129/463, train_loss: 0.0982\n",
      "130/463, train_loss: 0.0911\n",
      "131/463, train_loss: 0.1376\n",
      "132/463, train_loss: 0.5459\n",
      "133/463, train_loss: 0.3767\n",
      "134/463, train_loss: 0.3579\n",
      "135/463, train_loss: 0.2754\n",
      "136/463, train_loss: 0.1003\n",
      "137/463, train_loss: 0.4155\n",
      "138/463, train_loss: 0.3845\n",
      "139/463, train_loss: 0.1713\n",
      "140/463, train_loss: 0.2208\n",
      "141/463, train_loss: 0.1277\n",
      "142/463, train_loss: 0.3921\n",
      "143/463, train_loss: 0.5591\n",
      "144/463, train_loss: 0.1523\n",
      "145/463, train_loss: 0.1038\n",
      "146/463, train_loss: 0.3857\n",
      "147/463, train_loss: 0.3545\n",
      "148/463, train_loss: 0.1169\n",
      "149/463, train_loss: 0.1746\n",
      "150/463, train_loss: 0.3169\n",
      "151/463, train_loss: 0.1068\n",
      "152/463, train_loss: 0.2141\n",
      "153/463, train_loss: 0.2025\n",
      "154/463, train_loss: 0.2002\n",
      "155/463, train_loss: 0.1438\n",
      "156/463, train_loss: 0.1375\n",
      "157/463, train_loss: 0.2100\n",
      "158/463, train_loss: 0.1829\n",
      "159/463, train_loss: 0.2910\n",
      "160/463, train_loss: 0.1808\n",
      "161/463, train_loss: 0.1852\n",
      "162/463, train_loss: 0.5557\n",
      "163/463, train_loss: 0.1024\n",
      "164/463, train_loss: 0.1833\n",
      "165/463, train_loss: 0.5479\n",
      "166/463, train_loss: 0.1189\n",
      "167/463, train_loss: 0.1285\n",
      "168/463, train_loss: 0.3567\n",
      "169/463, train_loss: 0.2042\n",
      "170/463, train_loss: 0.2302\n",
      "171/463, train_loss: 0.8174\n",
      "172/463, train_loss: 0.3291\n",
      "173/463, train_loss: 0.4106\n",
      "174/463, train_loss: 0.2485\n",
      "175/463, train_loss: 0.3521\n",
      "176/463, train_loss: 0.1681\n",
      "177/463, train_loss: 0.4521\n",
      "178/463, train_loss: 0.1991\n",
      "179/463, train_loss: 0.3391\n",
      "180/463, train_loss: 0.2333\n",
      "181/463, train_loss: 0.0268\n",
      "182/463, train_loss: 0.2507\n",
      "183/463, train_loss: 0.3762\n",
      "184/463, train_loss: 0.1279\n",
      "185/463, train_loss: 0.0831\n",
      "186/463, train_loss: 0.1617\n",
      "187/463, train_loss: 0.1833\n",
      "188/463, train_loss: 0.1218\n",
      "189/463, train_loss: 0.1497\n",
      "190/463, train_loss: 0.0836\n",
      "191/463, train_loss: 0.2377\n",
      "192/463, train_loss: 0.0767\n",
      "193/463, train_loss: 0.1805\n",
      "194/463, train_loss: 0.2817\n",
      "195/463, train_loss: 0.1731\n",
      "196/463, train_loss: 0.0964\n",
      "197/463, train_loss: 0.0902\n",
      "198/463, train_loss: 0.0983\n",
      "199/463, train_loss: 0.3838\n",
      "200/463, train_loss: 0.2167\n",
      "201/463, train_loss: 0.4353\n",
      "202/463, train_loss: 0.0377\n",
      "203/463, train_loss: 0.1870\n",
      "204/463, train_loss: 0.1083\n",
      "205/463, train_loss: 0.1278\n",
      "206/463, train_loss: 0.2725\n",
      "207/463, train_loss: 0.3459\n",
      "208/463, train_loss: 0.1232\n",
      "209/463, train_loss: 0.2070\n",
      "210/463, train_loss: 0.1316\n",
      "211/463, train_loss: 0.2039\n",
      "212/463, train_loss: 0.2312\n",
      "213/463, train_loss: 0.1334\n",
      "214/463, train_loss: 0.0826\n",
      "215/463, train_loss: 0.0666\n",
      "216/463, train_loss: 0.1559\n",
      "217/463, train_loss: 0.0662\n",
      "218/463, train_loss: 0.0978\n",
      "219/463, train_loss: 0.6284\n",
      "220/463, train_loss: 0.1415\n",
      "221/463, train_loss: 0.1477\n",
      "222/463, train_loss: 0.2039\n",
      "223/463, train_loss: 0.1104\n",
      "224/463, train_loss: 0.3950\n",
      "225/463, train_loss: 0.0739\n",
      "226/463, train_loss: 0.4021\n",
      "227/463, train_loss: 0.1011\n",
      "228/463, train_loss: 0.1731\n",
      "229/463, train_loss: 0.5410\n",
      "230/463, train_loss: 0.0949\n",
      "231/463, train_loss: 0.1792\n",
      "232/463, train_loss: 0.2072\n",
      "233/463, train_loss: 0.2135\n",
      "234/463, train_loss: 0.1747\n",
      "235/463, train_loss: 0.1486\n",
      "236/463, train_loss: 0.4722\n",
      "237/463, train_loss: 0.1636\n",
      "238/463, train_loss: 0.1034\n",
      "239/463, train_loss: 0.3809\n",
      "240/463, train_loss: 0.5161\n",
      "241/463, train_loss: 0.2734\n",
      "242/463, train_loss: 0.0735\n",
      "243/463, train_loss: 0.1086\n",
      "244/463, train_loss: 0.1133\n",
      "245/463, train_loss: 0.0815\n",
      "246/463, train_loss: 0.3708\n",
      "247/463, train_loss: 0.4229\n",
      "248/463, train_loss: 0.1580\n",
      "249/463, train_loss: 0.2185\n",
      "250/463, train_loss: 0.3799\n",
      "251/463, train_loss: 0.2188\n",
      "252/463, train_loss: 0.3516\n",
      "253/463, train_loss: 0.2012\n",
      "254/463, train_loss: 0.1252\n",
      "255/463, train_loss: 0.1385\n",
      "256/463, train_loss: 0.5298\n",
      "257/463, train_loss: 0.1838\n",
      "258/463, train_loss: 0.1624\n",
      "259/463, train_loss: 0.1406\n",
      "260/463, train_loss: 0.0657\n",
      "261/463, train_loss: 0.1011\n",
      "262/463, train_loss: 0.1049\n",
      "263/463, train_loss: 0.1401\n",
      "264/463, train_loss: 0.1506\n",
      "265/463, train_loss: 0.1019\n",
      "266/463, train_loss: 0.2061\n",
      "267/463, train_loss: 0.4014\n",
      "268/463, train_loss: 0.1669\n",
      "269/463, train_loss: 0.1038\n",
      "270/463, train_loss: 0.1394\n",
      "271/463, train_loss: 0.1138\n",
      "272/463, train_loss: 0.1519\n",
      "273/463, train_loss: 0.7886\n",
      "274/463, train_loss: 0.3320\n",
      "275/463, train_loss: 0.1035\n",
      "276/463, train_loss: 0.1643\n",
      "277/463, train_loss: 0.1355\n",
      "278/463, train_loss: 0.0767\n",
      "279/463, train_loss: 0.0861\n",
      "280/463, train_loss: 0.0863\n",
      "281/463, train_loss: 0.1830\n",
      "282/463, train_loss: 0.3384\n",
      "283/463, train_loss: 0.2166\n",
      "284/463, train_loss: 0.4580\n",
      "285/463, train_loss: 0.3586\n",
      "286/463, train_loss: 0.1073\n",
      "287/463, train_loss: 0.2932\n",
      "288/463, train_loss: 0.8525\n",
      "289/463, train_loss: 0.2369\n",
      "290/463, train_loss: 0.1278\n",
      "291/463, train_loss: 0.2070\n",
      "292/463, train_loss: 0.1249\n",
      "293/463, train_loss: 0.1309\n",
      "294/463, train_loss: 0.6992\n",
      "295/463, train_loss: 0.1019\n",
      "296/463, train_loss: 0.2101\n",
      "297/463, train_loss: 0.1011\n",
      "298/463, train_loss: 0.2563\n",
      "299/463, train_loss: 0.0889\n",
      "300/463, train_loss: 0.1733\n",
      "301/463, train_loss: 0.6523\n",
      "302/463, train_loss: 0.1832\n",
      "303/463, train_loss: 0.1578\n",
      "304/463, train_loss: 0.0776\n",
      "305/463, train_loss: 0.1726\n",
      "306/463, train_loss: 0.3347\n",
      "307/463, train_loss: 0.2793\n",
      "308/463, train_loss: 0.7310\n",
      "309/463, train_loss: 0.1694\n",
      "310/463, train_loss: 0.1794\n",
      "311/463, train_loss: 0.1385\n",
      "312/463, train_loss: 0.0844\n",
      "313/463, train_loss: 0.1091\n",
      "314/463, train_loss: 0.2961\n",
      "315/463, train_loss: 0.1956\n",
      "316/463, train_loss: 0.3843\n",
      "317/463, train_loss: 0.0252\n",
      "318/463, train_loss: 0.1696\n",
      "319/463, train_loss: 0.3528\n",
      "320/463, train_loss: 0.1237\n",
      "321/463, train_loss: 0.3650\n",
      "322/463, train_loss: 0.3940\n",
      "323/463, train_loss: 0.1154\n",
      "324/463, train_loss: 0.1660\n",
      "325/463, train_loss: 0.1902\n",
      "326/463, train_loss: 0.2351\n",
      "327/463, train_loss: 0.0923\n",
      "328/463, train_loss: 0.1176\n",
      "329/463, train_loss: 0.0876\n",
      "330/463, train_loss: 0.1414\n",
      "331/463, train_loss: 0.4309\n",
      "332/463, train_loss: 0.5391\n",
      "333/463, train_loss: 0.1730\n",
      "334/463, train_loss: 0.2098\n",
      "335/463, train_loss: 0.2429\n",
      "336/463, train_loss: 0.3699\n",
      "337/463, train_loss: 0.1698\n",
      "338/463, train_loss: 0.3354\n",
      "339/463, train_loss: 0.0829\n",
      "340/463, train_loss: 0.1379\n",
      "341/463, train_loss: 0.1385\n",
      "342/463, train_loss: 0.0851\n",
      "343/463, train_loss: 0.1377\n",
      "344/463, train_loss: 0.0768\n",
      "345/463, train_loss: 0.2463\n",
      "346/463, train_loss: 0.0834\n",
      "347/463, train_loss: 0.4756\n",
      "348/463, train_loss: 0.1138\n",
      "349/463, train_loss: 0.0939\n",
      "350/463, train_loss: 0.0879\n",
      "351/463, train_loss: 0.2842\n",
      "352/463, train_loss: 0.1340\n",
      "353/463, train_loss: 0.1191\n",
      "354/463, train_loss: 0.1475\n",
      "355/463, train_loss: 0.1198\n",
      "356/463, train_loss: 0.0848\n",
      "357/463, train_loss: 0.1246\n",
      "358/463, train_loss: 0.0995\n",
      "359/463, train_loss: 0.0590\n",
      "360/463, train_loss: 0.1927\n",
      "361/463, train_loss: 0.2247\n",
      "362/463, train_loss: 0.6309\n",
      "363/463, train_loss: 0.4429\n",
      "364/463, train_loss: 0.1608\n",
      "365/463, train_loss: 0.4915\n",
      "366/463, train_loss: 0.1450\n",
      "367/463, train_loss: 0.2480\n",
      "368/463, train_loss: 0.1071\n",
      "369/463, train_loss: 0.2585\n",
      "370/463, train_loss: 0.1187\n",
      "371/463, train_loss: 0.3335\n",
      "372/463, train_loss: 0.1691\n",
      "373/463, train_loss: 0.1396\n",
      "374/463, train_loss: 0.3066\n",
      "375/463, train_loss: 0.2006\n",
      "376/463, train_loss: 0.1641\n",
      "377/463, train_loss: 0.2886\n",
      "378/463, train_loss: 0.2260\n",
      "379/463, train_loss: 0.4270\n",
      "380/463, train_loss: 0.3167\n",
      "381/463, train_loss: 0.1560\n",
      "382/463, train_loss: 0.3040\n",
      "383/463, train_loss: 0.1738\n",
      "384/463, train_loss: 0.0947\n",
      "385/463, train_loss: 0.1304\n",
      "386/463, train_loss: 0.1487\n",
      "387/463, train_loss: 0.3884\n",
      "388/463, train_loss: 0.2231\n",
      "389/463, train_loss: 0.3696\n",
      "390/463, train_loss: 0.1431\n",
      "391/463, train_loss: 0.1676\n",
      "392/463, train_loss: 0.0872\n",
      "393/463, train_loss: 0.0472\n",
      "394/463, train_loss: 0.0875\n",
      "395/463, train_loss: 0.1102\n",
      "396/463, train_loss: 0.1676\n",
      "397/463, train_loss: 0.0789\n",
      "398/463, train_loss: 0.1469\n",
      "399/463, train_loss: 0.1515\n",
      "400/463, train_loss: 0.1707\n",
      "401/463, train_loss: 0.3135\n",
      "402/463, train_loss: 0.2134\n",
      "403/463, train_loss: 0.1755\n",
      "404/463, train_loss: 0.5605\n",
      "405/463, train_loss: 0.7920\n",
      "406/463, train_loss: 0.0923\n",
      "407/463, train_loss: 0.2996\n",
      "408/463, train_loss: 0.1403\n",
      "409/463, train_loss: 0.3911\n",
      "410/463, train_loss: 0.1379\n",
      "411/463, train_loss: 0.1538\n",
      "412/463, train_loss: 0.7280\n",
      "413/463, train_loss: 0.1473\n",
      "414/463, train_loss: 0.2498\n",
      "415/463, train_loss: 0.4009\n",
      "416/463, train_loss: 0.0911\n",
      "417/463, train_loss: 0.4351\n",
      "418/463, train_loss: 0.2354\n",
      "419/463, train_loss: 0.5586\n",
      "420/463, train_loss: 0.1304\n",
      "421/463, train_loss: 0.5020\n",
      "422/463, train_loss: 0.2158\n",
      "423/463, train_loss: 0.1383\n",
      "424/463, train_loss: 0.3857\n",
      "425/463, train_loss: 0.6440\n",
      "426/463, train_loss: 0.1498\n",
      "427/463, train_loss: 0.0939\n",
      "428/463, train_loss: 0.1004\n",
      "429/463, train_loss: 0.1438\n",
      "430/463, train_loss: 0.1046\n",
      "431/463, train_loss: 0.0970\n",
      "432/463, train_loss: 0.1793\n",
      "433/463, train_loss: 0.2014\n",
      "434/463, train_loss: 0.0870\n",
      "435/463, train_loss: 0.0870\n",
      "436/463, train_loss: 0.0924\n",
      "437/463, train_loss: 0.0780\n",
      "438/463, train_loss: 0.1227\n",
      "439/463, train_loss: 0.1401\n",
      "440/463, train_loss: 0.2224\n",
      "441/463, train_loss: 0.2042\n",
      "442/463, train_loss: 0.1871\n",
      "443/463, train_loss: 0.0594\n",
      "444/463, train_loss: 0.1368\n",
      "445/463, train_loss: 0.1309\n",
      "446/463, train_loss: 0.4663\n",
      "447/463, train_loss: 0.3237\n",
      "448/463, train_loss: 0.1729\n",
      "449/463, train_loss: 0.1646\n",
      "450/463, train_loss: 0.2432\n",
      "451/463, train_loss: 0.1726\n",
      "452/463, train_loss: 0.1117\n",
      "453/463, train_loss: 0.3408\n",
      "454/463, train_loss: 0.1782\n",
      "455/463, train_loss: 0.0831\n",
      "456/463, train_loss: 0.1144\n",
      "457/463, train_loss: 0.2208\n",
      "458/463, train_loss: 0.2729\n",
      "459/463, train_loss: 0.0784\n",
      "460/463, train_loss: 0.2737\n",
      "461/463, train_loss: 0.0930\n",
      "462/463, train_loss: 0.0562\n",
      "463/463, train_loss: 0.1071\n",
      "epoch 17 average loss: 0.2239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 19:11:20 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 19:11:23 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 19:11:25 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 18/100\n",
      "1/463, train_loss: 0.2255\n",
      "2/463, train_loss: 0.1169\n",
      "3/463, train_loss: 0.1799\n",
      "4/463, train_loss: 0.1575\n",
      "5/463, train_loss: 0.2607\n",
      "6/463, train_loss: 0.2151\n",
      "7/463, train_loss: 0.1771\n",
      "8/463, train_loss: 0.1056\n",
      "9/463, train_loss: 0.1846\n",
      "10/463, train_loss: 0.3396\n",
      "11/463, train_loss: 0.1136\n",
      "12/463, train_loss: 0.1403\n",
      "13/463, train_loss: 0.2427\n",
      "14/463, train_loss: 0.1857\n",
      "15/463, train_loss: 0.3174\n",
      "16/463, train_loss: 0.1234\n",
      "17/463, train_loss: 0.0656\n",
      "18/463, train_loss: 0.1097\n",
      "19/463, train_loss: 0.2180\n",
      "20/463, train_loss: 0.1460\n",
      "21/463, train_loss: 0.1648\n",
      "22/463, train_loss: 0.1902\n",
      "23/463, train_loss: 0.4683\n",
      "24/463, train_loss: 0.1615\n",
      "25/463, train_loss: 0.1560\n",
      "26/463, train_loss: 0.1614\n",
      "27/463, train_loss: 0.0941\n",
      "28/463, train_loss: 0.0701\n",
      "29/463, train_loss: 0.0784\n",
      "30/463, train_loss: 0.1340\n",
      "31/463, train_loss: 0.0454\n",
      "32/463, train_loss: 0.1223\n",
      "33/463, train_loss: 0.3071\n",
      "34/463, train_loss: 0.5381\n",
      "35/463, train_loss: 0.1726\n",
      "36/463, train_loss: 0.0876\n",
      "37/463, train_loss: 0.3706\n",
      "38/463, train_loss: 0.1956\n",
      "39/463, train_loss: 0.1393\n",
      "40/463, train_loss: 0.5112\n",
      "41/463, train_loss: 0.2231\n",
      "42/463, train_loss: 0.1052\n",
      "43/463, train_loss: 0.2445\n",
      "44/463, train_loss: 0.1001\n",
      "45/463, train_loss: 0.1987\n",
      "46/463, train_loss: 0.1090\n",
      "47/463, train_loss: 0.3022\n",
      "48/463, train_loss: 0.2427\n",
      "49/463, train_loss: 0.1241\n",
      "50/463, train_loss: 0.2349\n",
      "51/463, train_loss: 0.1979\n",
      "52/463, train_loss: 0.2347\n",
      "53/463, train_loss: 0.2754\n",
      "54/463, train_loss: 0.0827\n",
      "55/463, train_loss: 0.4507\n",
      "56/463, train_loss: 0.1523\n",
      "57/463, train_loss: 0.2480\n",
      "58/463, train_loss: 0.2037\n",
      "59/463, train_loss: 0.0656\n",
      "60/463, train_loss: 0.4355\n",
      "61/463, train_loss: 0.5781\n",
      "62/463, train_loss: 0.1106\n",
      "63/463, train_loss: 0.0766\n",
      "64/463, train_loss: 0.3855\n",
      "65/463, train_loss: 0.1357\n",
      "66/463, train_loss: 0.4219\n",
      "67/463, train_loss: 0.2266\n",
      "68/463, train_loss: 0.1329\n",
      "69/463, train_loss: 0.0597\n",
      "70/463, train_loss: 0.1755\n",
      "71/463, train_loss: 0.0993\n",
      "72/463, train_loss: 0.2651\n",
      "73/463, train_loss: 0.1797\n",
      "74/463, train_loss: 0.1187\n",
      "75/463, train_loss: 0.1309\n",
      "76/463, train_loss: 0.0797\n",
      "77/463, train_loss: 0.1508\n",
      "78/463, train_loss: 0.3379\n",
      "79/463, train_loss: 0.1614\n",
      "80/463, train_loss: 0.2351\n",
      "81/463, train_loss: 0.1032\n",
      "82/463, train_loss: 0.0887\n",
      "83/463, train_loss: 0.1361\n",
      "84/463, train_loss: 0.1786\n",
      "85/463, train_loss: 0.3047\n",
      "86/463, train_loss: 0.1177\n",
      "87/463, train_loss: 0.5410\n",
      "88/463, train_loss: 0.1855\n",
      "89/463, train_loss: 0.1511\n",
      "90/463, train_loss: 0.1714\n",
      "91/463, train_loss: 0.3928\n",
      "92/463, train_loss: 0.1056\n",
      "93/463, train_loss: 0.1169\n",
      "94/463, train_loss: 0.3171\n",
      "95/463, train_loss: 0.1937\n",
      "96/463, train_loss: 0.0989\n",
      "97/463, train_loss: 0.1936\n",
      "98/463, train_loss: 0.1005\n",
      "99/463, train_loss: 0.7441\n",
      "100/463, train_loss: 0.8062\n",
      "101/463, train_loss: 0.2460\n",
      "102/463, train_loss: 0.2352\n",
      "103/463, train_loss: 0.1954\n",
      "104/463, train_loss: 0.1046\n",
      "105/463, train_loss: 0.2058\n",
      "106/463, train_loss: 0.0994\n",
      "107/463, train_loss: 0.1146\n",
      "108/463, train_loss: 0.1990\n",
      "109/463, train_loss: 0.1707\n",
      "110/463, train_loss: 0.0654\n",
      "111/463, train_loss: 0.1315\n",
      "112/463, train_loss: 0.1479\n",
      "113/463, train_loss: 0.2267\n",
      "114/463, train_loss: 0.0709\n",
      "115/463, train_loss: 0.1271\n",
      "116/463, train_loss: 0.0774\n",
      "117/463, train_loss: 0.1185\n",
      "118/463, train_loss: 0.3823\n",
      "119/463, train_loss: 0.1733\n",
      "120/463, train_loss: 0.2147\n",
      "121/463, train_loss: 0.0605\n",
      "122/463, train_loss: 0.0645\n",
      "123/463, train_loss: 0.1652\n",
      "124/463, train_loss: 0.3240\n",
      "125/463, train_loss: 0.0890\n",
      "126/463, train_loss: 0.0721\n",
      "127/463, train_loss: 0.0877\n",
      "128/463, train_loss: 0.2465\n",
      "129/463, train_loss: 0.5859\n",
      "130/463, train_loss: 0.0691\n",
      "131/463, train_loss: 0.3374\n",
      "132/463, train_loss: 0.3767\n",
      "133/463, train_loss: 0.1622\n",
      "134/463, train_loss: 0.2581\n",
      "135/463, train_loss: 0.2949\n",
      "136/463, train_loss: 0.1858\n",
      "137/463, train_loss: 0.1343\n",
      "138/463, train_loss: 0.1080\n",
      "139/463, train_loss: 0.2418\n",
      "140/463, train_loss: 0.0802\n",
      "141/463, train_loss: 0.1569\n",
      "142/463, train_loss: 0.1880\n",
      "143/463, train_loss: 0.0797\n",
      "144/463, train_loss: 0.5103\n",
      "145/463, train_loss: 0.3704\n",
      "146/463, train_loss: 0.1443\n",
      "147/463, train_loss: 0.0718\n",
      "148/463, train_loss: 0.2690\n",
      "149/463, train_loss: 0.1570\n",
      "150/463, train_loss: 0.5171\n",
      "151/463, train_loss: 0.2378\n",
      "152/463, train_loss: 0.1410\n",
      "153/463, train_loss: 0.1467\n",
      "154/463, train_loss: 0.0618\n",
      "155/463, train_loss: 0.2457\n",
      "156/463, train_loss: 0.3196\n",
      "157/463, train_loss: 0.4519\n",
      "158/463, train_loss: 0.1445\n",
      "159/463, train_loss: 0.1201\n",
      "160/463, train_loss: 0.1520\n",
      "161/463, train_loss: 0.1403\n",
      "162/463, train_loss: 0.0940\n",
      "163/463, train_loss: 0.0546\n",
      "164/463, train_loss: 0.1104\n",
      "165/463, train_loss: 0.1053\n",
      "166/463, train_loss: 0.3142\n",
      "167/463, train_loss: 0.2939\n",
      "168/463, train_loss: 0.0989\n",
      "169/463, train_loss: 0.2822\n",
      "170/463, train_loss: 0.1188\n",
      "171/463, train_loss: 0.1364\n",
      "172/463, train_loss: 0.3206\n",
      "173/463, train_loss: 0.0732\n",
      "174/463, train_loss: 0.1992\n",
      "175/463, train_loss: 0.1111\n",
      "176/463, train_loss: 0.1472\n",
      "177/463, train_loss: 0.2710\n",
      "178/463, train_loss: 0.2917\n",
      "179/463, train_loss: 0.1898\n",
      "180/463, train_loss: 0.0873\n",
      "181/463, train_loss: 0.1367\n",
      "182/463, train_loss: 0.1160\n",
      "183/463, train_loss: 0.1072\n",
      "184/463, train_loss: 0.2014\n",
      "185/463, train_loss: 0.0914\n",
      "186/463, train_loss: 0.1353\n",
      "187/463, train_loss: 0.1716\n",
      "188/463, train_loss: 0.1301\n",
      "189/463, train_loss: 0.5537\n",
      "190/463, train_loss: 0.2080\n",
      "191/463, train_loss: 0.8213\n",
      "192/463, train_loss: 0.1858\n",
      "193/463, train_loss: 0.1511\n",
      "194/463, train_loss: 0.3640\n",
      "195/463, train_loss: 0.1497\n",
      "196/463, train_loss: 0.1541\n",
      "197/463, train_loss: 0.4563\n",
      "198/463, train_loss: 0.3269\n",
      "199/463, train_loss: 0.0752\n",
      "200/463, train_loss: 0.1639\n",
      "201/463, train_loss: 0.5981\n",
      "202/463, train_loss: 0.1276\n",
      "203/463, train_loss: 0.3542\n",
      "204/463, train_loss: 0.1234\n",
      "205/463, train_loss: 0.7715\n",
      "206/463, train_loss: 0.4912\n",
      "207/463, train_loss: 0.1227\n",
      "208/463, train_loss: 0.3926\n",
      "209/463, train_loss: 0.1688\n",
      "210/463, train_loss: 0.1804\n",
      "211/463, train_loss: 0.1793\n",
      "212/463, train_loss: 0.2469\n",
      "213/463, train_loss: 0.4580\n",
      "214/463, train_loss: 0.3259\n",
      "215/463, train_loss: 0.6021\n",
      "216/463, train_loss: 0.2407\n",
      "217/463, train_loss: 0.1077\n",
      "218/463, train_loss: 0.0787\n",
      "219/463, train_loss: 0.3301\n",
      "220/463, train_loss: 0.1489\n",
      "221/463, train_loss: 0.7676\n",
      "222/463, train_loss: 0.4629\n",
      "223/463, train_loss: 0.3965\n",
      "224/463, train_loss: 0.7095\n",
      "225/463, train_loss: 0.2090\n",
      "226/463, train_loss: 0.2490\n",
      "227/463, train_loss: 0.1205\n",
      "228/463, train_loss: 0.1061\n",
      "229/463, train_loss: 0.1412\n",
      "230/463, train_loss: 0.1078\n",
      "231/463, train_loss: 0.4341\n",
      "232/463, train_loss: 0.3037\n",
      "233/463, train_loss: 0.2749\n",
      "234/463, train_loss: 0.1326\n",
      "235/463, train_loss: 0.1860\n",
      "236/463, train_loss: 0.0724\n",
      "237/463, train_loss: 0.1011\n",
      "238/463, train_loss: 0.5640\n",
      "239/463, train_loss: 0.2267\n",
      "240/463, train_loss: 0.1815\n",
      "241/463, train_loss: 0.1588\n",
      "242/463, train_loss: 0.1835\n",
      "243/463, train_loss: 0.1153\n",
      "244/463, train_loss: 0.0571\n",
      "245/463, train_loss: 0.3552\n",
      "246/463, train_loss: 0.1379\n",
      "247/463, train_loss: 0.0883\n",
      "248/463, train_loss: 0.1084\n",
      "249/463, train_loss: 0.1213\n",
      "250/463, train_loss: 0.3667\n",
      "251/463, train_loss: 0.4604\n",
      "252/463, train_loss: 0.0884\n",
      "253/463, train_loss: 0.1372\n",
      "254/463, train_loss: 0.1979\n",
      "255/463, train_loss: 0.5078\n",
      "256/463, train_loss: 0.1824\n",
      "257/463, train_loss: 0.1310\n",
      "258/463, train_loss: 0.0567\n",
      "259/463, train_loss: 0.1353\n",
      "260/463, train_loss: 0.1378\n",
      "261/463, train_loss: 0.0917\n",
      "262/463, train_loss: 0.1769\n",
      "263/463, train_loss: 0.1237\n",
      "264/463, train_loss: 0.1619\n",
      "265/463, train_loss: 0.0743\n",
      "266/463, train_loss: 0.1982\n",
      "267/463, train_loss: 0.1274\n",
      "268/463, train_loss: 0.1017\n",
      "269/463, train_loss: 0.3833\n",
      "270/463, train_loss: 0.1125\n",
      "271/463, train_loss: 0.0727\n",
      "272/463, train_loss: 0.4524\n",
      "273/463, train_loss: 0.2620\n",
      "274/463, train_loss: 0.1355\n",
      "275/463, train_loss: 0.1527\n",
      "276/463, train_loss: 0.1531\n",
      "277/463, train_loss: 0.3174\n",
      "278/463, train_loss: 0.0781\n",
      "279/463, train_loss: 0.1383\n",
      "280/463, train_loss: 0.3682\n",
      "281/463, train_loss: 0.3982\n",
      "282/463, train_loss: 0.1902\n",
      "283/463, train_loss: 0.1670\n",
      "284/463, train_loss: 0.0989\n",
      "285/463, train_loss: 0.3428\n",
      "286/463, train_loss: 0.2578\n",
      "287/463, train_loss: 0.2141\n",
      "288/463, train_loss: 0.1488\n",
      "289/463, train_loss: 0.2915\n",
      "290/463, train_loss: 0.0914\n",
      "291/463, train_loss: 0.0807\n",
      "292/463, train_loss: 0.0991\n",
      "293/463, train_loss: 0.3833\n",
      "294/463, train_loss: 0.1214\n",
      "295/463, train_loss: 0.2280\n",
      "296/463, train_loss: 0.0863\n",
      "297/463, train_loss: 0.0844\n",
      "298/463, train_loss: 0.1071\n",
      "299/463, train_loss: 0.7749\n",
      "300/463, train_loss: 0.3674\n",
      "301/463, train_loss: 0.1039\n",
      "302/463, train_loss: 0.1538\n",
      "303/463, train_loss: 0.2014\n",
      "304/463, train_loss: 0.3657\n",
      "305/463, train_loss: 0.1372\n",
      "306/463, train_loss: 0.0903\n",
      "307/463, train_loss: 0.3237\n",
      "308/463, train_loss: 0.0772\n",
      "309/463, train_loss: 0.2925\n",
      "310/463, train_loss: 0.2571\n",
      "311/463, train_loss: 0.6265\n",
      "312/463, train_loss: 0.1755\n",
      "313/463, train_loss: 0.4121\n",
      "314/463, train_loss: 0.3208\n",
      "315/463, train_loss: 0.1428\n",
      "316/463, train_loss: 0.1289\n",
      "317/463, train_loss: 0.8276\n",
      "318/463, train_loss: 0.1415\n",
      "319/463, train_loss: 0.1318\n",
      "320/463, train_loss: 0.4941\n",
      "321/463, train_loss: 0.1123\n",
      "322/463, train_loss: 0.5342\n",
      "323/463, train_loss: 0.1357\n",
      "324/463, train_loss: 0.1393\n",
      "325/463, train_loss: 0.1866\n",
      "326/463, train_loss: 0.1333\n",
      "327/463, train_loss: 0.1101\n",
      "328/463, train_loss: 0.0839\n",
      "329/463, train_loss: 0.4058\n",
      "330/463, train_loss: 0.1132\n",
      "331/463, train_loss: 0.1943\n",
      "332/463, train_loss: 0.1963\n",
      "333/463, train_loss: 0.2634\n",
      "334/463, train_loss: 0.0813\n",
      "335/463, train_loss: 0.3845\n",
      "336/463, train_loss: 0.1227\n",
      "337/463, train_loss: 0.0581\n",
      "338/463, train_loss: 0.2194\n",
      "339/463, train_loss: 0.0890\n",
      "340/463, train_loss: 0.1198\n",
      "341/463, train_loss: 0.0841\n",
      "342/463, train_loss: 0.2058\n",
      "343/463, train_loss: 0.1087\n",
      "344/463, train_loss: 0.1089\n",
      "345/463, train_loss: 0.2023\n",
      "346/463, train_loss: 0.1238\n",
      "347/463, train_loss: 0.1108\n",
      "348/463, train_loss: 0.1923\n",
      "349/463, train_loss: 0.1074\n",
      "350/463, train_loss: 0.0692\n",
      "351/463, train_loss: 0.0876\n",
      "352/463, train_loss: 0.2091\n",
      "353/463, train_loss: 0.1360\n",
      "354/463, train_loss: 0.1768\n",
      "355/463, train_loss: 0.1644\n",
      "356/463, train_loss: 0.4868\n",
      "357/463, train_loss: 0.2776\n",
      "358/463, train_loss: 0.1677\n",
      "359/463, train_loss: 0.6338\n",
      "360/463, train_loss: 0.6318\n",
      "361/463, train_loss: 0.1061\n",
      "362/463, train_loss: 0.0981\n",
      "363/463, train_loss: 0.0809\n",
      "364/463, train_loss: 0.1143\n",
      "365/463, train_loss: 0.1035\n",
      "366/463, train_loss: 0.2532\n",
      "367/463, train_loss: 0.1655\n",
      "368/463, train_loss: 0.1150\n",
      "369/463, train_loss: 1.0801\n",
      "370/463, train_loss: 0.1664\n",
      "371/463, train_loss: 0.4753\n",
      "372/463, train_loss: 0.3110\n",
      "373/463, train_loss: 0.2053\n",
      "374/463, train_loss: 0.5581\n",
      "375/463, train_loss: 0.5259\n",
      "376/463, train_loss: 0.1170\n",
      "377/463, train_loss: 0.1323\n",
      "378/463, train_loss: 0.1807\n",
      "379/463, train_loss: 0.3584\n",
      "380/463, train_loss: 0.1350\n",
      "381/463, train_loss: 0.1370\n",
      "382/463, train_loss: 0.1044\n",
      "383/463, train_loss: 0.3230\n",
      "384/463, train_loss: 0.1985\n",
      "385/463, train_loss: 0.1490\n",
      "386/463, train_loss: 0.1620\n",
      "387/463, train_loss: 0.2620\n",
      "388/463, train_loss: 0.1350\n",
      "389/463, train_loss: 0.1565\n",
      "390/463, train_loss: 0.2769\n",
      "391/463, train_loss: 0.1239\n",
      "392/463, train_loss: 0.2458\n",
      "393/463, train_loss: 0.1311\n",
      "394/463, train_loss: 0.3130\n",
      "395/463, train_loss: 0.1377\n",
      "396/463, train_loss: 0.3486\n",
      "397/463, train_loss: 0.1455\n",
      "398/463, train_loss: 0.2167\n",
      "399/463, train_loss: 0.2700\n",
      "400/463, train_loss: 0.1768\n",
      "401/463, train_loss: 0.0966\n",
      "402/463, train_loss: 0.2070\n",
      "403/463, train_loss: 0.1100\n",
      "404/463, train_loss: 0.0784\n",
      "405/463, train_loss: 0.1006\n",
      "406/463, train_loss: 0.2001\n",
      "407/463, train_loss: 0.1377\n",
      "408/463, train_loss: 0.1628\n",
      "409/463, train_loss: 0.1809\n",
      "410/463, train_loss: 0.0803\n",
      "411/463, train_loss: 0.1781\n",
      "412/463, train_loss: 0.4749\n",
      "413/463, train_loss: 0.1542\n",
      "414/463, train_loss: 0.0851\n",
      "415/463, train_loss: 0.4009\n",
      "416/463, train_loss: 0.1418\n",
      "417/463, train_loss: 0.0945\n",
      "418/463, train_loss: 0.2317\n",
      "419/463, train_loss: 0.5127\n",
      "420/463, train_loss: 0.8760\n",
      "421/463, train_loss: 0.1888\n",
      "422/463, train_loss: 0.4180\n",
      "423/463, train_loss: 0.1018\n",
      "424/463, train_loss: 0.1227\n",
      "425/463, train_loss: 0.1138\n",
      "426/463, train_loss: 0.2129\n",
      "427/463, train_loss: 0.1693\n",
      "428/463, train_loss: 0.3081\n",
      "429/463, train_loss: 0.2024\n",
      "430/463, train_loss: 0.3918\n",
      "431/463, train_loss: 0.4741\n",
      "432/463, train_loss: 0.1270\n",
      "433/463, train_loss: 0.3850\n",
      "434/463, train_loss: 0.0940\n",
      "435/463, train_loss: 0.4932\n",
      "436/463, train_loss: 0.0844\n",
      "437/463, train_loss: 0.1458\n",
      "438/463, train_loss: 0.1582\n",
      "439/463, train_loss: 0.0674\n",
      "440/463, train_loss: 0.2240\n",
      "441/463, train_loss: 0.1340\n",
      "442/463, train_loss: 0.1355\n",
      "443/463, train_loss: 0.0950\n",
      "444/463, train_loss: 0.0548\n",
      "445/463, train_loss: 0.0665\n",
      "446/463, train_loss: 0.2834\n",
      "447/463, train_loss: 0.0985\n",
      "448/463, train_loss: 0.2273\n",
      "449/463, train_loss: 0.0617\n",
      "450/463, train_loss: 0.2842\n",
      "451/463, train_loss: 0.1785\n",
      "452/463, train_loss: 0.1438\n",
      "453/463, train_loss: 0.3442\n",
      "454/463, train_loss: 0.3875\n",
      "455/463, train_loss: 0.0647\n",
      "456/463, train_loss: 0.4277\n",
      "457/463, train_loss: 0.1254\n",
      "458/463, train_loss: 0.0974\n",
      "459/463, train_loss: 0.2382\n",
      "460/463, train_loss: 0.1046\n",
      "461/463, train_loss: 0.3923\n",
      "462/463, train_loss: 0.0473\n",
      "463/463, train_loss: 0.1803\n",
      "epoch 18 average loss: 0.2172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 21:24:39 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 21:24:41 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 21:24:44 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 19/100\n",
      "1/463, train_loss: 0.3315\n",
      "2/463, train_loss: 0.1598\n",
      "3/463, train_loss: 0.1180\n",
      "4/463, train_loss: 0.1876\n",
      "5/463, train_loss: 0.3645\n",
      "6/463, train_loss: 0.5449\n",
      "7/463, train_loss: 0.1031\n",
      "8/463, train_loss: 0.1224\n",
      "9/463, train_loss: 0.1212\n",
      "10/463, train_loss: 0.1095\n",
      "11/463, train_loss: 0.1014\n",
      "12/463, train_loss: 0.1499\n",
      "13/463, train_loss: 0.1923\n",
      "14/463, train_loss: 0.2120\n",
      "15/463, train_loss: 0.1255\n",
      "16/463, train_loss: 0.1956\n",
      "17/463, train_loss: 0.5205\n",
      "18/463, train_loss: 0.1149\n",
      "19/463, train_loss: 0.7822\n",
      "20/463, train_loss: 0.0905\n",
      "21/463, train_loss: 0.1229\n",
      "22/463, train_loss: 0.1550\n",
      "23/463, train_loss: 0.1888\n",
      "24/463, train_loss: 0.1477\n",
      "25/463, train_loss: 0.1531\n",
      "26/463, train_loss: 0.1045\n",
      "27/463, train_loss: 0.2554\n",
      "28/463, train_loss: 0.1350\n",
      "29/463, train_loss: 0.3347\n",
      "30/463, train_loss: 0.1002\n",
      "31/463, train_loss: 0.4932\n",
      "32/463, train_loss: 0.2754\n",
      "33/463, train_loss: 0.0938\n",
      "34/463, train_loss: 0.1046\n",
      "35/463, train_loss: 0.1038\n",
      "36/463, train_loss: 0.0417\n",
      "37/463, train_loss: 0.2247\n",
      "38/463, train_loss: 0.2510\n",
      "39/463, train_loss: 0.1752\n",
      "40/463, train_loss: 0.1520\n",
      "41/463, train_loss: 0.0719\n",
      "42/463, train_loss: 0.4993\n",
      "43/463, train_loss: 0.1296\n",
      "44/463, train_loss: 0.0814\n",
      "45/463, train_loss: 0.0538\n",
      "46/463, train_loss: 0.4536\n",
      "47/463, train_loss: 0.0667\n",
      "48/463, train_loss: 0.1406\n",
      "49/463, train_loss: 0.0811\n",
      "50/463, train_loss: 0.1389\n",
      "51/463, train_loss: 0.1458\n",
      "52/463, train_loss: 0.1014\n",
      "53/463, train_loss: 0.2463\n",
      "54/463, train_loss: 0.0690\n",
      "55/463, train_loss: 0.1111\n",
      "56/463, train_loss: 0.4590\n",
      "57/463, train_loss: 0.1022\n",
      "58/463, train_loss: 0.0698\n",
      "59/463, train_loss: 0.0388\n",
      "60/463, train_loss: 0.0857\n",
      "61/463, train_loss: 0.1350\n",
      "62/463, train_loss: 0.6025\n",
      "63/463, train_loss: 0.3145\n",
      "64/463, train_loss: 0.2159\n",
      "65/463, train_loss: 0.1394\n",
      "66/463, train_loss: 0.1304\n",
      "67/463, train_loss: 0.4136\n",
      "68/463, train_loss: 0.0746\n",
      "69/463, train_loss: 0.2539\n",
      "70/463, train_loss: 0.2142\n",
      "71/463, train_loss: 0.1516\n",
      "72/463, train_loss: 0.2385\n",
      "73/463, train_loss: 0.0978\n",
      "74/463, train_loss: 0.1951\n",
      "75/463, train_loss: 0.2197\n",
      "76/463, train_loss: 0.4548\n",
      "77/463, train_loss: 0.1152\n",
      "78/463, train_loss: 0.1006\n",
      "79/463, train_loss: 0.1219\n",
      "80/463, train_loss: 0.4385\n",
      "81/463, train_loss: 0.1177\n",
      "82/463, train_loss: 0.1732\n",
      "83/463, train_loss: 0.1707\n",
      "84/463, train_loss: 0.7402\n",
      "85/463, train_loss: 0.1593\n",
      "86/463, train_loss: 0.0782\n",
      "87/463, train_loss: 0.0793\n",
      "88/463, train_loss: 0.2185\n",
      "89/463, train_loss: 0.0729\n",
      "90/463, train_loss: 0.0812\n",
      "91/463, train_loss: 0.0901\n",
      "92/463, train_loss: 0.1027\n",
      "93/463, train_loss: 0.1257\n",
      "94/463, train_loss: 0.1179\n",
      "95/463, train_loss: 0.3342\n",
      "96/463, train_loss: 0.1903\n",
      "97/463, train_loss: 0.1685\n",
      "98/463, train_loss: 0.1729\n",
      "99/463, train_loss: 0.2351\n",
      "100/463, train_loss: 0.1656\n",
      "101/463, train_loss: 0.1444\n",
      "102/463, train_loss: 0.0548\n",
      "103/463, train_loss: 0.4570\n",
      "104/463, train_loss: 0.0894\n",
      "105/463, train_loss: 0.2295\n",
      "106/463, train_loss: 0.0874\n",
      "107/463, train_loss: 0.0807\n",
      "108/463, train_loss: 0.2703\n",
      "109/463, train_loss: 0.1036\n",
      "110/463, train_loss: 0.1267\n",
      "111/463, train_loss: 0.0682\n",
      "112/463, train_loss: 0.1527\n",
      "113/463, train_loss: 0.1064\n",
      "114/463, train_loss: 0.1387\n",
      "115/463, train_loss: 0.2473\n",
      "116/463, train_loss: 0.1044\n",
      "117/463, train_loss: 0.1880\n",
      "118/463, train_loss: 0.1260\n",
      "119/463, train_loss: 0.1938\n",
      "120/463, train_loss: 0.1211\n",
      "121/463, train_loss: 0.1333\n",
      "122/463, train_loss: 0.0800\n",
      "123/463, train_loss: 0.4993\n",
      "124/463, train_loss: 0.1436\n",
      "125/463, train_loss: 0.1758\n",
      "126/463, train_loss: 0.1532\n",
      "127/463, train_loss: 0.2893\n",
      "128/463, train_loss: 0.8730\n",
      "129/463, train_loss: 0.0925\n",
      "130/463, train_loss: 0.1741\n",
      "131/463, train_loss: 0.1315\n",
      "132/463, train_loss: 0.1094\n",
      "133/463, train_loss: 0.0671\n",
      "134/463, train_loss: 0.1714\n",
      "135/463, train_loss: 0.2795\n",
      "136/463, train_loss: 0.1968\n",
      "137/463, train_loss: 0.1283\n",
      "138/463, train_loss: 0.1774\n",
      "139/463, train_loss: 0.7422\n",
      "140/463, train_loss: 0.1162\n",
      "141/463, train_loss: 0.2069\n",
      "142/463, train_loss: 0.0878\n",
      "143/463, train_loss: 0.2090\n",
      "144/463, train_loss: 0.1790\n",
      "145/463, train_loss: 0.9609\n",
      "146/463, train_loss: 0.0415\n",
      "147/463, train_loss: 0.1451\n",
      "148/463, train_loss: 0.0390\n",
      "149/463, train_loss: 0.0526\n",
      "150/463, train_loss: 0.1843\n",
      "151/463, train_loss: 0.2920\n",
      "152/463, train_loss: 0.1699\n",
      "153/463, train_loss: 0.1514\n",
      "154/463, train_loss: 0.2147\n",
      "155/463, train_loss: 0.1479\n",
      "156/463, train_loss: 0.3420\n",
      "157/463, train_loss: 0.2395\n",
      "158/463, train_loss: 0.0745\n",
      "159/463, train_loss: 0.1180\n",
      "160/463, train_loss: 0.1360\n",
      "161/463, train_loss: 0.1050\n",
      "162/463, train_loss: 0.1024\n",
      "163/463, train_loss: 0.1149\n",
      "164/463, train_loss: 0.1558\n",
      "165/463, train_loss: 0.1774\n",
      "166/463, train_loss: 0.2109\n",
      "167/463, train_loss: 0.2996\n",
      "168/463, train_loss: 0.1111\n",
      "169/463, train_loss: 0.1324\n",
      "170/463, train_loss: 0.9321\n",
      "171/463, train_loss: 0.1326\n",
      "172/463, train_loss: 0.2542\n",
      "173/463, train_loss: 0.3723\n",
      "174/463, train_loss: 0.2595\n",
      "175/463, train_loss: 0.1089\n",
      "176/463, train_loss: 0.1392\n",
      "177/463, train_loss: 0.2006\n",
      "178/463, train_loss: 0.2629\n",
      "179/463, train_loss: 0.1660\n",
      "180/463, train_loss: 0.1146\n",
      "181/463, train_loss: 0.2639\n",
      "182/463, train_loss: 0.2200\n",
      "183/463, train_loss: 0.1517\n",
      "184/463, train_loss: 0.1768\n",
      "185/463, train_loss: 0.0856\n",
      "186/463, train_loss: 0.2449\n",
      "187/463, train_loss: 0.3352\n",
      "188/463, train_loss: 0.1582\n",
      "189/463, train_loss: 0.1251\n",
      "190/463, train_loss: 0.9468\n",
      "191/463, train_loss: 0.3499\n",
      "192/463, train_loss: 0.2292\n",
      "193/463, train_loss: 0.1221\n",
      "194/463, train_loss: 0.0647\n",
      "195/463, train_loss: 0.1197\n",
      "196/463, train_loss: 0.0660\n",
      "197/463, train_loss: 0.0573\n",
      "198/463, train_loss: 0.0866\n",
      "199/463, train_loss: 0.0786\n",
      "200/463, train_loss: 0.1178\n",
      "201/463, train_loss: 0.0620\n",
      "202/463, train_loss: 0.2578\n",
      "203/463, train_loss: 0.6890\n",
      "204/463, train_loss: 0.2181\n",
      "205/463, train_loss: 0.3315\n",
      "206/463, train_loss: 0.3289\n",
      "207/463, train_loss: 0.4050\n",
      "208/463, train_loss: 0.2271\n",
      "209/463, train_loss: 0.1177\n",
      "210/463, train_loss: 0.2151\n",
      "211/463, train_loss: 0.1725\n",
      "212/463, train_loss: 0.3989\n",
      "213/463, train_loss: 0.1895\n",
      "214/463, train_loss: 0.1940\n",
      "215/463, train_loss: 0.1934\n",
      "216/463, train_loss: 0.2886\n",
      "217/463, train_loss: 0.1211\n",
      "218/463, train_loss: 0.2438\n",
      "219/463, train_loss: 0.1315\n",
      "220/463, train_loss: 0.2201\n",
      "221/463, train_loss: 0.2283\n",
      "222/463, train_loss: 0.1306\n",
      "223/463, train_loss: 0.2515\n",
      "224/463, train_loss: 0.3584\n",
      "225/463, train_loss: 0.3799\n",
      "226/463, train_loss: 0.0665\n",
      "227/463, train_loss: 0.3145\n",
      "228/463, train_loss: 0.2410\n",
      "229/463, train_loss: 0.4204\n",
      "230/463, train_loss: 0.1661\n",
      "231/463, train_loss: 0.1509\n",
      "232/463, train_loss: 0.1544\n",
      "233/463, train_loss: 0.1438\n",
      "234/463, train_loss: 0.2284\n",
      "235/463, train_loss: 0.3542\n",
      "236/463, train_loss: 0.3281\n",
      "237/463, train_loss: 0.1934\n",
      "238/463, train_loss: 0.0931\n",
      "239/463, train_loss: 0.2263\n",
      "240/463, train_loss: 0.3577\n",
      "241/463, train_loss: 0.2458\n",
      "242/463, train_loss: 0.1036\n",
      "243/463, train_loss: 0.1345\n",
      "244/463, train_loss: 0.2056\n",
      "245/463, train_loss: 0.3452\n",
      "246/463, train_loss: 0.0765\n",
      "247/463, train_loss: 0.1464\n",
      "248/463, train_loss: 0.2386\n",
      "249/463, train_loss: 0.0950\n",
      "250/463, train_loss: 0.1077\n",
      "251/463, train_loss: 0.1760\n",
      "252/463, train_loss: 0.1027\n",
      "253/463, train_loss: 0.2495\n",
      "254/463, train_loss: 0.5063\n",
      "255/463, train_loss: 0.3271\n",
      "256/463, train_loss: 0.1225\n",
      "257/463, train_loss: 0.1399\n",
      "258/463, train_loss: 0.1123\n",
      "259/463, train_loss: 0.1189\n",
      "260/463, train_loss: 0.0757\n",
      "261/463, train_loss: 0.1260\n",
      "262/463, train_loss: 0.0890\n",
      "263/463, train_loss: 0.1639\n",
      "264/463, train_loss: 0.1895\n",
      "265/463, train_loss: 0.1758\n",
      "266/463, train_loss: 0.4119\n",
      "267/463, train_loss: 0.1057\n",
      "268/463, train_loss: 0.0681\n",
      "269/463, train_loss: 0.1202\n",
      "270/463, train_loss: 0.0439\n",
      "271/463, train_loss: 0.0993\n",
      "272/463, train_loss: 0.2502\n",
      "273/463, train_loss: 0.5391\n",
      "274/463, train_loss: 0.1947\n",
      "275/463, train_loss: 0.0876\n",
      "276/463, train_loss: 0.4282\n",
      "277/463, train_loss: 0.1467\n",
      "278/463, train_loss: 0.2041\n",
      "279/463, train_loss: 0.1122\n",
      "280/463, train_loss: 0.1434\n",
      "281/463, train_loss: 0.1093\n",
      "282/463, train_loss: 0.1448\n",
      "283/463, train_loss: 0.1226\n",
      "284/463, train_loss: 0.2920\n",
      "285/463, train_loss: 0.1299\n",
      "286/463, train_loss: 0.6455\n",
      "287/463, train_loss: 0.2253\n",
      "288/463, train_loss: 0.0563\n",
      "289/463, train_loss: 0.0640\n",
      "290/463, train_loss: 0.2178\n",
      "291/463, train_loss: 0.1169\n",
      "292/463, train_loss: 0.2468\n",
      "293/463, train_loss: 0.3967\n",
      "294/463, train_loss: 0.4429\n",
      "295/463, train_loss: 0.0862\n",
      "296/463, train_loss: 0.1497\n",
      "297/463, train_loss: 0.0643\n",
      "298/463, train_loss: 0.0783\n",
      "299/463, train_loss: 0.1470\n",
      "300/463, train_loss: 0.1494\n",
      "301/463, train_loss: 0.1136\n",
      "302/463, train_loss: 0.1281\n",
      "303/463, train_loss: 0.1682\n",
      "304/463, train_loss: 0.6899\n",
      "305/463, train_loss: 0.1207\n",
      "306/463, train_loss: 0.2356\n",
      "307/463, train_loss: 0.4426\n",
      "308/463, train_loss: 0.6582\n",
      "309/463, train_loss: 0.2046\n",
      "310/463, train_loss: 0.2649\n",
      "311/463, train_loss: 0.1023\n",
      "312/463, train_loss: 0.1586\n",
      "313/463, train_loss: 0.1318\n",
      "314/463, train_loss: 0.0919\n",
      "315/463, train_loss: 0.1774\n",
      "316/463, train_loss: 0.4651\n",
      "317/463, train_loss: 0.1476\n",
      "318/463, train_loss: 0.2769\n",
      "319/463, train_loss: 0.1158\n",
      "320/463, train_loss: 0.1036\n",
      "321/463, train_loss: 0.0884\n",
      "322/463, train_loss: 0.5928\n",
      "323/463, train_loss: 0.1869\n",
      "324/463, train_loss: 0.0979\n",
      "325/463, train_loss: 0.1324\n",
      "326/463, train_loss: 0.0791\n",
      "327/463, train_loss: 0.2468\n",
      "328/463, train_loss: 0.1628\n",
      "329/463, train_loss: 0.0779\n",
      "330/463, train_loss: 0.2971\n",
      "331/463, train_loss: 0.0975\n",
      "332/463, train_loss: 0.2083\n",
      "333/463, train_loss: 0.1895\n",
      "334/463, train_loss: 0.3804\n",
      "335/463, train_loss: 0.1216\n",
      "336/463, train_loss: 0.1541\n",
      "337/463, train_loss: 0.0955\n",
      "338/463, train_loss: 0.3779\n",
      "339/463, train_loss: 0.4478\n",
      "340/463, train_loss: 0.2433\n",
      "341/463, train_loss: 0.2725\n",
      "342/463, train_loss: 0.1772\n",
      "343/463, train_loss: 0.0569\n",
      "344/463, train_loss: 0.4158\n",
      "345/463, train_loss: 0.1471\n",
      "346/463, train_loss: 0.0874\n",
      "347/463, train_loss: 0.2539\n",
      "348/463, train_loss: 0.1580\n",
      "349/463, train_loss: 0.0561\n",
      "350/463, train_loss: 0.1411\n",
      "351/463, train_loss: 0.4482\n",
      "352/463, train_loss: 0.1786\n",
      "353/463, train_loss: 0.1698\n",
      "354/463, train_loss: 0.1713\n",
      "355/463, train_loss: 0.0898\n",
      "356/463, train_loss: 0.2458\n",
      "357/463, train_loss: 0.1554\n",
      "358/463, train_loss: 0.1351\n",
      "359/463, train_loss: 0.4097\n",
      "360/463, train_loss: 0.1049\n",
      "361/463, train_loss: 0.2629\n",
      "362/463, train_loss: 0.1666\n",
      "363/463, train_loss: 0.2346\n",
      "364/463, train_loss: 0.2019\n",
      "365/463, train_loss: 0.2091\n",
      "366/463, train_loss: 0.5664\n",
      "367/463, train_loss: 0.1099\n",
      "368/463, train_loss: 0.1053\n",
      "369/463, train_loss: 0.1278\n",
      "370/463, train_loss: 0.1774\n",
      "371/463, train_loss: 0.1035\n",
      "372/463, train_loss: 0.1084\n",
      "373/463, train_loss: 1.1816\n",
      "374/463, train_loss: 0.2056\n",
      "375/463, train_loss: 0.2732\n",
      "376/463, train_loss: 1.1797\n",
      "377/463, train_loss: 0.1780\n",
      "378/463, train_loss: 0.1157\n",
      "379/463, train_loss: 0.1980\n",
      "380/463, train_loss: 0.0863\n",
      "381/463, train_loss: 0.1838\n",
      "382/463, train_loss: 0.1989\n",
      "383/463, train_loss: 0.2078\n",
      "384/463, train_loss: 0.2637\n",
      "385/463, train_loss: 0.2188\n",
      "386/463, train_loss: 0.2266\n",
      "387/463, train_loss: 0.3340\n",
      "388/463, train_loss: 0.0964\n",
      "389/463, train_loss: 0.0720\n",
      "390/463, train_loss: 0.1022\n",
      "391/463, train_loss: 0.1992\n",
      "392/463, train_loss: 0.2260\n",
      "393/463, train_loss: 0.0746\n",
      "394/463, train_loss: 0.5210\n",
      "395/463, train_loss: 0.2054\n",
      "396/463, train_loss: 0.2209\n",
      "397/463, train_loss: 0.6240\n",
      "398/463, train_loss: 0.0846\n",
      "399/463, train_loss: 0.1140\n",
      "400/463, train_loss: 0.2581\n",
      "401/463, train_loss: 0.0800\n",
      "402/463, train_loss: 0.0646\n",
      "403/463, train_loss: 0.1489\n",
      "404/463, train_loss: 0.1123\n",
      "405/463, train_loss: 0.1724\n",
      "406/463, train_loss: 0.3208\n",
      "407/463, train_loss: 0.1428\n",
      "408/463, train_loss: 0.2158\n",
      "409/463, train_loss: 0.1860\n",
      "410/463, train_loss: 0.1389\n",
      "411/463, train_loss: 0.1213\n",
      "412/463, train_loss: 0.1560\n",
      "413/463, train_loss: 0.0599\n",
      "414/463, train_loss: 0.1410\n",
      "415/463, train_loss: 0.1197\n",
      "416/463, train_loss: 0.0873\n",
      "417/463, train_loss: 0.4348\n",
      "418/463, train_loss: 0.1488\n",
      "419/463, train_loss: 0.1722\n",
      "420/463, train_loss: 0.1304\n",
      "421/463, train_loss: 0.1335\n",
      "422/463, train_loss: 0.6807\n",
      "423/463, train_loss: 0.1528\n",
      "424/463, train_loss: 0.1140\n",
      "425/463, train_loss: 0.6357\n",
      "426/463, train_loss: 0.0862\n",
      "427/463, train_loss: 0.3455\n",
      "428/463, train_loss: 0.1816\n",
      "429/463, train_loss: 0.2341\n",
      "430/463, train_loss: 0.1067\n",
      "431/463, train_loss: 0.3276\n",
      "432/463, train_loss: 0.1708\n",
      "433/463, train_loss: 0.1892\n",
      "434/463, train_loss: 0.0981\n",
      "435/463, train_loss: 0.1072\n",
      "436/463, train_loss: 0.0567\n",
      "437/463, train_loss: 0.1624\n",
      "438/463, train_loss: 0.1332\n",
      "439/463, train_loss: 0.0685\n",
      "440/463, train_loss: 0.1810\n",
      "441/463, train_loss: 0.1006\n",
      "442/463, train_loss: 0.2756\n",
      "443/463, train_loss: 0.0945\n",
      "444/463, train_loss: 0.1305\n",
      "445/463, train_loss: 0.1824\n",
      "446/463, train_loss: 0.2416\n",
      "447/463, train_loss: 0.2061\n",
      "448/463, train_loss: 0.3879\n",
      "449/463, train_loss: 0.2467\n",
      "450/463, train_loss: 0.3716\n",
      "451/463, train_loss: 0.1460\n",
      "452/463, train_loss: 0.1489\n",
      "453/463, train_loss: 0.3030\n",
      "454/463, train_loss: 0.0545\n",
      "455/463, train_loss: 0.4338\n",
      "456/463, train_loss: 0.3577\n",
      "457/463, train_loss: 0.1285\n",
      "458/463, train_loss: 0.2048\n",
      "459/463, train_loss: 0.1401\n",
      "460/463, train_loss: 0.1095\n",
      "461/463, train_loss: 0.1146\n",
      "462/463, train_loss: 0.4771\n",
      "463/463, train_loss: 0.1245\n",
      "epoch 19 average loss: 0.2079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/21 23:37:14 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 23:37:16 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/21 23:37:19 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 20/100\n",
      "1/463, train_loss: 0.0599\n",
      "2/463, train_loss: 0.2881\n",
      "3/463, train_loss: 0.1479\n",
      "4/463, train_loss: 0.0688\n",
      "5/463, train_loss: 0.4307\n",
      "6/463, train_loss: 0.3164\n",
      "7/463, train_loss: 0.1289\n",
      "8/463, train_loss: 0.1460\n",
      "9/463, train_loss: 0.0810\n",
      "10/463, train_loss: 0.1824\n",
      "11/463, train_loss: 0.1106\n",
      "12/463, train_loss: 0.1515\n",
      "13/463, train_loss: 0.0984\n",
      "14/463, train_loss: 0.1185\n",
      "15/463, train_loss: 0.2070\n",
      "16/463, train_loss: 0.5732\n",
      "17/463, train_loss: 0.0842\n",
      "18/463, train_loss: 0.5625\n",
      "19/463, train_loss: 0.1525\n",
      "20/463, train_loss: 0.1360\n",
      "21/463, train_loss: 0.1146\n",
      "22/463, train_loss: 0.2903\n",
      "23/463, train_loss: 0.3518\n",
      "24/463, train_loss: 0.1587\n",
      "25/463, train_loss: 0.3340\n",
      "26/463, train_loss: 0.1956\n",
      "27/463, train_loss: 0.1328\n",
      "28/463, train_loss: 0.2366\n",
      "29/463, train_loss: 0.1436\n",
      "30/463, train_loss: 0.1183\n",
      "31/463, train_loss: 0.1659\n",
      "32/463, train_loss: 0.1632\n",
      "33/463, train_loss: 0.2004\n",
      "34/463, train_loss: 0.1248\n",
      "35/463, train_loss: 0.0981\n",
      "36/463, train_loss: 0.1024\n",
      "37/463, train_loss: 0.1100\n",
      "38/463, train_loss: 0.0745\n",
      "39/463, train_loss: 0.0815\n",
      "40/463, train_loss: 0.1658\n",
      "41/463, train_loss: 0.3518\n",
      "42/463, train_loss: 0.0993\n",
      "43/463, train_loss: 0.2654\n",
      "44/463, train_loss: 0.1565\n",
      "45/463, train_loss: 0.1599\n",
      "46/463, train_loss: 0.2040\n",
      "47/463, train_loss: 0.0964\n",
      "48/463, train_loss: 0.3711\n",
      "49/463, train_loss: 0.0495\n",
      "50/463, train_loss: 0.0905\n",
      "51/463, train_loss: 0.1777\n",
      "52/463, train_loss: 0.7100\n",
      "53/463, train_loss: 0.1465\n",
      "54/463, train_loss: 0.0670\n",
      "55/463, train_loss: 0.2993\n",
      "56/463, train_loss: 0.2021\n",
      "57/463, train_loss: 0.1157\n",
      "58/463, train_loss: 0.1567\n",
      "59/463, train_loss: 0.0778\n",
      "60/463, train_loss: 0.1714\n",
      "61/463, train_loss: 0.0922\n",
      "62/463, train_loss: 0.1279\n",
      "63/463, train_loss: 0.1854\n",
      "64/463, train_loss: 0.1136\n",
      "65/463, train_loss: 0.1178\n",
      "66/463, train_loss: 0.0833\n",
      "67/463, train_loss: 0.0975\n",
      "68/463, train_loss: 0.5986\n",
      "69/463, train_loss: 0.0778\n",
      "70/463, train_loss: 0.1503\n",
      "71/463, train_loss: 0.1089\n",
      "72/463, train_loss: 0.0879\n",
      "73/463, train_loss: 0.1499\n",
      "74/463, train_loss: 0.1316\n",
      "75/463, train_loss: 0.0939\n",
      "76/463, train_loss: 0.1478\n",
      "77/463, train_loss: 0.0974\n",
      "78/463, train_loss: 0.1062\n",
      "79/463, train_loss: 0.4705\n",
      "80/463, train_loss: 0.1699\n",
      "81/463, train_loss: 0.0886\n",
      "82/463, train_loss: 0.1669\n",
      "83/463, train_loss: 0.3970\n",
      "84/463, train_loss: 0.0823\n",
      "85/463, train_loss: 0.1541\n",
      "86/463, train_loss: 0.2278\n",
      "87/463, train_loss: 0.0767\n",
      "88/463, train_loss: 0.0871\n",
      "89/463, train_loss: 0.0880\n",
      "90/463, train_loss: 0.4009\n",
      "91/463, train_loss: 0.0961\n",
      "92/463, train_loss: 0.3306\n",
      "93/463, train_loss: 0.4165\n",
      "94/463, train_loss: 0.1342\n",
      "95/463, train_loss: 0.1841\n",
      "96/463, train_loss: 0.1736\n",
      "97/463, train_loss: 0.1561\n",
      "98/463, train_loss: 0.4436\n",
      "99/463, train_loss: 0.1548\n",
      "100/463, train_loss: 0.3667\n",
      "101/463, train_loss: 0.0982\n",
      "102/463, train_loss: 0.2742\n",
      "103/463, train_loss: 0.1760\n",
      "104/463, train_loss: 0.0736\n",
      "105/463, train_loss: 0.7290\n",
      "106/463, train_loss: 0.0735\n",
      "107/463, train_loss: 0.1863\n",
      "108/463, train_loss: 0.0507\n",
      "109/463, train_loss: 0.2505\n",
      "110/463, train_loss: 0.1799\n",
      "111/463, train_loss: 0.0598\n",
      "112/463, train_loss: 0.1711\n",
      "113/463, train_loss: 0.0680\n",
      "114/463, train_loss: 0.1152\n",
      "115/463, train_loss: 0.1462\n",
      "116/463, train_loss: 0.0858\n",
      "117/463, train_loss: 0.0474\n",
      "118/463, train_loss: 0.1515\n",
      "119/463, train_loss: 0.2153\n",
      "120/463, train_loss: 0.0649\n",
      "121/463, train_loss: 0.1394\n",
      "122/463, train_loss: 0.2786\n",
      "123/463, train_loss: 0.1011\n",
      "124/463, train_loss: 0.2311\n",
      "125/463, train_loss: 0.1469\n",
      "126/463, train_loss: 0.0820\n",
      "127/463, train_loss: 0.8154\n",
      "128/463, train_loss: 0.2094\n",
      "129/463, train_loss: 0.0942\n",
      "130/463, train_loss: 0.4680\n",
      "131/463, train_loss: 0.2382\n",
      "132/463, train_loss: 0.1858\n",
      "133/463, train_loss: 0.1602\n",
      "134/463, train_loss: 0.0899\n",
      "135/463, train_loss: 0.1250\n",
      "136/463, train_loss: 0.3450\n",
      "137/463, train_loss: 0.3853\n",
      "138/463, train_loss: 0.1407\n",
      "139/463, train_loss: 0.2407\n",
      "140/463, train_loss: 0.1174\n",
      "141/463, train_loss: 0.2070\n",
      "142/463, train_loss: 0.0903\n",
      "143/463, train_loss: 0.1042\n",
      "144/463, train_loss: 0.1248\n",
      "145/463, train_loss: 0.1249\n",
      "146/463, train_loss: 0.1426\n",
      "147/463, train_loss: 0.4788\n",
      "148/463, train_loss: 0.1322\n",
      "149/463, train_loss: 0.2384\n",
      "150/463, train_loss: 0.2059\n",
      "151/463, train_loss: 0.2214\n",
      "152/463, train_loss: 0.1758\n",
      "153/463, train_loss: 0.2429\n",
      "154/463, train_loss: 0.2993\n",
      "155/463, train_loss: 0.2362\n",
      "156/463, train_loss: 0.2125\n",
      "157/463, train_loss: 0.0792\n",
      "158/463, train_loss: 0.1433\n",
      "159/463, train_loss: 0.3210\n",
      "160/463, train_loss: 0.2029\n",
      "161/463, train_loss: 0.2043\n",
      "162/463, train_loss: 0.1383\n",
      "163/463, train_loss: 0.2715\n",
      "164/463, train_loss: 0.0844\n",
      "165/463, train_loss: 0.1051\n",
      "166/463, train_loss: 0.1302\n",
      "167/463, train_loss: 0.6211\n",
      "168/463, train_loss: 0.0964\n",
      "169/463, train_loss: 0.3435\n",
      "170/463, train_loss: 0.1943\n",
      "171/463, train_loss: 0.1421\n",
      "172/463, train_loss: 0.2026\n",
      "173/463, train_loss: 0.3772\n",
      "174/463, train_loss: 0.0671\n",
      "175/463, train_loss: 0.1390\n",
      "176/463, train_loss: 0.1340\n",
      "177/463, train_loss: 0.1073\n",
      "178/463, train_loss: 0.1704\n",
      "179/463, train_loss: 0.1354\n",
      "180/463, train_loss: 0.1742\n",
      "181/463, train_loss: 0.4465\n",
      "182/463, train_loss: 0.0796\n",
      "183/463, train_loss: 0.2498\n",
      "184/463, train_loss: 0.1825\n",
      "185/463, train_loss: 0.1138\n",
      "186/463, train_loss: 0.1455\n",
      "187/463, train_loss: 0.4163\n",
      "188/463, train_loss: 0.3228\n",
      "189/463, train_loss: 0.0910\n",
      "190/463, train_loss: 0.2505\n",
      "191/463, train_loss: 0.0845\n",
      "192/463, train_loss: 0.2988\n",
      "193/463, train_loss: 0.1082\n",
      "194/463, train_loss: 0.1560\n",
      "195/463, train_loss: 0.0847\n",
      "196/463, train_loss: 0.0545\n",
      "197/463, train_loss: 0.2026\n",
      "198/463, train_loss: 0.1323\n",
      "199/463, train_loss: 0.3237\n",
      "200/463, train_loss: 0.0561\n",
      "201/463, train_loss: 0.4253\n",
      "202/463, train_loss: 0.0685\n",
      "203/463, train_loss: 0.0724\n",
      "204/463, train_loss: 0.2947\n",
      "205/463, train_loss: 0.4194\n",
      "206/463, train_loss: 0.1135\n",
      "207/463, train_loss: 0.2549\n",
      "208/463, train_loss: 0.1154\n",
      "209/463, train_loss: 0.6089\n",
      "210/463, train_loss: 0.2520\n",
      "211/463, train_loss: 0.0319\n",
      "212/463, train_loss: 0.1057\n",
      "213/463, train_loss: 0.4600\n",
      "214/463, train_loss: 0.1073\n",
      "215/463, train_loss: 0.2988\n",
      "216/463, train_loss: 0.1355\n",
      "217/463, train_loss: 0.1492\n",
      "218/463, train_loss: 0.1934\n",
      "219/463, train_loss: 0.1180\n",
      "220/463, train_loss: 0.1383\n",
      "221/463, train_loss: 0.2280\n",
      "222/463, train_loss: 0.2529\n",
      "223/463, train_loss: 0.1495\n",
      "224/463, train_loss: 0.2412\n",
      "225/463, train_loss: 0.0999\n",
      "226/463, train_loss: 0.0589\n",
      "227/463, train_loss: 0.1482\n",
      "228/463, train_loss: 0.1196\n",
      "229/463, train_loss: 0.3259\n",
      "230/463, train_loss: 0.2097\n",
      "231/463, train_loss: 0.2754\n",
      "232/463, train_loss: 0.3660\n",
      "233/463, train_loss: 0.1819\n",
      "234/463, train_loss: 0.1169\n",
      "235/463, train_loss: 0.4114\n",
      "236/463, train_loss: 0.2529\n",
      "237/463, train_loss: 0.1486\n",
      "238/463, train_loss: 0.3179\n",
      "239/463, train_loss: 0.1663\n",
      "240/463, train_loss: 0.0602\n",
      "241/463, train_loss: 0.2234\n",
      "242/463, train_loss: 0.1531\n",
      "243/463, train_loss: 0.1437\n",
      "244/463, train_loss: 0.0790\n",
      "245/463, train_loss: 0.1576\n",
      "246/463, train_loss: 0.4502\n",
      "247/463, train_loss: 0.0863\n",
      "248/463, train_loss: 0.4233\n",
      "249/463, train_loss: 0.0878\n",
      "250/463, train_loss: 0.6631\n",
      "251/463, train_loss: 0.1736\n",
      "252/463, train_loss: 0.2490\n",
      "253/463, train_loss: 0.1377\n",
      "254/463, train_loss: 0.1821\n",
      "255/463, train_loss: 0.2266\n",
      "256/463, train_loss: 0.1231\n",
      "257/463, train_loss: 0.1948\n",
      "258/463, train_loss: 0.1886\n",
      "259/463, train_loss: 0.1377\n",
      "260/463, train_loss: 0.1067\n",
      "261/463, train_loss: 0.1533\n",
      "262/463, train_loss: 0.3750\n",
      "263/463, train_loss: 1.0195\n",
      "264/463, train_loss: 0.4546\n",
      "265/463, train_loss: 0.0830\n",
      "266/463, train_loss: 0.2291\n",
      "267/463, train_loss: 0.1609\n",
      "268/463, train_loss: 0.1081\n",
      "269/463, train_loss: 0.4778\n",
      "270/463, train_loss: 0.1011\n",
      "271/463, train_loss: 0.2214\n",
      "272/463, train_loss: 0.1212\n",
      "273/463, train_loss: 0.0967\n",
      "274/463, train_loss: 0.0701\n",
      "275/463, train_loss: 0.3494\n",
      "276/463, train_loss: 0.4165\n",
      "277/463, train_loss: 0.0973\n",
      "278/463, train_loss: 0.1455\n",
      "279/463, train_loss: 0.1648\n",
      "280/463, train_loss: 0.0720\n",
      "281/463, train_loss: 0.0910\n",
      "282/463, train_loss: 0.1051\n",
      "283/463, train_loss: 0.1382\n",
      "284/463, train_loss: 0.2117\n",
      "285/463, train_loss: 0.2598\n",
      "286/463, train_loss: 0.1071\n",
      "287/463, train_loss: 0.1241\n",
      "288/463, train_loss: 0.1914\n",
      "289/463, train_loss: 0.1112\n",
      "290/463, train_loss: 0.1489\n",
      "291/463, train_loss: 0.7051\n",
      "292/463, train_loss: 0.1260\n",
      "293/463, train_loss: 0.1724\n",
      "294/463, train_loss: 0.0624\n",
      "295/463, train_loss: 0.1328\n",
      "296/463, train_loss: 0.2937\n",
      "297/463, train_loss: 0.1183\n",
      "298/463, train_loss: 0.0617\n",
      "299/463, train_loss: 0.0622\n",
      "300/463, train_loss: 0.2277\n",
      "301/463, train_loss: 0.6323\n",
      "302/463, train_loss: 0.1638\n",
      "303/463, train_loss: 0.1484\n",
      "304/463, train_loss: 0.2310\n",
      "305/463, train_loss: 0.3499\n",
      "306/463, train_loss: 0.3887\n",
      "307/463, train_loss: 0.2544\n",
      "308/463, train_loss: 0.1646\n",
      "309/463, train_loss: 0.4990\n",
      "310/463, train_loss: 0.1008\n",
      "311/463, train_loss: 0.1136\n",
      "312/463, train_loss: 0.2183\n",
      "313/463, train_loss: 0.1475\n",
      "314/463, train_loss: 0.3481\n",
      "315/463, train_loss: 0.2661\n",
      "316/463, train_loss: 0.4824\n",
      "317/463, train_loss: 0.1061\n",
      "318/463, train_loss: 0.0880\n",
      "319/463, train_loss: 0.2489\n",
      "320/463, train_loss: 0.2542\n",
      "321/463, train_loss: 0.3413\n",
      "322/463, train_loss: 0.2156\n",
      "323/463, train_loss: 0.1758\n",
      "324/463, train_loss: 0.1743\n",
      "325/463, train_loss: 0.2817\n",
      "326/463, train_loss: 0.1307\n",
      "327/463, train_loss: 0.4287\n",
      "328/463, train_loss: 0.1261\n",
      "329/463, train_loss: 0.0414\n",
      "330/463, train_loss: 0.1470\n",
      "331/463, train_loss: 0.0863\n",
      "332/463, train_loss: 0.4231\n",
      "333/463, train_loss: 0.1222\n",
      "334/463, train_loss: 0.2007\n",
      "335/463, train_loss: 0.1012\n",
      "336/463, train_loss: 0.0609\n",
      "337/463, train_loss: 0.2410\n",
      "338/463, train_loss: 0.0538\n",
      "339/463, train_loss: 0.0328\n",
      "340/463, train_loss: 0.1860\n",
      "341/463, train_loss: 0.0707\n",
      "342/463, train_loss: 0.2012\n",
      "343/463, train_loss: 0.1624\n",
      "344/463, train_loss: 0.1652\n",
      "345/463, train_loss: 0.1058\n",
      "346/463, train_loss: 0.0887\n",
      "347/463, train_loss: 0.4268\n",
      "348/463, train_loss: 0.2096\n",
      "349/463, train_loss: 0.2401\n",
      "350/463, train_loss: 0.2568\n",
      "351/463, train_loss: 0.1476\n",
      "352/463, train_loss: 0.2927\n",
      "353/463, train_loss: 0.1262\n",
      "354/463, train_loss: 0.3647\n",
      "355/463, train_loss: 0.0470\n",
      "356/463, train_loss: 0.4509\n",
      "357/463, train_loss: 0.1477\n",
      "358/463, train_loss: 0.1737\n",
      "359/463, train_loss: 0.1230\n",
      "360/463, train_loss: 0.0252\n",
      "361/463, train_loss: 0.1328\n",
      "362/463, train_loss: 0.1479\n",
      "363/463, train_loss: 0.1584\n",
      "364/463, train_loss: 0.1473\n",
      "365/463, train_loss: 0.1058\n",
      "366/463, train_loss: 0.1948\n",
      "367/463, train_loss: 0.1897\n",
      "368/463, train_loss: 0.1289\n",
      "369/463, train_loss: 0.2769\n",
      "370/463, train_loss: 0.0834\n",
      "371/463, train_loss: 0.3154\n",
      "372/463, train_loss: 0.2267\n",
      "373/463, train_loss: 0.1509\n",
      "374/463, train_loss: 0.1174\n",
      "375/463, train_loss: 0.1019\n",
      "376/463, train_loss: 0.4258\n",
      "377/463, train_loss: 0.7344\n",
      "378/463, train_loss: 0.0948\n",
      "379/463, train_loss: 0.4600\n",
      "380/463, train_loss: 0.5308\n",
      "381/463, train_loss: 0.2249\n",
      "382/463, train_loss: 0.1181\n",
      "383/463, train_loss: 0.0964\n",
      "384/463, train_loss: 0.1118\n",
      "385/463, train_loss: 0.2939\n",
      "386/463, train_loss: 0.5630\n",
      "387/463, train_loss: 0.1578\n",
      "388/463, train_loss: 0.2078\n",
      "389/463, train_loss: 0.2356\n",
      "390/463, train_loss: 0.1582\n",
      "391/463, train_loss: 0.1172\n",
      "392/463, train_loss: 0.0782\n",
      "393/463, train_loss: 0.7510\n",
      "394/463, train_loss: 0.1646\n",
      "395/463, train_loss: 0.1621\n",
      "396/463, train_loss: 0.1016\n",
      "397/463, train_loss: 0.3223\n",
      "398/463, train_loss: 0.1429\n",
      "399/463, train_loss: 0.1113\n",
      "400/463, train_loss: 0.5786\n",
      "401/463, train_loss: 0.1510\n",
      "402/463, train_loss: 0.1882\n",
      "403/463, train_loss: 0.5400\n",
      "404/463, train_loss: 0.0966\n",
      "405/463, train_loss: 0.0790\n",
      "406/463, train_loss: 0.1562\n",
      "407/463, train_loss: 0.3337\n",
      "408/463, train_loss: 0.1587\n",
      "409/463, train_loss: 0.1245\n",
      "410/463, train_loss: 0.4016\n",
      "411/463, train_loss: 0.2700\n",
      "412/463, train_loss: 0.5317\n",
      "413/463, train_loss: 0.0861\n",
      "414/463, train_loss: 0.1365\n",
      "415/463, train_loss: 0.2461\n",
      "416/463, train_loss: 0.1049\n",
      "417/463, train_loss: 0.3765\n",
      "418/463, train_loss: 0.6973\n",
      "419/463, train_loss: 0.7280\n",
      "420/463, train_loss: 0.1274\n",
      "421/463, train_loss: 0.1650\n",
      "422/463, train_loss: 0.2026\n",
      "423/463, train_loss: 0.2471\n",
      "424/463, train_loss: 0.1119\n",
      "425/463, train_loss: 0.0957\n",
      "426/463, train_loss: 0.1295\n",
      "427/463, train_loss: 0.1254\n",
      "428/463, train_loss: 0.1967\n",
      "429/463, train_loss: 0.0998\n",
      "430/463, train_loss: 0.0791\n",
      "431/463, train_loss: 0.4534\n",
      "432/463, train_loss: 0.3738\n",
      "433/463, train_loss: 0.2379\n",
      "434/463, train_loss: 0.2098\n",
      "435/463, train_loss: 0.1111\n",
      "436/463, train_loss: 0.3320\n",
      "437/463, train_loss: 0.0764\n",
      "438/463, train_loss: 0.2659\n",
      "439/463, train_loss: 0.2825\n",
      "440/463, train_loss: 0.1624\n",
      "441/463, train_loss: 0.2008\n",
      "442/463, train_loss: 0.1357\n",
      "443/463, train_loss: 0.1274\n",
      "444/463, train_loss: 0.2094\n",
      "445/463, train_loss: 0.1398\n",
      "446/463, train_loss: 0.1309\n",
      "447/463, train_loss: 0.1398\n",
      "448/463, train_loss: 0.1580\n",
      "449/463, train_loss: 0.2634\n",
      "450/463, train_loss: 0.3284\n",
      "451/463, train_loss: 0.0969\n",
      "452/463, train_loss: 0.1123\n",
      "453/463, train_loss: 0.6675\n",
      "454/463, train_loss: 0.1387\n",
      "455/463, train_loss: 0.1011\n",
      "456/463, train_loss: 0.2854\n",
      "457/463, train_loss: 0.1646\n",
      "458/463, train_loss: 0.1790\n",
      "459/463, train_loss: 0.1548\n",
      "460/463, train_loss: 0.1410\n",
      "461/463, train_loss: 0.2150\n",
      "462/463, train_loss: 0.1721\n",
      "463/463, train_loss: 0.3264\n",
      "epoch 20 average loss: 0.2060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 01:55:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 01:55:49 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 01:55:52 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mixed/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630.nii.gz\n",
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.5343262259371997, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.5343262259371997, 'AP_IoU_0.10_MaxDet_100': 0.566628409568036, 'nodule_AP_IoU_0.10_MaxDet_100': 0.566628409568036, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8917378981908163, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.8917378981908163, 'AR_IoU_0.10_MaxDet_100': 0.9487179517745972, 'nodule_AR_IoU_0.10_MaxDet_100': 0.9487179517745972}\n",
      "current epoch: 20 current metric: 0.7354 best metric: 0.7468 at epoch 15\n",
      "----------\n",
      "epoch 21/100\n",
      "1/463, train_loss: 0.0673\n",
      "2/463, train_loss: 0.0624\n",
      "3/463, train_loss: 0.2004\n",
      "4/463, train_loss: 0.1560\n",
      "5/463, train_loss: 0.3789\n",
      "6/463, train_loss: 0.0787\n",
      "7/463, train_loss: 0.3882\n",
      "8/463, train_loss: 0.2129\n",
      "9/463, train_loss: 0.4070\n",
      "10/463, train_loss: 0.3198\n",
      "11/463, train_loss: 0.1770\n",
      "12/463, train_loss: 0.1282\n",
      "13/463, train_loss: 0.1217\n",
      "14/463, train_loss: 0.5996\n",
      "15/463, train_loss: 0.1595\n",
      "16/463, train_loss: 0.2153\n",
      "17/463, train_loss: 0.2576\n",
      "18/463, train_loss: 0.3894\n",
      "19/463, train_loss: 0.2311\n",
      "20/463, train_loss: 0.1843\n",
      "21/463, train_loss: 0.1050\n",
      "22/463, train_loss: 0.2607\n",
      "23/463, train_loss: 0.1509\n",
      "24/463, train_loss: 0.0880\n",
      "25/463, train_loss: 0.1282\n",
      "26/463, train_loss: 0.2815\n",
      "27/463, train_loss: 0.0553\n",
      "28/463, train_loss: 0.3628\n",
      "29/463, train_loss: 0.0663\n",
      "30/463, train_loss: 0.1953\n",
      "31/463, train_loss: 0.1141\n",
      "32/463, train_loss: 0.3799\n",
      "33/463, train_loss: 0.0731\n",
      "34/463, train_loss: 0.1561\n",
      "35/463, train_loss: 0.0609\n",
      "36/463, train_loss: 0.0828\n",
      "37/463, train_loss: 0.2023\n",
      "38/463, train_loss: 0.1611\n",
      "39/463, train_loss: 0.0704\n",
      "40/463, train_loss: 0.0631\n",
      "41/463, train_loss: 0.3223\n",
      "42/463, train_loss: 0.1033\n",
      "43/463, train_loss: 0.0858\n",
      "44/463, train_loss: 0.2097\n",
      "45/463, train_loss: 0.2012\n",
      "46/463, train_loss: 0.2450\n",
      "47/463, train_loss: 0.5811\n",
      "48/463, train_loss: 0.2568\n",
      "49/463, train_loss: 0.1121\n",
      "50/463, train_loss: 0.1462\n",
      "51/463, train_loss: 0.1914\n",
      "52/463, train_loss: 0.1431\n",
      "53/463, train_loss: 0.1042\n",
      "54/463, train_loss: 0.1947\n",
      "55/463, train_loss: 0.1283\n",
      "56/463, train_loss: 0.0190\n",
      "57/463, train_loss: 0.1833\n",
      "58/463, train_loss: 0.1066\n",
      "59/463, train_loss: 0.1047\n",
      "60/463, train_loss: 0.1941\n",
      "61/463, train_loss: 0.1497\n",
      "62/463, train_loss: 0.0581\n",
      "63/463, train_loss: 0.1597\n",
      "64/463, train_loss: 0.2183\n",
      "65/463, train_loss: 0.5674\n",
      "66/463, train_loss: 0.2671\n",
      "67/463, train_loss: 0.1516\n",
      "68/463, train_loss: 0.1184\n",
      "69/463, train_loss: 0.4280\n",
      "70/463, train_loss: 0.1110\n",
      "71/463, train_loss: 0.1743\n",
      "72/463, train_loss: 0.1309\n",
      "73/463, train_loss: 0.1794\n",
      "74/463, train_loss: 0.0958\n",
      "75/463, train_loss: 0.0988\n",
      "76/463, train_loss: 0.0738\n",
      "77/463, train_loss: 0.1298\n",
      "78/463, train_loss: 0.0817\n",
      "79/463, train_loss: 0.0881\n",
      "80/463, train_loss: 0.1550\n",
      "81/463, train_loss: 0.1619\n",
      "82/463, train_loss: 0.2339\n",
      "83/463, train_loss: 0.1180\n",
      "84/463, train_loss: 0.1506\n",
      "85/463, train_loss: 0.1630\n",
      "86/463, train_loss: 0.0862\n",
      "87/463, train_loss: 0.4214\n",
      "88/463, train_loss: 0.3584\n",
      "89/463, train_loss: 0.0859\n",
      "90/463, train_loss: 0.2240\n",
      "91/463, train_loss: 0.1626\n",
      "92/463, train_loss: 0.1748\n",
      "93/463, train_loss: 0.5244\n",
      "94/463, train_loss: 0.1338\n",
      "95/463, train_loss: 0.1294\n",
      "96/463, train_loss: 0.1160\n",
      "97/463, train_loss: 0.2147\n",
      "98/463, train_loss: 0.1995\n",
      "99/463, train_loss: 0.5210\n",
      "100/463, train_loss: 0.4131\n",
      "101/463, train_loss: 0.1553\n",
      "102/463, train_loss: 0.1306\n",
      "103/463, train_loss: 0.1868\n",
      "104/463, train_loss: 0.1965\n",
      "105/463, train_loss: 0.0981\n",
      "106/463, train_loss: 0.1406\n",
      "107/463, train_loss: 0.3184\n",
      "108/463, train_loss: 0.2268\n",
      "109/463, train_loss: 0.2932\n",
      "110/463, train_loss: 0.0806\n",
      "111/463, train_loss: 0.2175\n",
      "112/463, train_loss: 0.2842\n",
      "113/463, train_loss: 1.3174\n",
      "114/463, train_loss: 0.1211\n",
      "115/463, train_loss: 0.1539\n",
      "116/463, train_loss: 0.1396\n",
      "117/463, train_loss: 0.0837\n",
      "118/463, train_loss: 0.1187\n",
      "119/463, train_loss: 0.1030\n",
      "120/463, train_loss: 0.3877\n",
      "121/463, train_loss: 0.1103\n",
      "122/463, train_loss: 0.1792\n",
      "123/463, train_loss: 0.0728\n",
      "124/463, train_loss: 0.4478\n",
      "125/463, train_loss: 0.1819\n",
      "126/463, train_loss: 0.0964\n",
      "127/463, train_loss: 0.0567\n",
      "128/463, train_loss: 0.2390\n",
      "129/463, train_loss: 0.1046\n",
      "130/463, train_loss: 0.0761\n",
      "131/463, train_loss: 0.5566\n",
      "132/463, train_loss: 0.0879\n",
      "133/463, train_loss: 0.0740\n",
      "134/463, train_loss: 0.0580\n",
      "135/463, train_loss: 0.1508\n",
      "136/463, train_loss: 0.2078\n",
      "137/463, train_loss: 0.0739\n",
      "138/463, train_loss: 0.1225\n",
      "139/463, train_loss: 0.1098\n",
      "140/463, train_loss: 0.1046\n",
      "141/463, train_loss: 0.4316\n",
      "142/463, train_loss: 0.4817\n",
      "143/463, train_loss: 0.1044\n",
      "144/463, train_loss: 0.1100\n",
      "145/463, train_loss: 0.1002\n",
      "146/463, train_loss: 0.5835\n",
      "147/463, train_loss: 0.1826\n",
      "148/463, train_loss: 0.4700\n",
      "149/463, train_loss: 0.2152\n",
      "150/463, train_loss: 0.1611\n",
      "151/463, train_loss: 0.3425\n",
      "152/463, train_loss: 0.4810\n",
      "153/463, train_loss: 0.0574\n",
      "154/463, train_loss: 0.1222\n",
      "155/463, train_loss: 0.1471\n",
      "156/463, train_loss: 0.1141\n",
      "157/463, train_loss: 0.6538\n",
      "158/463, train_loss: 0.1382\n",
      "159/463, train_loss: 0.3066\n",
      "160/463, train_loss: 0.0889\n",
      "161/463, train_loss: 0.1376\n",
      "162/463, train_loss: 0.1846\n",
      "163/463, train_loss: 0.1655\n",
      "164/463, train_loss: 0.4143\n",
      "165/463, train_loss: 0.2292\n",
      "166/463, train_loss: 0.1796\n",
      "167/463, train_loss: 0.2097\n",
      "168/463, train_loss: 0.3721\n",
      "169/463, train_loss: 0.2360\n",
      "170/463, train_loss: 0.1146\n",
      "171/463, train_loss: 0.3696\n",
      "172/463, train_loss: 0.3474\n",
      "173/463, train_loss: 0.0753\n",
      "174/463, train_loss: 0.2153\n",
      "175/463, train_loss: 0.3281\n",
      "176/463, train_loss: 0.0808\n",
      "177/463, train_loss: 0.0717\n",
      "178/463, train_loss: 0.1449\n",
      "179/463, train_loss: 0.1087\n",
      "180/463, train_loss: 0.2161\n",
      "181/463, train_loss: 0.1680\n",
      "182/463, train_loss: 0.1440\n",
      "183/463, train_loss: 0.2993\n",
      "184/463, train_loss: 0.2202\n",
      "185/463, train_loss: 0.0667\n",
      "186/463, train_loss: 0.1493\n",
      "187/463, train_loss: 0.1576\n",
      "188/463, train_loss: 0.3325\n",
      "189/463, train_loss: 0.1737\n",
      "190/463, train_loss: 0.0634\n",
      "191/463, train_loss: 0.1451\n",
      "192/463, train_loss: 0.9575\n",
      "193/463, train_loss: 0.2346\n",
      "194/463, train_loss: 0.4429\n",
      "195/463, train_loss: 0.1514\n",
      "196/463, train_loss: 0.1049\n",
      "197/463, train_loss: 0.1160\n",
      "198/463, train_loss: 0.3545\n",
      "199/463, train_loss: 0.1902\n",
      "200/463, train_loss: 0.1335\n",
      "201/463, train_loss: 0.1893\n",
      "202/463, train_loss: 0.2220\n",
      "203/463, train_loss: 0.1711\n",
      "204/463, train_loss: 0.0867\n",
      "205/463, train_loss: 0.1327\n",
      "206/463, train_loss: 0.1196\n",
      "207/463, train_loss: 0.1519\n",
      "208/463, train_loss: 0.1958\n",
      "209/463, train_loss: 0.3154\n",
      "210/463, train_loss: 0.2092\n",
      "211/463, train_loss: 0.0963\n",
      "212/463, train_loss: 0.1827\n",
      "213/463, train_loss: 0.6128\n",
      "214/463, train_loss: 0.0862\n",
      "215/463, train_loss: 0.0600\n",
      "216/463, train_loss: 0.1461\n",
      "217/463, train_loss: 0.1029\n",
      "218/463, train_loss: 0.1934\n",
      "219/463, train_loss: 0.1964\n",
      "220/463, train_loss: 0.1023\n",
      "221/463, train_loss: 0.1058\n",
      "222/463, train_loss: 0.0728\n",
      "223/463, train_loss: 0.0543\n",
      "224/463, train_loss: 0.1885\n",
      "225/463, train_loss: 0.0527\n",
      "226/463, train_loss: 0.2268\n",
      "227/463, train_loss: 0.1578\n",
      "228/463, train_loss: 0.0622\n",
      "229/463, train_loss: 0.2329\n",
      "230/463, train_loss: 0.1726\n",
      "231/463, train_loss: 0.1107\n",
      "232/463, train_loss: 0.0793\n",
      "233/463, train_loss: 0.1826\n",
      "234/463, train_loss: 0.0844\n",
      "235/463, train_loss: 0.0483\n",
      "236/463, train_loss: 0.1185\n",
      "237/463, train_loss: 0.3557\n",
      "238/463, train_loss: 0.3262\n",
      "239/463, train_loss: 0.2969\n",
      "240/463, train_loss: 0.1733\n",
      "241/463, train_loss: 0.3127\n",
      "242/463, train_loss: 0.1370\n",
      "243/463, train_loss: 0.3730\n",
      "244/463, train_loss: 0.1597\n",
      "245/463, train_loss: 0.1117\n",
      "246/463, train_loss: 0.2463\n",
      "247/463, train_loss: 0.5195\n",
      "248/463, train_loss: 0.2390\n",
      "249/463, train_loss: 0.1614\n",
      "250/463, train_loss: 0.0823\n",
      "251/463, train_loss: 0.0610\n",
      "252/463, train_loss: 0.1510\n",
      "253/463, train_loss: 0.0964\n",
      "254/463, train_loss: 0.3608\n",
      "255/463, train_loss: 0.1598\n",
      "256/463, train_loss: 0.1200\n",
      "257/463, train_loss: 0.1137\n",
      "258/463, train_loss: 0.1166\n",
      "259/463, train_loss: 0.2203\n",
      "260/463, train_loss: 0.0683\n",
      "261/463, train_loss: 0.0751\n",
      "262/463, train_loss: 0.2058\n",
      "263/463, train_loss: 0.0842\n",
      "264/463, train_loss: 0.0885\n",
      "265/463, train_loss: 0.1216\n",
      "266/463, train_loss: 0.1392\n",
      "267/463, train_loss: 0.1038\n",
      "268/463, train_loss: 0.2571\n",
      "269/463, train_loss: 0.4727\n",
      "270/463, train_loss: 0.1714\n",
      "271/463, train_loss: 0.0913\n",
      "272/463, train_loss: 0.1287\n",
      "273/463, train_loss: 0.7046\n",
      "274/463, train_loss: 0.1682\n",
      "275/463, train_loss: 0.1172\n",
      "276/463, train_loss: 0.0944\n",
      "277/463, train_loss: 0.0756\n",
      "278/463, train_loss: 0.2408\n",
      "279/463, train_loss: 0.2249\n",
      "280/463, train_loss: 0.5391\n",
      "281/463, train_loss: 0.1285\n",
      "282/463, train_loss: 0.0997\n",
      "283/463, train_loss: 0.2510\n",
      "284/463, train_loss: 0.2505\n",
      "285/463, train_loss: 0.1451\n",
      "286/463, train_loss: 0.0990\n",
      "287/463, train_loss: 0.1036\n",
      "288/463, train_loss: 0.1816\n",
      "289/463, train_loss: 0.0980\n",
      "290/463, train_loss: 0.7651\n",
      "291/463, train_loss: 0.2175\n",
      "292/463, train_loss: 0.0733\n",
      "293/463, train_loss: 0.0981\n",
      "294/463, train_loss: 0.1268\n",
      "295/463, train_loss: 0.1633\n",
      "296/463, train_loss: 0.1057\n",
      "297/463, train_loss: 0.0637\n",
      "298/463, train_loss: 0.1128\n",
      "299/463, train_loss: 0.2773\n",
      "300/463, train_loss: 0.2322\n",
      "301/463, train_loss: 0.2954\n",
      "302/463, train_loss: 0.3240\n",
      "303/463, train_loss: 0.2183\n",
      "304/463, train_loss: 0.0994\n",
      "305/463, train_loss: 0.0885\n",
      "306/463, train_loss: 0.3120\n",
      "307/463, train_loss: 0.0543\n",
      "308/463, train_loss: 0.1510\n",
      "309/463, train_loss: 0.1626\n",
      "310/463, train_loss: 0.3655\n",
      "311/463, train_loss: 0.3098\n",
      "312/463, train_loss: 0.1332\n",
      "313/463, train_loss: 0.0773\n",
      "314/463, train_loss: 0.0952\n",
      "315/463, train_loss: 0.1205\n",
      "316/463, train_loss: 0.2632\n",
      "317/463, train_loss: 0.1902\n",
      "318/463, train_loss: 0.2385\n",
      "319/463, train_loss: 0.1680\n",
      "320/463, train_loss: 0.3494\n",
      "321/463, train_loss: 0.1265\n",
      "322/463, train_loss: 0.1405\n",
      "323/463, train_loss: 0.0988\n",
      "324/463, train_loss: 0.1223\n",
      "325/463, train_loss: 0.1037\n",
      "326/463, train_loss: 0.0870\n",
      "327/463, train_loss: 0.0815\n",
      "328/463, train_loss: 0.3335\n",
      "329/463, train_loss: 0.2642\n",
      "330/463, train_loss: 0.1169\n",
      "331/463, train_loss: 0.2295\n",
      "332/463, train_loss: 0.4565\n",
      "333/463, train_loss: 0.0825\n",
      "334/463, train_loss: 0.5493\n",
      "335/463, train_loss: 0.4277\n",
      "336/463, train_loss: 0.0858\n",
      "337/463, train_loss: 0.2039\n",
      "338/463, train_loss: 0.0953\n",
      "339/463, train_loss: 0.3230\n",
      "340/463, train_loss: 0.1218\n",
      "341/463, train_loss: 0.0840\n",
      "342/463, train_loss: 0.6943\n",
      "343/463, train_loss: 0.1533\n",
      "344/463, train_loss: 0.1024\n",
      "345/463, train_loss: 0.0721\n",
      "346/463, train_loss: 0.3560\n",
      "347/463, train_loss: 0.2188\n",
      "348/463, train_loss: 0.2377\n",
      "349/463, train_loss: 0.1207\n",
      "350/463, train_loss: 0.1619\n",
      "351/463, train_loss: 0.4297\n",
      "352/463, train_loss: 0.1287\n",
      "353/463, train_loss: 0.3125\n",
      "354/463, train_loss: 0.1128\n",
      "355/463, train_loss: 0.3684\n",
      "356/463, train_loss: 0.1027\n",
      "357/463, train_loss: 0.0872\n",
      "358/463, train_loss: 0.2119\n",
      "359/463, train_loss: 0.0688\n",
      "360/463, train_loss: 0.2327\n",
      "361/463, train_loss: 0.3257\n",
      "362/463, train_loss: 0.3616\n",
      "363/463, train_loss: 0.1818\n",
      "364/463, train_loss: 0.1465\n",
      "365/463, train_loss: 0.1300\n",
      "366/463, train_loss: 0.2426\n",
      "367/463, train_loss: 0.0674\n",
      "368/463, train_loss: 0.5850\n",
      "369/463, train_loss: 0.1259\n",
      "370/463, train_loss: 0.1251\n",
      "371/463, train_loss: 0.1376\n",
      "372/463, train_loss: 0.1787\n",
      "373/463, train_loss: 0.3718\n",
      "374/463, train_loss: 0.1525\n",
      "375/463, train_loss: 0.1595\n",
      "376/463, train_loss: 0.1779\n",
      "377/463, train_loss: 0.5913\n",
      "378/463, train_loss: 0.1403\n",
      "379/463, train_loss: 0.0784\n",
      "380/463, train_loss: 0.1146\n",
      "381/463, train_loss: 0.1456\n",
      "382/463, train_loss: 0.0745\n",
      "383/463, train_loss: 0.0984\n",
      "384/463, train_loss: 0.5068\n",
      "385/463, train_loss: 0.0765\n",
      "386/463, train_loss: 0.1741\n",
      "387/463, train_loss: 0.3450\n",
      "388/463, train_loss: 0.5732\n",
      "389/463, train_loss: 0.2849\n",
      "390/463, train_loss: 0.1351\n",
      "391/463, train_loss: 0.1177\n",
      "392/463, train_loss: 0.1986\n",
      "393/463, train_loss: 0.0959\n",
      "394/463, train_loss: 0.1229\n",
      "395/463, train_loss: 0.2174\n",
      "396/463, train_loss: 0.1587\n",
      "397/463, train_loss: 0.0814\n",
      "398/463, train_loss: 0.1232\n",
      "399/463, train_loss: 0.1815\n",
      "400/463, train_loss: 0.4592\n",
      "401/463, train_loss: 0.1750\n",
      "402/463, train_loss: 0.0734\n",
      "403/463, train_loss: 0.0699\n",
      "404/463, train_loss: 0.1008\n",
      "405/463, train_loss: 0.0557\n",
      "406/463, train_loss: 0.4453\n",
      "407/463, train_loss: 0.3672\n",
      "408/463, train_loss: 0.1074\n",
      "409/463, train_loss: 0.0667\n",
      "410/463, train_loss: 0.0734\n",
      "411/463, train_loss: 0.0945\n",
      "412/463, train_loss: 0.0880\n",
      "413/463, train_loss: 0.1333\n",
      "414/463, train_loss: 0.2069\n",
      "415/463, train_loss: 0.1299\n",
      "416/463, train_loss: 0.2280\n",
      "417/463, train_loss: 0.2430\n",
      "418/463, train_loss: 0.1017\n",
      "419/463, train_loss: 0.1122\n",
      "420/463, train_loss: 0.1952\n",
      "421/463, train_loss: 0.0588\n",
      "422/463, train_loss: 0.5986\n",
      "423/463, train_loss: 0.1184\n",
      "424/463, train_loss: 0.0386\n",
      "425/463, train_loss: 0.1680\n",
      "426/463, train_loss: 0.0885\n",
      "427/463, train_loss: 0.2473\n",
      "428/463, train_loss: 0.2036\n",
      "429/463, train_loss: 0.2346\n",
      "430/463, train_loss: 0.1472\n",
      "431/463, train_loss: 0.1899\n",
      "432/463, train_loss: 0.0659\n",
      "433/463, train_loss: 0.1048\n",
      "434/463, train_loss: 0.3298\n",
      "435/463, train_loss: 0.1361\n",
      "436/463, train_loss: 0.2100\n",
      "437/463, train_loss: 0.8726\n",
      "438/463, train_loss: 0.1675\n",
      "439/463, train_loss: 0.1010\n",
      "440/463, train_loss: 0.1059\n",
      "441/463, train_loss: 0.4636\n",
      "442/463, train_loss: 0.0619\n",
      "443/463, train_loss: 0.5767\n",
      "444/463, train_loss: 0.1567\n",
      "445/463, train_loss: 0.0776\n",
      "446/463, train_loss: 0.1423\n",
      "447/463, train_loss: 0.1484\n",
      "448/463, train_loss: 0.0778\n",
      "449/463, train_loss: 0.3438\n",
      "450/463, train_loss: 0.0632\n",
      "451/463, train_loss: 0.3506\n",
      "452/463, train_loss: 0.1567\n",
      "453/463, train_loss: 0.4797\n",
      "454/463, train_loss: 0.1193\n",
      "455/463, train_loss: 0.1614\n",
      "456/463, train_loss: 0.1791\n",
      "457/463, train_loss: 0.3809\n",
      "458/463, train_loss: 0.2747\n",
      "459/463, train_loss: 0.1776\n",
      "460/463, train_loss: 0.2327\n",
      "461/463, train_loss: 0.1372\n",
      "462/463, train_loss: 0.0995\n",
      "463/463, train_loss: 0.0747\n",
      "epoch 21 average loss: 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 04:27:41 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 04:27:45 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 04:27:48 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 22/100\n",
      "1/463, train_loss: 0.1604\n",
      "2/463, train_loss: 0.0984\n",
      "3/463, train_loss: 0.1139\n",
      "4/463, train_loss: 0.1951\n",
      "5/463, train_loss: 0.1144\n",
      "6/463, train_loss: 0.1263\n",
      "7/463, train_loss: 0.2207\n",
      "8/463, train_loss: 0.1726\n",
      "9/463, train_loss: 0.3870\n",
      "10/463, train_loss: 0.2883\n",
      "11/463, train_loss: 0.1833\n",
      "12/463, train_loss: 0.1327\n",
      "13/463, train_loss: 0.1411\n",
      "14/463, train_loss: 0.2588\n",
      "15/463, train_loss: 0.0758\n",
      "16/463, train_loss: 0.1781\n",
      "17/463, train_loss: 0.1199\n",
      "18/463, train_loss: 0.1103\n",
      "19/463, train_loss: 0.2729\n",
      "20/463, train_loss: 0.1133\n",
      "21/463, train_loss: 0.1362\n",
      "22/463, train_loss: 0.1948\n",
      "23/463, train_loss: 0.3328\n",
      "24/463, train_loss: 0.1497\n",
      "25/463, train_loss: 0.2213\n",
      "26/463, train_loss: 0.1171\n",
      "27/463, train_loss: 0.0763\n",
      "28/463, train_loss: 0.1510\n",
      "29/463, train_loss: 0.1576\n",
      "30/463, train_loss: 0.3330\n",
      "31/463, train_loss: 0.1301\n",
      "32/463, train_loss: 0.2212\n",
      "33/463, train_loss: 0.4126\n",
      "34/463, train_loss: 0.1119\n",
      "35/463, train_loss: 0.1050\n",
      "36/463, train_loss: 0.3955\n",
      "37/463, train_loss: 0.2893\n",
      "38/463, train_loss: 0.1653\n",
      "39/463, train_loss: 0.0994\n",
      "40/463, train_loss: 0.1151\n",
      "41/463, train_loss: 0.1453\n",
      "42/463, train_loss: 0.0830\n",
      "43/463, train_loss: 0.1464\n",
      "44/463, train_loss: 0.1344\n",
      "45/463, train_loss: 0.1049\n",
      "46/463, train_loss: 0.1740\n",
      "47/463, train_loss: 0.0596\n",
      "48/463, train_loss: 0.0972\n",
      "49/463, train_loss: 0.0587\n",
      "50/463, train_loss: 0.0722\n",
      "51/463, train_loss: 0.1027\n",
      "52/463, train_loss: 0.1941\n",
      "53/463, train_loss: 0.1710\n",
      "54/463, train_loss: 0.2021\n",
      "55/463, train_loss: 0.0408\n",
      "56/463, train_loss: 0.1624\n",
      "57/463, train_loss: 0.0876\n",
      "58/463, train_loss: 0.4089\n",
      "59/463, train_loss: 0.0985\n",
      "60/463, train_loss: 0.0822\n",
      "61/463, train_loss: 0.2461\n",
      "62/463, train_loss: 0.8242\n",
      "63/463, train_loss: 0.9624\n",
      "64/463, train_loss: 0.5674\n",
      "65/463, train_loss: 0.1583\n",
      "66/463, train_loss: 0.1449\n",
      "67/463, train_loss: 0.1080\n",
      "68/463, train_loss: 0.1465\n",
      "69/463, train_loss: 0.2200\n",
      "70/463, train_loss: 0.1254\n",
      "71/463, train_loss: 0.1296\n",
      "72/463, train_loss: 0.3145\n",
      "73/463, train_loss: 0.1475\n",
      "74/463, train_loss: 0.1643\n",
      "75/463, train_loss: 0.0594\n",
      "76/463, train_loss: 0.1083\n",
      "77/463, train_loss: 0.0869\n",
      "78/463, train_loss: 0.1685\n",
      "79/463, train_loss: 0.3884\n",
      "80/463, train_loss: 0.0861\n",
      "81/463, train_loss: 0.1104\n",
      "82/463, train_loss: 0.2048\n",
      "83/463, train_loss: 0.2102\n",
      "84/463, train_loss: 0.1072\n",
      "85/463, train_loss: 0.3491\n",
      "86/463, train_loss: 0.4136\n",
      "87/463, train_loss: 0.6411\n",
      "88/463, train_loss: 0.0994\n",
      "89/463, train_loss: 0.1536\n",
      "90/463, train_loss: 0.0953\n",
      "91/463, train_loss: 0.1589\n",
      "92/463, train_loss: 0.3433\n",
      "93/463, train_loss: 0.1851\n",
      "94/463, train_loss: 0.1282\n",
      "95/463, train_loss: 0.1228\n",
      "96/463, train_loss: 0.1213\n",
      "97/463, train_loss: 0.0548\n",
      "98/463, train_loss: 0.1218\n",
      "99/463, train_loss: 0.0834\n",
      "100/463, train_loss: 0.5840\n",
      "101/463, train_loss: 0.2344\n",
      "102/463, train_loss: 0.1348\n",
      "103/463, train_loss: 0.1284\n",
      "104/463, train_loss: 0.4788\n",
      "105/463, train_loss: 0.1785\n",
      "106/463, train_loss: 0.0616\n",
      "107/463, train_loss: 0.1641\n",
      "108/463, train_loss: 0.1436\n",
      "109/463, train_loss: 0.2305\n",
      "110/463, train_loss: 0.1311\n",
      "111/463, train_loss: 0.0770\n",
      "112/463, train_loss: 0.3459\n",
      "113/463, train_loss: 0.4810\n",
      "114/463, train_loss: 0.0725\n",
      "115/463, train_loss: 0.2336\n",
      "116/463, train_loss: 0.1125\n",
      "117/463, train_loss: 0.1016\n",
      "118/463, train_loss: 0.4321\n",
      "119/463, train_loss: 0.0671\n",
      "120/463, train_loss: 0.2852\n",
      "121/463, train_loss: 0.2859\n",
      "122/463, train_loss: 0.1456\n",
      "123/463, train_loss: 0.1427\n",
      "124/463, train_loss: 0.1943\n",
      "125/463, train_loss: 0.1750\n",
      "126/463, train_loss: 0.2825\n",
      "127/463, train_loss: 0.0953\n",
      "128/463, train_loss: 0.1190\n",
      "129/463, train_loss: 0.4121\n",
      "130/463, train_loss: 0.5205\n",
      "131/463, train_loss: 0.1168\n",
      "132/463, train_loss: 0.2632\n",
      "133/463, train_loss: 0.1224\n",
      "134/463, train_loss: 0.0675\n",
      "135/463, train_loss: 0.0919\n",
      "136/463, train_loss: 0.1145\n",
      "137/463, train_loss: 0.1383\n",
      "138/463, train_loss: 0.0903\n",
      "139/463, train_loss: 0.2993\n",
      "140/463, train_loss: 0.2495\n",
      "141/463, train_loss: 0.2744\n",
      "142/463, train_loss: 0.2969\n",
      "143/463, train_loss: 0.1433\n",
      "144/463, train_loss: 0.7070\n",
      "145/463, train_loss: 0.1313\n",
      "146/463, train_loss: 0.2737\n",
      "147/463, train_loss: 0.4185\n",
      "148/463, train_loss: 0.0946\n",
      "149/463, train_loss: 0.0800\n",
      "150/463, train_loss: 0.0606\n",
      "151/463, train_loss: 0.1060\n",
      "152/463, train_loss: 0.0365\n",
      "153/463, train_loss: 0.1486\n",
      "154/463, train_loss: 0.1611\n",
      "155/463, train_loss: 0.1266\n",
      "156/463, train_loss: 0.0513\n",
      "157/463, train_loss: 0.3850\n",
      "158/463, train_loss: 0.3213\n",
      "159/463, train_loss: 0.2842\n",
      "160/463, train_loss: 0.1121\n",
      "161/463, train_loss: 0.1870\n",
      "162/463, train_loss: 0.1472\n",
      "163/463, train_loss: 0.0725\n",
      "164/463, train_loss: 0.1077\n",
      "165/463, train_loss: 0.1368\n",
      "166/463, train_loss: 0.0659\n",
      "167/463, train_loss: 0.3062\n",
      "168/463, train_loss: 0.3164\n",
      "169/463, train_loss: 0.1210\n",
      "170/463, train_loss: 0.2874\n",
      "171/463, train_loss: 0.1506\n",
      "172/463, train_loss: 0.4629\n",
      "173/463, train_loss: 0.2971\n",
      "174/463, train_loss: 0.1368\n",
      "175/463, train_loss: 0.0909\n",
      "176/463, train_loss: 0.1099\n",
      "177/463, train_loss: 0.1405\n",
      "178/463, train_loss: 0.1322\n",
      "179/463, train_loss: 0.3420\n",
      "180/463, train_loss: 0.0695\n",
      "181/463, train_loss: 0.1475\n",
      "182/463, train_loss: 0.1321\n",
      "183/463, train_loss: 0.4399\n",
      "184/463, train_loss: 0.1210\n",
      "185/463, train_loss: 0.1221\n",
      "186/463, train_loss: 0.1274\n",
      "187/463, train_loss: 0.1143\n",
      "188/463, train_loss: 0.4775\n",
      "189/463, train_loss: 0.0789\n",
      "190/463, train_loss: 0.4973\n",
      "191/463, train_loss: 0.2168\n",
      "192/463, train_loss: 0.1305\n",
      "193/463, train_loss: 0.1914\n",
      "194/463, train_loss: 0.1351\n",
      "195/463, train_loss: 0.1229\n",
      "196/463, train_loss: 0.1180\n",
      "197/463, train_loss: 0.1337\n",
      "198/463, train_loss: 0.1222\n",
      "199/463, train_loss: 0.2477\n",
      "200/463, train_loss: 0.0829\n",
      "201/463, train_loss: 0.0995\n",
      "202/463, train_loss: 0.0569\n",
      "203/463, train_loss: 0.0682\n",
      "204/463, train_loss: 0.0873\n",
      "205/463, train_loss: 0.0782\n",
      "206/463, train_loss: 0.1284\n",
      "207/463, train_loss: 0.0707\n",
      "208/463, train_loss: 0.2316\n",
      "209/463, train_loss: 0.1318\n",
      "210/463, train_loss: 0.1339\n",
      "211/463, train_loss: 0.2156\n",
      "212/463, train_loss: 0.0757\n",
      "213/463, train_loss: 0.4019\n",
      "214/463, train_loss: 0.1057\n",
      "215/463, train_loss: 0.0314\n",
      "216/463, train_loss: 0.0468\n",
      "217/463, train_loss: 0.4971\n",
      "218/463, train_loss: 0.1270\n",
      "219/463, train_loss: 0.2715\n",
      "220/463, train_loss: 0.1715\n",
      "221/463, train_loss: 0.1223\n",
      "222/463, train_loss: 0.0574\n",
      "223/463, train_loss: 0.0770\n",
      "224/463, train_loss: 0.1707\n",
      "225/463, train_loss: 0.0867\n",
      "226/463, train_loss: 0.1508\n",
      "227/463, train_loss: 0.1364\n",
      "228/463, train_loss: 0.1536\n",
      "229/463, train_loss: 0.0087\n",
      "230/463, train_loss: 0.0535\n",
      "231/463, train_loss: 0.0862\n",
      "232/463, train_loss: 0.1611\n",
      "233/463, train_loss: 0.2627\n",
      "234/463, train_loss: 0.1331\n",
      "235/463, train_loss: 0.0764\n",
      "236/463, train_loss: 0.3643\n",
      "237/463, train_loss: 0.4348\n",
      "238/463, train_loss: 0.0562\n",
      "239/463, train_loss: 0.0542\n",
      "240/463, train_loss: 0.2108\n",
      "241/463, train_loss: 1.0547\n",
      "242/463, train_loss: 0.1469\n",
      "243/463, train_loss: 0.6172\n",
      "244/463, train_loss: 0.5669\n",
      "245/463, train_loss: 0.2047\n",
      "246/463, train_loss: 0.2385\n",
      "247/463, train_loss: 0.3674\n",
      "248/463, train_loss: 0.2302\n",
      "249/463, train_loss: 0.6157\n",
      "250/463, train_loss: 0.1550\n",
      "251/463, train_loss: 0.4631\n",
      "252/463, train_loss: 0.1995\n",
      "253/463, train_loss: 0.1003\n",
      "254/463, train_loss: 0.1482\n",
      "255/463, train_loss: 0.1680\n",
      "256/463, train_loss: 0.2083\n",
      "257/463, train_loss: 0.4500\n",
      "258/463, train_loss: 0.1528\n",
      "259/463, train_loss: 0.0662\n",
      "260/463, train_loss: 0.3801\n",
      "261/463, train_loss: 0.1582\n",
      "262/463, train_loss: 0.1895\n",
      "263/463, train_loss: 0.2881\n",
      "264/463, train_loss: 0.2446\n",
      "265/463, train_loss: 0.1904\n",
      "266/463, train_loss: 0.1790\n",
      "267/463, train_loss: 0.2435\n",
      "268/463, train_loss: 0.1887\n",
      "269/463, train_loss: 0.0763\n",
      "270/463, train_loss: 0.1218\n",
      "271/463, train_loss: 0.1378\n",
      "272/463, train_loss: 0.1523\n",
      "273/463, train_loss: 0.1261\n",
      "274/463, train_loss: 0.4844\n",
      "275/463, train_loss: 0.1713\n",
      "276/463, train_loss: 0.1533\n",
      "277/463, train_loss: 0.0665\n",
      "278/463, train_loss: 0.1328\n",
      "279/463, train_loss: 0.2465\n",
      "280/463, train_loss: 0.2341\n",
      "281/463, train_loss: 0.1066\n",
      "282/463, train_loss: 0.1109\n",
      "283/463, train_loss: 0.0907\n",
      "284/463, train_loss: 0.0759\n",
      "285/463, train_loss: 0.0723\n",
      "286/463, train_loss: 0.1138\n",
      "287/463, train_loss: 0.2944\n",
      "288/463, train_loss: 0.2727\n",
      "289/463, train_loss: 0.0704\n",
      "290/463, train_loss: 0.2764\n",
      "291/463, train_loss: 0.6587\n",
      "292/463, train_loss: 0.1639\n",
      "293/463, train_loss: 0.0888\n",
      "294/463, train_loss: 0.3413\n",
      "295/463, train_loss: 0.1052\n",
      "296/463, train_loss: 0.3264\n",
      "297/463, train_loss: 0.0594\n",
      "298/463, train_loss: 0.1409\n",
      "299/463, train_loss: 0.1565\n",
      "300/463, train_loss: 0.5000\n",
      "301/463, train_loss: 0.2571\n",
      "302/463, train_loss: 0.1692\n",
      "303/463, train_loss: 0.1181\n",
      "304/463, train_loss: 0.2939\n",
      "305/463, train_loss: 0.9155\n",
      "306/463, train_loss: 0.4253\n",
      "307/463, train_loss: 0.1871\n",
      "308/463, train_loss: 0.0938\n",
      "309/463, train_loss: 0.0913\n",
      "310/463, train_loss: 0.0771\n",
      "311/463, train_loss: 0.1472\n",
      "312/463, train_loss: 0.1129\n",
      "313/463, train_loss: 0.1234\n",
      "314/463, train_loss: 0.1320\n",
      "315/463, train_loss: 0.2209\n",
      "316/463, train_loss: 0.0707\n",
      "317/463, train_loss: 0.3149\n",
      "318/463, train_loss: 0.4358\n",
      "319/463, train_loss: 0.0660\n",
      "320/463, train_loss: 0.4863\n",
      "321/463, train_loss: 0.1010\n",
      "322/463, train_loss: 0.2949\n",
      "323/463, train_loss: 0.5459\n",
      "324/463, train_loss: 0.2313\n",
      "325/463, train_loss: 0.3364\n",
      "326/463, train_loss: 0.1398\n",
      "327/463, train_loss: 0.1411\n",
      "328/463, train_loss: 0.1403\n",
      "329/463, train_loss: 0.3086\n",
      "330/463, train_loss: 0.1855\n",
      "331/463, train_loss: 0.1997\n",
      "332/463, train_loss: 0.1992\n",
      "333/463, train_loss: 0.1311\n",
      "334/463, train_loss: 0.1912\n",
      "335/463, train_loss: 0.1093\n",
      "336/463, train_loss: 0.1372\n",
      "337/463, train_loss: 0.1672\n",
      "338/463, train_loss: 0.3306\n",
      "339/463, train_loss: 0.1226\n",
      "340/463, train_loss: 0.3137\n",
      "341/463, train_loss: 0.1207\n",
      "342/463, train_loss: 0.1650\n",
      "343/463, train_loss: 0.4360\n",
      "344/463, train_loss: 0.2603\n",
      "345/463, train_loss: 0.0634\n",
      "346/463, train_loss: 0.0894\n",
      "347/463, train_loss: 0.0861\n",
      "348/463, train_loss: 0.3259\n",
      "349/463, train_loss: 0.1299\n",
      "350/463, train_loss: 0.1185\n",
      "351/463, train_loss: 0.1216\n",
      "352/463, train_loss: 0.1868\n",
      "353/463, train_loss: 0.4329\n",
      "354/463, train_loss: 0.5581\n",
      "355/463, train_loss: 0.1584\n",
      "356/463, train_loss: 0.4165\n",
      "357/463, train_loss: 0.1183\n",
      "358/463, train_loss: 0.1772\n",
      "359/463, train_loss: 0.1565\n",
      "360/463, train_loss: 0.1066\n",
      "361/463, train_loss: 0.2935\n",
      "362/463, train_loss: 0.1289\n",
      "363/463, train_loss: 0.1516\n",
      "364/463, train_loss: 0.4153\n",
      "365/463, train_loss: 0.1125\n",
      "366/463, train_loss: 0.1134\n",
      "367/463, train_loss: 0.4797\n",
      "368/463, train_loss: 0.4929\n",
      "369/463, train_loss: 0.2120\n",
      "370/463, train_loss: 0.1870\n",
      "371/463, train_loss: 0.0731\n",
      "372/463, train_loss: 0.1174\n",
      "373/463, train_loss: 0.1212\n",
      "374/463, train_loss: 0.2089\n",
      "375/463, train_loss: 0.0656\n",
      "376/463, train_loss: 0.0817\n",
      "377/463, train_loss: 0.5005\n",
      "378/463, train_loss: 0.1831\n",
      "379/463, train_loss: 0.2231\n",
      "380/463, train_loss: 0.1664\n",
      "381/463, train_loss: 0.1053\n",
      "382/463, train_loss: 0.1467\n",
      "383/463, train_loss: 0.3242\n",
      "384/463, train_loss: 0.0690\n",
      "385/463, train_loss: 0.0997\n",
      "386/463, train_loss: 0.4160\n",
      "387/463, train_loss: 0.2439\n",
      "388/463, train_loss: 0.3723\n",
      "389/463, train_loss: 0.1049\n",
      "390/463, train_loss: 0.1389\n",
      "391/463, train_loss: 0.1578\n",
      "392/463, train_loss: 0.1394\n",
      "393/463, train_loss: 0.0872\n",
      "394/463, train_loss: 0.1678\n",
      "395/463, train_loss: 0.2395\n",
      "396/463, train_loss: 0.0866\n",
      "397/463, train_loss: 0.1509\n",
      "398/463, train_loss: 0.1587\n",
      "399/463, train_loss: 0.1292\n",
      "400/463, train_loss: 0.3516\n",
      "401/463, train_loss: 0.0795\n",
      "402/463, train_loss: 0.0479\n",
      "403/463, train_loss: 0.2207\n",
      "404/463, train_loss: 0.4609\n",
      "405/463, train_loss: 0.2078\n",
      "406/463, train_loss: 0.1265\n",
      "407/463, train_loss: 0.1489\n",
      "408/463, train_loss: 0.1240\n",
      "409/463, train_loss: 0.4490\n",
      "410/463, train_loss: 0.6025\n",
      "411/463, train_loss: 0.3394\n",
      "412/463, train_loss: 0.1401\n",
      "413/463, train_loss: 0.1220\n",
      "414/463, train_loss: 0.1218\n",
      "415/463, train_loss: 0.0905\n",
      "416/463, train_loss: 0.0957\n",
      "417/463, train_loss: 0.4463\n",
      "418/463, train_loss: 0.1577\n",
      "419/463, train_loss: 0.1127\n",
      "420/463, train_loss: 0.5068\n",
      "421/463, train_loss: 0.0732\n",
      "422/463, train_loss: 0.1073\n",
      "423/463, train_loss: 0.0957\n",
      "424/463, train_loss: 0.1426\n",
      "425/463, train_loss: 0.1443\n",
      "426/463, train_loss: 0.1641\n",
      "427/463, train_loss: 0.0759\n",
      "428/463, train_loss: 0.2471\n",
      "429/463, train_loss: 0.3074\n",
      "430/463, train_loss: 0.3569\n",
      "431/463, train_loss: 0.0612\n",
      "432/463, train_loss: 0.0857\n",
      "433/463, train_loss: 0.0879\n",
      "434/463, train_loss: 0.1287\n",
      "435/463, train_loss: 0.4326\n",
      "436/463, train_loss: 0.4690\n",
      "437/463, train_loss: 0.7495\n",
      "438/463, train_loss: 0.4404\n",
      "439/463, train_loss: 0.1335\n",
      "440/463, train_loss: 0.1656\n",
      "441/463, train_loss: 0.0741\n",
      "442/463, train_loss: 0.0923\n",
      "443/463, train_loss: 0.6558\n",
      "444/463, train_loss: 0.2426\n",
      "445/463, train_loss: 0.1071\n",
      "446/463, train_loss: 0.1880\n",
      "447/463, train_loss: 0.1547\n",
      "448/463, train_loss: 0.1669\n",
      "449/463, train_loss: 0.1516\n",
      "450/463, train_loss: 0.1069\n",
      "451/463, train_loss: 0.3379\n",
      "452/463, train_loss: 0.3130\n",
      "453/463, train_loss: 0.1925\n",
      "454/463, train_loss: 0.2642\n",
      "455/463, train_loss: 0.4585\n",
      "456/463, train_loss: 0.2090\n",
      "457/463, train_loss: 0.0643\n",
      "458/463, train_loss: 0.3718\n",
      "459/463, train_loss: 0.2539\n",
      "460/463, train_loss: 0.1833\n",
      "461/463, train_loss: 0.0761\n",
      "462/463, train_loss: 0.3018\n",
      "463/463, train_loss: 0.3232\n",
      "epoch 22 average loss: 0.2052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 06:40:08 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 06:40:11 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 06:40:14 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 23/100\n",
      "1/463, train_loss: 0.0945\n",
      "2/463, train_loss: 0.1259\n",
      "3/463, train_loss: 0.1671\n",
      "4/463, train_loss: 0.1181\n",
      "5/463, train_loss: 0.1176\n",
      "6/463, train_loss: 0.1957\n",
      "7/463, train_loss: 0.3066\n",
      "8/463, train_loss: 0.1371\n",
      "9/463, train_loss: 0.0851\n",
      "10/463, train_loss: 0.1917\n",
      "11/463, train_loss: 0.3525\n",
      "12/463, train_loss: 0.3188\n",
      "13/463, train_loss: 0.0760\n",
      "14/463, train_loss: 0.0853\n",
      "15/463, train_loss: 0.2190\n",
      "16/463, train_loss: 0.0690\n",
      "17/463, train_loss: 0.1499\n",
      "18/463, train_loss: 0.1222\n",
      "19/463, train_loss: 0.1095\n",
      "20/463, train_loss: 0.1499\n",
      "21/463, train_loss: 0.6455\n",
      "22/463, train_loss: 0.4285\n",
      "23/463, train_loss: 0.0450\n",
      "24/463, train_loss: 0.0963\n",
      "25/463, train_loss: 0.1029\n",
      "26/463, train_loss: 0.0513\n",
      "27/463, train_loss: 0.0996\n",
      "28/463, train_loss: 0.3687\n",
      "29/463, train_loss: 0.2240\n",
      "30/463, train_loss: 0.1041\n",
      "31/463, train_loss: 0.1440\n",
      "32/463, train_loss: 0.1315\n",
      "33/463, train_loss: 0.1238\n",
      "34/463, train_loss: 0.1056\n",
      "35/463, train_loss: 0.1571\n",
      "36/463, train_loss: 0.1992\n",
      "37/463, train_loss: 0.1047\n",
      "38/463, train_loss: 0.0958\n",
      "39/463, train_loss: 0.1104\n",
      "40/463, train_loss: 0.2764\n",
      "41/463, train_loss: 0.1128\n",
      "42/463, train_loss: 0.1216\n",
      "43/463, train_loss: 0.1375\n",
      "44/463, train_loss: 0.1927\n",
      "45/463, train_loss: 0.1360\n",
      "46/463, train_loss: 0.0435\n",
      "47/463, train_loss: 0.1580\n",
      "48/463, train_loss: 0.1366\n",
      "49/463, train_loss: 0.0787\n",
      "50/463, train_loss: 0.3433\n",
      "51/463, train_loss: 0.3186\n",
      "52/463, train_loss: 0.0349\n",
      "53/463, train_loss: 0.0864\n",
      "54/463, train_loss: 0.0845\n",
      "55/463, train_loss: 0.1648\n",
      "56/463, train_loss: 0.0897\n",
      "57/463, train_loss: 0.1152\n",
      "58/463, train_loss: 0.4050\n",
      "59/463, train_loss: 0.2717\n",
      "60/463, train_loss: 0.2448\n",
      "61/463, train_loss: 0.1367\n",
      "62/463, train_loss: 0.2500\n",
      "63/463, train_loss: 0.0764\n",
      "64/463, train_loss: 0.1650\n",
      "65/463, train_loss: 0.4302\n",
      "66/463, train_loss: 0.1166\n",
      "67/463, train_loss: 0.1208\n",
      "68/463, train_loss: 0.1907\n",
      "69/463, train_loss: 0.1670\n",
      "70/463, train_loss: 0.1965\n",
      "71/463, train_loss: 0.1566\n",
      "72/463, train_loss: 0.3984\n",
      "73/463, train_loss: 0.1323\n",
      "74/463, train_loss: 0.1384\n",
      "75/463, train_loss: 0.0662\n",
      "76/463, train_loss: 0.0850\n",
      "77/463, train_loss: 0.3330\n",
      "78/463, train_loss: 0.1409\n",
      "79/463, train_loss: 0.1580\n",
      "80/463, train_loss: 0.3335\n",
      "81/463, train_loss: 0.0971\n",
      "82/463, train_loss: 0.3152\n",
      "83/463, train_loss: 0.1299\n",
      "84/463, train_loss: 0.1181\n",
      "85/463, train_loss: 0.1743\n",
      "86/463, train_loss: 0.1462\n",
      "87/463, train_loss: 0.0682\n",
      "88/463, train_loss: 0.3188\n",
      "89/463, train_loss: 0.0705\n",
      "90/463, train_loss: 0.0913\n",
      "91/463, train_loss: 0.0870\n",
      "92/463, train_loss: 0.1129\n",
      "93/463, train_loss: 0.0919\n",
      "94/463, train_loss: 0.3445\n",
      "95/463, train_loss: 0.2236\n",
      "96/463, train_loss: 0.6309\n",
      "97/463, train_loss: 0.6138\n",
      "98/463, train_loss: 0.1671\n",
      "99/463, train_loss: 0.1836\n",
      "100/463, train_loss: 0.2006\n",
      "101/463, train_loss: 0.1797\n",
      "102/463, train_loss: 0.0856\n",
      "103/463, train_loss: 0.2078\n",
      "104/463, train_loss: 0.0626\n",
      "105/463, train_loss: 0.4546\n",
      "106/463, train_loss: 0.2325\n",
      "107/463, train_loss: 0.4116\n",
      "108/463, train_loss: 0.2292\n",
      "109/463, train_loss: 0.1158\n",
      "110/463, train_loss: 0.4006\n",
      "111/463, train_loss: 0.3086\n",
      "112/463, train_loss: 0.2341\n",
      "113/463, train_loss: 0.2844\n",
      "114/463, train_loss: 0.1350\n",
      "115/463, train_loss: 0.1942\n",
      "116/463, train_loss: 0.1277\n",
      "117/463, train_loss: 0.1035\n",
      "118/463, train_loss: 0.2235\n",
      "119/463, train_loss: 0.1782\n",
      "120/463, train_loss: 0.1270\n",
      "121/463, train_loss: 0.1029\n",
      "122/463, train_loss: 0.2927\n",
      "123/463, train_loss: 0.1310\n",
      "124/463, train_loss: 0.6133\n",
      "125/463, train_loss: 0.1224\n",
      "126/463, train_loss: 0.3599\n",
      "127/463, train_loss: 0.0953\n",
      "128/463, train_loss: 0.3232\n",
      "129/463, train_loss: 0.1049\n",
      "130/463, train_loss: 0.0705\n",
      "131/463, train_loss: 0.0146\n",
      "132/463, train_loss: 0.1118\n",
      "133/463, train_loss: 0.1309\n",
      "134/463, train_loss: 0.1450\n",
      "135/463, train_loss: 0.2356\n",
      "136/463, train_loss: 0.1160\n",
      "137/463, train_loss: 0.1292\n",
      "138/463, train_loss: 0.2573\n",
      "139/463, train_loss: 0.0929\n",
      "140/463, train_loss: 0.1094\n",
      "141/463, train_loss: 0.0729\n",
      "142/463, train_loss: 0.1200\n",
      "143/463, train_loss: 0.5386\n",
      "144/463, train_loss: 0.1484\n",
      "145/463, train_loss: 0.0660\n",
      "146/463, train_loss: 0.0974\n",
      "147/463, train_loss: 0.1036\n",
      "148/463, train_loss: 0.0729\n",
      "149/463, train_loss: 0.1255\n",
      "150/463, train_loss: 0.1091\n",
      "151/463, train_loss: 0.3423\n",
      "152/463, train_loss: 0.4963\n",
      "153/463, train_loss: 0.0416\n",
      "154/463, train_loss: 0.1098\n",
      "155/463, train_loss: 0.1747\n",
      "156/463, train_loss: 0.1775\n",
      "157/463, train_loss: 0.1968\n",
      "158/463, train_loss: 0.1206\n",
      "159/463, train_loss: 0.2151\n",
      "160/463, train_loss: 0.2778\n",
      "161/463, train_loss: 0.1672\n",
      "162/463, train_loss: 0.0929\n",
      "163/463, train_loss: 0.1287\n",
      "164/463, train_loss: 0.0651\n",
      "165/463, train_loss: 0.0930\n",
      "166/463, train_loss: 0.2401\n",
      "167/463, train_loss: 0.1381\n",
      "168/463, train_loss: 0.4121\n",
      "169/463, train_loss: 0.5396\n",
      "170/463, train_loss: 0.2812\n",
      "171/463, train_loss: 0.1300\n",
      "172/463, train_loss: 0.0947\n",
      "173/463, train_loss: 0.2233\n",
      "174/463, train_loss: 0.1396\n",
      "175/463, train_loss: 0.1721\n",
      "176/463, train_loss: 0.2292\n",
      "177/463, train_loss: 0.2042\n",
      "178/463, train_loss: 0.0697\n",
      "179/463, train_loss: 0.5767\n",
      "180/463, train_loss: 0.2275\n",
      "181/463, train_loss: 0.0476\n",
      "182/463, train_loss: 0.1113\n",
      "183/463, train_loss: 0.2581\n",
      "184/463, train_loss: 0.1392\n",
      "185/463, train_loss: 0.1348\n",
      "186/463, train_loss: 0.2053\n",
      "187/463, train_loss: 0.0872\n",
      "188/463, train_loss: 0.1116\n",
      "189/463, train_loss: 0.1747\n",
      "190/463, train_loss: 0.2285\n",
      "191/463, train_loss: 0.1215\n",
      "192/463, train_loss: 0.0532\n",
      "193/463, train_loss: 0.2839\n",
      "194/463, train_loss: 0.1228\n",
      "195/463, train_loss: 0.0991\n",
      "196/463, train_loss: 0.0934\n",
      "197/463, train_loss: 0.1489\n",
      "198/463, train_loss: 0.4800\n",
      "199/463, train_loss: 0.1644\n",
      "200/463, train_loss: 0.0949\n",
      "201/463, train_loss: 0.1902\n",
      "202/463, train_loss: 0.3181\n",
      "203/463, train_loss: 0.2078\n",
      "204/463, train_loss: 0.0502\n",
      "205/463, train_loss: 0.3081\n",
      "206/463, train_loss: 0.0859\n",
      "207/463, train_loss: 0.0681\n",
      "208/463, train_loss: 0.1829\n",
      "209/463, train_loss: 0.1296\n",
      "210/463, train_loss: 0.0803\n",
      "211/463, train_loss: 0.0698\n",
      "212/463, train_loss: 0.0128\n",
      "213/463, train_loss: 0.1428\n",
      "214/463, train_loss: 0.0623\n",
      "215/463, train_loss: 0.0867\n",
      "216/463, train_loss: 0.5444\n",
      "217/463, train_loss: 0.2261\n",
      "218/463, train_loss: 0.1405\n",
      "219/463, train_loss: 0.1362\n",
      "220/463, train_loss: 0.2374\n",
      "221/463, train_loss: 0.1866\n",
      "222/463, train_loss: 0.1170\n",
      "223/463, train_loss: 0.0804\n",
      "224/463, train_loss: 0.1265\n",
      "225/463, train_loss: 0.0756\n",
      "226/463, train_loss: 0.1599\n",
      "227/463, train_loss: 0.3042\n",
      "228/463, train_loss: 0.1151\n",
      "229/463, train_loss: 0.1859\n",
      "230/463, train_loss: 0.0548\n",
      "231/463, train_loss: 0.3970\n",
      "232/463, train_loss: 0.5146\n",
      "233/463, train_loss: 0.1284\n",
      "234/463, train_loss: 0.1119\n",
      "235/463, train_loss: 0.0756\n",
      "236/463, train_loss: 0.0980\n",
      "237/463, train_loss: 0.0693\n",
      "238/463, train_loss: 0.1022\n",
      "239/463, train_loss: 0.0955\n",
      "240/463, train_loss: 0.1004\n",
      "241/463, train_loss: 0.0457\n",
      "242/463, train_loss: 0.3901\n",
      "243/463, train_loss: 0.0712\n",
      "244/463, train_loss: 0.1487\n",
      "245/463, train_loss: 0.2515\n",
      "246/463, train_loss: 0.1282\n",
      "247/463, train_loss: 0.1582\n",
      "248/463, train_loss: 0.6982\n",
      "249/463, train_loss: 0.0822\n",
      "250/463, train_loss: 0.1125\n",
      "251/463, train_loss: 0.1119\n",
      "252/463, train_loss: 0.1746\n",
      "253/463, train_loss: 0.1603\n",
      "254/463, train_loss: 0.1335\n",
      "255/463, train_loss: 0.4604\n",
      "256/463, train_loss: 0.1254\n",
      "257/463, train_loss: 0.1135\n",
      "258/463, train_loss: 0.2028\n",
      "259/463, train_loss: 0.9854\n",
      "260/463, train_loss: 0.0847\n",
      "261/463, train_loss: 0.0990\n",
      "262/463, train_loss: 0.1797\n",
      "263/463, train_loss: 0.1210\n",
      "264/463, train_loss: 0.0637\n",
      "265/463, train_loss: 0.5381\n",
      "266/463, train_loss: 0.1575\n",
      "267/463, train_loss: 0.3618\n",
      "268/463, train_loss: 0.2539\n",
      "269/463, train_loss: 0.1661\n",
      "270/463, train_loss: 0.1683\n",
      "271/463, train_loss: 0.1223\n",
      "272/463, train_loss: 0.1373\n",
      "273/463, train_loss: 0.5869\n",
      "274/463, train_loss: 0.3938\n",
      "275/463, train_loss: 0.0475\n",
      "276/463, train_loss: 0.3901\n",
      "277/463, train_loss: 0.1198\n",
      "278/463, train_loss: 0.1079\n",
      "279/463, train_loss: 0.0807\n",
      "280/463, train_loss: 0.2421\n",
      "281/463, train_loss: 0.0637\n",
      "282/463, train_loss: 0.1947\n",
      "283/463, train_loss: 0.2301\n",
      "284/463, train_loss: 0.1820\n",
      "285/463, train_loss: 0.0642\n",
      "286/463, train_loss: 0.2625\n",
      "287/463, train_loss: 0.1650\n",
      "288/463, train_loss: 0.0671\n",
      "289/463, train_loss: 0.1611\n",
      "290/463, train_loss: 0.1404\n",
      "291/463, train_loss: 0.1626\n",
      "292/463, train_loss: 0.3667\n",
      "293/463, train_loss: 0.2250\n",
      "294/463, train_loss: 0.1565\n",
      "295/463, train_loss: 0.2252\n",
      "296/463, train_loss: 0.1534\n",
      "297/463, train_loss: 0.1113\n",
      "298/463, train_loss: 0.0808\n",
      "299/463, train_loss: 0.0891\n",
      "300/463, train_loss: 0.4871\n",
      "301/463, train_loss: 0.8926\n",
      "302/463, train_loss: 0.1370\n",
      "303/463, train_loss: 0.0807\n",
      "304/463, train_loss: 0.2391\n",
      "305/463, train_loss: 0.1281\n",
      "306/463, train_loss: 0.1387\n",
      "307/463, train_loss: 0.2488\n",
      "308/463, train_loss: 0.2169\n",
      "309/463, train_loss: 0.1137\n",
      "310/463, train_loss: 0.1544\n",
      "311/463, train_loss: 0.1405\n",
      "312/463, train_loss: 0.1295\n",
      "313/463, train_loss: 0.1229\n",
      "314/463, train_loss: 0.2563\n",
      "315/463, train_loss: 0.1046\n",
      "316/463, train_loss: 0.1653\n",
      "317/463, train_loss: 0.1132\n",
      "318/463, train_loss: 0.1204\n",
      "319/463, train_loss: 0.0906\n",
      "320/463, train_loss: 0.5952\n",
      "321/463, train_loss: 0.6353\n",
      "322/463, train_loss: 0.2695\n",
      "323/463, train_loss: 0.1658\n",
      "324/463, train_loss: 0.2206\n",
      "325/463, train_loss: 0.1324\n",
      "326/463, train_loss: 1.1592\n",
      "327/463, train_loss: 0.2708\n",
      "328/463, train_loss: 0.1445\n",
      "329/463, train_loss: 0.1080\n",
      "330/463, train_loss: 0.1350\n",
      "331/463, train_loss: 0.5566\n",
      "332/463, train_loss: 0.2175\n",
      "333/463, train_loss: 0.2128\n",
      "334/463, train_loss: 0.1497\n",
      "335/463, train_loss: 0.1074\n",
      "336/463, train_loss: 0.1024\n",
      "337/463, train_loss: 0.2393\n",
      "338/463, train_loss: 0.0989\n",
      "339/463, train_loss: 0.4380\n",
      "340/463, train_loss: 0.1610\n",
      "341/463, train_loss: 0.1644\n",
      "342/463, train_loss: 0.0681\n",
      "343/463, train_loss: 0.1936\n",
      "344/463, train_loss: 0.1283\n",
      "345/463, train_loss: 0.0615\n",
      "346/463, train_loss: 0.4180\n",
      "347/463, train_loss: 0.0952\n",
      "348/463, train_loss: 0.0342\n",
      "349/463, train_loss: 0.0958\n",
      "350/463, train_loss: 0.1437\n",
      "351/463, train_loss: 0.2009\n",
      "352/463, train_loss: 0.3503\n",
      "353/463, train_loss: 0.1093\n",
      "354/463, train_loss: 0.1495\n",
      "355/463, train_loss: 0.1241\n",
      "356/463, train_loss: 0.0957\n",
      "357/463, train_loss: 0.1262\n",
      "358/463, train_loss: 0.2302\n",
      "359/463, train_loss: 0.8472\n",
      "360/463, train_loss: 0.3135\n",
      "361/463, train_loss: 0.2017\n",
      "362/463, train_loss: 0.0246\n",
      "363/463, train_loss: 0.3853\n",
      "364/463, train_loss: 0.1819\n",
      "365/463, train_loss: 0.0726\n",
      "366/463, train_loss: 0.1184\n",
      "367/463, train_loss: 0.1014\n",
      "368/463, train_loss: 0.1159\n",
      "369/463, train_loss: 0.2568\n",
      "370/463, train_loss: 0.2219\n",
      "371/463, train_loss: 0.2585\n",
      "372/463, train_loss: 0.3542\n",
      "373/463, train_loss: 0.1443\n",
      "374/463, train_loss: 0.5024\n",
      "375/463, train_loss: 0.1699\n",
      "376/463, train_loss: 0.2373\n",
      "377/463, train_loss: 0.1816\n",
      "378/463, train_loss: 0.1334\n",
      "379/463, train_loss: 0.1500\n",
      "380/463, train_loss: 0.1514\n",
      "381/463, train_loss: 0.0620\n",
      "382/463, train_loss: 0.1201\n",
      "383/463, train_loss: 0.0903\n",
      "384/463, train_loss: 0.1724\n",
      "385/463, train_loss: 0.0883\n",
      "386/463, train_loss: 0.3770\n",
      "387/463, train_loss: 0.5669\n",
      "388/463, train_loss: 0.1426\n",
      "389/463, train_loss: 0.1172\n",
      "390/463, train_loss: 0.0927\n",
      "391/463, train_loss: 0.0910\n",
      "392/463, train_loss: 0.2278\n",
      "393/463, train_loss: 0.1283\n",
      "394/463, train_loss: 0.7139\n",
      "395/463, train_loss: 0.4021\n",
      "396/463, train_loss: 0.0779\n",
      "397/463, train_loss: 0.1890\n",
      "398/463, train_loss: 0.1134\n",
      "399/463, train_loss: 0.1279\n",
      "400/463, train_loss: 0.1327\n",
      "401/463, train_loss: 0.2852\n",
      "402/463, train_loss: 0.1033\n",
      "403/463, train_loss: 0.6138\n",
      "404/463, train_loss: 0.2009\n",
      "405/463, train_loss: 0.0674\n",
      "406/463, train_loss: 0.1544\n",
      "407/463, train_loss: 0.1411\n",
      "408/463, train_loss: 0.4192\n",
      "409/463, train_loss: 0.1592\n",
      "410/463, train_loss: 0.1290\n",
      "411/463, train_loss: 0.1851\n",
      "412/463, train_loss: 0.1074\n",
      "413/463, train_loss: 0.3245\n",
      "414/463, train_loss: 0.1355\n",
      "415/463, train_loss: 0.1439\n",
      "416/463, train_loss: 0.0898\n",
      "417/463, train_loss: 0.2284\n",
      "418/463, train_loss: 0.1592\n",
      "419/463, train_loss: 0.2678\n",
      "420/463, train_loss: 0.0812\n",
      "421/463, train_loss: 0.1975\n",
      "422/463, train_loss: 0.3320\n",
      "423/463, train_loss: 0.1691\n",
      "424/463, train_loss: 0.1027\n",
      "425/463, train_loss: 0.3843\n",
      "426/463, train_loss: 0.1287\n",
      "427/463, train_loss: 0.1493\n",
      "428/463, train_loss: 0.0792\n",
      "429/463, train_loss: 0.1064\n",
      "430/463, train_loss: 0.0774\n",
      "431/463, train_loss: 0.5430\n",
      "432/463, train_loss: 0.3247\n",
      "433/463, train_loss: 0.0980\n",
      "434/463, train_loss: 0.1204\n",
      "435/463, train_loss: 0.1324\n",
      "436/463, train_loss: 0.0703\n",
      "437/463, train_loss: 0.1122\n",
      "438/463, train_loss: 0.0819\n",
      "439/463, train_loss: 0.1299\n",
      "440/463, train_loss: 0.6953\n",
      "441/463, train_loss: 0.3525\n",
      "442/463, train_loss: 0.0640\n",
      "443/463, train_loss: 0.2715\n",
      "444/463, train_loss: 0.0879\n",
      "445/463, train_loss: 0.0958\n",
      "446/463, train_loss: 0.0833\n",
      "447/463, train_loss: 0.1168\n",
      "448/463, train_loss: 0.1204\n",
      "449/463, train_loss: 0.1392\n",
      "450/463, train_loss: 0.1290\n",
      "451/463, train_loss: 0.0924\n",
      "452/463, train_loss: 0.1671\n",
      "453/463, train_loss: 0.1464\n",
      "454/463, train_loss: 0.2019\n",
      "455/463, train_loss: 0.0786\n",
      "456/463, train_loss: 0.0975\n",
      "457/463, train_loss: 0.1327\n",
      "458/463, train_loss: 0.1266\n",
      "459/463, train_loss: 0.0989\n",
      "460/463, train_loss: 0.1287\n",
      "461/463, train_loss: 0.1711\n",
      "462/463, train_loss: 0.0563\n",
      "463/463, train_loss: 0.1077\n",
      "epoch 23 average loss: 0.1914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 08:52:35 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 08:52:37 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 08:52:40 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 24/100\n",
      "1/463, train_loss: 0.0876\n",
      "2/463, train_loss: 0.3091\n",
      "3/463, train_loss: 0.3418\n",
      "4/463, train_loss: 0.1010\n",
      "5/463, train_loss: 0.0560\n",
      "6/463, train_loss: 0.0939\n",
      "7/463, train_loss: 0.0804\n",
      "8/463, train_loss: 0.0916\n",
      "9/463, train_loss: 0.2007\n",
      "10/463, train_loss: 0.5010\n",
      "11/463, train_loss: 0.1039\n",
      "12/463, train_loss: 0.0500\n",
      "13/463, train_loss: 0.2273\n",
      "14/463, train_loss: 0.1023\n",
      "15/463, train_loss: 0.0604\n",
      "16/463, train_loss: 0.0740\n",
      "17/463, train_loss: 0.1552\n",
      "18/463, train_loss: 0.0715\n",
      "19/463, train_loss: 0.0606\n",
      "20/463, train_loss: 0.3516\n",
      "21/463, train_loss: 0.2673\n",
      "22/463, train_loss: 0.1301\n",
      "23/463, train_loss: 0.4971\n",
      "24/463, train_loss: 0.1464\n",
      "25/463, train_loss: 0.0848\n",
      "26/463, train_loss: 0.0841\n",
      "27/463, train_loss: 0.1133\n",
      "28/463, train_loss: 0.2292\n",
      "29/463, train_loss: 0.4678\n",
      "30/463, train_loss: 0.1774\n",
      "31/463, train_loss: 0.1992\n",
      "32/463, train_loss: 0.1461\n",
      "33/463, train_loss: 0.1514\n",
      "34/463, train_loss: 0.0596\n",
      "35/463, train_loss: 0.0605\n",
      "36/463, train_loss: 0.3450\n",
      "37/463, train_loss: 0.0775\n",
      "38/463, train_loss: 0.1626\n",
      "39/463, train_loss: 0.0878\n",
      "40/463, train_loss: 0.1017\n",
      "41/463, train_loss: 0.0990\n",
      "42/463, train_loss: 0.3174\n",
      "43/463, train_loss: 0.1205\n",
      "44/463, train_loss: 0.1692\n",
      "45/463, train_loss: 0.1869\n",
      "46/463, train_loss: 0.1129\n",
      "47/463, train_loss: 0.1002\n",
      "48/463, train_loss: 0.3457\n",
      "49/463, train_loss: 0.0801\n",
      "50/463, train_loss: 0.4407\n",
      "51/463, train_loss: 0.0623\n",
      "52/463, train_loss: 0.0503\n",
      "53/463, train_loss: 0.1727\n",
      "54/463, train_loss: 0.1306\n",
      "55/463, train_loss: 0.0696\n",
      "56/463, train_loss: 0.1267\n",
      "57/463, train_loss: 0.0751\n",
      "58/463, train_loss: 0.1143\n",
      "59/463, train_loss: 0.0844\n",
      "60/463, train_loss: 0.3071\n",
      "61/463, train_loss: 0.2147\n",
      "62/463, train_loss: 0.4561\n",
      "63/463, train_loss: 0.0655\n",
      "64/463, train_loss: 0.1029\n",
      "65/463, train_loss: 0.3110\n",
      "66/463, train_loss: 0.2612\n",
      "67/463, train_loss: 0.1284\n",
      "68/463, train_loss: 0.1738\n",
      "69/463, train_loss: 0.1305\n",
      "70/463, train_loss: 0.3677\n",
      "71/463, train_loss: 0.1797\n",
      "72/463, train_loss: 0.0682\n",
      "73/463, train_loss: 0.2195\n",
      "74/463, train_loss: 0.1973\n",
      "75/463, train_loss: 0.2382\n",
      "76/463, train_loss: 0.1923\n",
      "77/463, train_loss: 0.0623\n",
      "78/463, train_loss: 0.2490\n",
      "79/463, train_loss: 0.2595\n",
      "80/463, train_loss: 0.1188\n",
      "81/463, train_loss: 0.1517\n",
      "82/463, train_loss: 0.1140\n",
      "83/463, train_loss: 0.1338\n",
      "84/463, train_loss: 0.1479\n",
      "85/463, train_loss: 0.1221\n",
      "86/463, train_loss: 0.0612\n",
      "87/463, train_loss: 0.2290\n",
      "88/463, train_loss: 0.1434\n",
      "89/463, train_loss: 0.2688\n",
      "90/463, train_loss: 0.3647\n",
      "91/463, train_loss: 0.2676\n",
      "92/463, train_loss: 0.0981\n",
      "93/463, train_loss: 0.1208\n",
      "94/463, train_loss: 0.1074\n",
      "95/463, train_loss: 0.2253\n",
      "96/463, train_loss: 0.1132\n",
      "97/463, train_loss: 0.0578\n",
      "98/463, train_loss: 0.3118\n",
      "99/463, train_loss: 0.0943\n",
      "100/463, train_loss: 0.3247\n",
      "101/463, train_loss: 0.1158\n",
      "102/463, train_loss: 0.2856\n",
      "103/463, train_loss: 0.1027\n",
      "104/463, train_loss: 0.2172\n",
      "105/463, train_loss: 0.4939\n",
      "106/463, train_loss: 0.0514\n",
      "107/463, train_loss: 0.0734\n",
      "108/463, train_loss: 0.2869\n",
      "109/463, train_loss: 0.0910\n",
      "110/463, train_loss: 0.1356\n",
      "111/463, train_loss: 0.1284\n",
      "112/463, train_loss: 0.1163\n",
      "113/463, train_loss: 0.1102\n",
      "114/463, train_loss: 0.1825\n",
      "115/463, train_loss: 0.2954\n",
      "116/463, train_loss: 0.0948\n",
      "117/463, train_loss: 0.3372\n",
      "118/463, train_loss: 0.1268\n",
      "119/463, train_loss: 0.3149\n",
      "120/463, train_loss: 0.1038\n",
      "121/463, train_loss: 0.1064\n",
      "122/463, train_loss: 0.2910\n",
      "123/463, train_loss: 0.2634\n",
      "124/463, train_loss: 0.1191\n",
      "125/463, train_loss: 0.2029\n",
      "126/463, train_loss: 0.1077\n",
      "127/463, train_loss: 0.0419\n",
      "128/463, train_loss: 0.0644\n",
      "129/463, train_loss: 0.1636\n",
      "130/463, train_loss: 0.3757\n",
      "131/463, train_loss: 0.1119\n",
      "132/463, train_loss: 0.1190\n",
      "133/463, train_loss: 0.0399\n",
      "134/463, train_loss: 0.1052\n",
      "135/463, train_loss: 0.1812\n",
      "136/463, train_loss: 0.1886\n",
      "137/463, train_loss: 0.4028\n",
      "138/463, train_loss: 0.2367\n",
      "139/463, train_loss: 0.1267\n",
      "140/463, train_loss: 0.3013\n",
      "141/463, train_loss: 0.2324\n",
      "142/463, train_loss: 0.0974\n",
      "143/463, train_loss: 0.1243\n",
      "144/463, train_loss: 0.2625\n",
      "145/463, train_loss: 0.1230\n",
      "146/463, train_loss: 0.6450\n",
      "147/463, train_loss: 0.2678\n",
      "148/463, train_loss: 0.1069\n",
      "149/463, train_loss: 0.3877\n",
      "150/463, train_loss: 0.1818\n",
      "151/463, train_loss: 0.0159\n",
      "152/463, train_loss: 0.1838\n",
      "153/463, train_loss: 0.1797\n",
      "154/463, train_loss: 0.2284\n",
      "155/463, train_loss: 0.6948\n",
      "156/463, train_loss: 0.4287\n",
      "157/463, train_loss: 0.2095\n",
      "158/463, train_loss: 0.1265\n",
      "159/463, train_loss: 0.1107\n",
      "160/463, train_loss: 0.1017\n",
      "161/463, train_loss: 0.1329\n",
      "162/463, train_loss: 0.1093\n",
      "163/463, train_loss: 0.1733\n",
      "164/463, train_loss: 0.1763\n",
      "165/463, train_loss: 0.3267\n",
      "166/463, train_loss: 0.1404\n",
      "167/463, train_loss: 0.2334\n",
      "168/463, train_loss: 0.1824\n",
      "169/463, train_loss: 0.1278\n",
      "170/463, train_loss: 0.0485\n",
      "171/463, train_loss: 0.0793\n",
      "172/463, train_loss: 0.4080\n",
      "173/463, train_loss: 0.2062\n",
      "174/463, train_loss: 0.1997\n",
      "175/463, train_loss: 0.1083\n",
      "176/463, train_loss: 0.0883\n",
      "177/463, train_loss: 0.0961\n",
      "178/463, train_loss: 0.3396\n",
      "179/463, train_loss: 0.2686\n",
      "180/463, train_loss: 0.2739\n",
      "181/463, train_loss: 0.0987\n",
      "182/463, train_loss: 0.1373\n",
      "183/463, train_loss: 0.1464\n",
      "184/463, train_loss: 0.2461\n",
      "185/463, train_loss: 0.1661\n",
      "186/463, train_loss: 0.3235\n",
      "187/463, train_loss: 0.3198\n",
      "188/463, train_loss: 0.0757\n",
      "189/463, train_loss: 0.1594\n",
      "190/463, train_loss: 0.1663\n",
      "191/463, train_loss: 0.3125\n",
      "192/463, train_loss: 0.1439\n",
      "193/463, train_loss: 0.1106\n",
      "194/463, train_loss: 0.3774\n",
      "195/463, train_loss: 0.3059\n",
      "196/463, train_loss: 0.1833\n",
      "197/463, train_loss: 0.3945\n",
      "198/463, train_loss: 0.0535\n",
      "199/463, train_loss: 0.0881\n",
      "200/463, train_loss: 0.1840\n",
      "201/463, train_loss: 0.2172\n",
      "202/463, train_loss: 0.0598\n",
      "203/463, train_loss: 0.3647\n",
      "204/463, train_loss: 0.1381\n",
      "205/463, train_loss: 0.0624\n",
      "206/463, train_loss: 0.1276\n",
      "207/463, train_loss: 0.1445\n",
      "208/463, train_loss: 0.3682\n",
      "209/463, train_loss: 0.2314\n",
      "210/463, train_loss: 0.0917\n",
      "211/463, train_loss: 0.2659\n",
      "212/463, train_loss: 0.0690\n",
      "213/463, train_loss: 0.6353\n",
      "214/463, train_loss: 0.1261\n",
      "215/463, train_loss: 0.3569\n",
      "216/463, train_loss: 0.1152\n",
      "217/463, train_loss: 0.2283\n",
      "218/463, train_loss: 0.2927\n",
      "219/463, train_loss: 0.1012\n",
      "220/463, train_loss: 0.1683\n",
      "221/463, train_loss: 0.0293\n",
      "222/463, train_loss: 0.3323\n",
      "223/463, train_loss: 0.1807\n",
      "224/463, train_loss: 0.3311\n",
      "225/463, train_loss: 0.4321\n",
      "226/463, train_loss: 0.1320\n",
      "227/463, train_loss: 0.1501\n",
      "228/463, train_loss: 0.2932\n",
      "229/463, train_loss: 0.1753\n",
      "230/463, train_loss: 0.5977\n",
      "231/463, train_loss: 0.0734\n",
      "232/463, train_loss: 0.1205\n",
      "233/463, train_loss: 0.1519\n",
      "234/463, train_loss: 0.1062\n",
      "235/463, train_loss: 0.0938\n",
      "236/463, train_loss: 0.1257\n",
      "237/463, train_loss: 0.4153\n",
      "238/463, train_loss: 0.2983\n",
      "239/463, train_loss: 0.1193\n",
      "240/463, train_loss: 0.0677\n",
      "241/463, train_loss: 0.1381\n",
      "242/463, train_loss: 0.2419\n",
      "243/463, train_loss: 0.2148\n",
      "244/463, train_loss: 0.0708\n",
      "245/463, train_loss: 0.0900\n",
      "246/463, train_loss: 0.1827\n",
      "247/463, train_loss: 0.1956\n",
      "248/463, train_loss: 0.1205\n",
      "249/463, train_loss: 0.1013\n",
      "250/463, train_loss: 0.4268\n",
      "251/463, train_loss: 0.1149\n",
      "252/463, train_loss: 0.1993\n",
      "253/463, train_loss: 0.0760\n",
      "254/463, train_loss: 0.1201\n",
      "255/463, train_loss: 0.1076\n",
      "256/463, train_loss: 0.0817\n",
      "257/463, train_loss: 0.6982\n",
      "258/463, train_loss: 0.0594\n",
      "259/463, train_loss: 0.1367\n",
      "260/463, train_loss: 0.2866\n",
      "261/463, train_loss: 0.2015\n",
      "262/463, train_loss: 0.1746\n",
      "263/463, train_loss: 0.4294\n",
      "264/463, train_loss: 0.0876\n",
      "265/463, train_loss: 0.4543\n",
      "266/463, train_loss: 0.4602\n",
      "267/463, train_loss: 0.6279\n",
      "268/463, train_loss: 0.3093\n",
      "269/463, train_loss: 0.2461\n",
      "270/463, train_loss: 0.2380\n",
      "271/463, train_loss: 0.1732\n",
      "272/463, train_loss: 0.2439\n",
      "273/463, train_loss: 0.5254\n",
      "274/463, train_loss: 0.1627\n",
      "275/463, train_loss: 0.2185\n",
      "276/463, train_loss: 0.1592\n",
      "277/463, train_loss: 0.0788\n",
      "278/463, train_loss: 0.1038\n",
      "279/463, train_loss: 0.1236\n",
      "280/463, train_loss: 0.1653\n",
      "281/463, train_loss: 0.1046\n",
      "282/463, train_loss: 0.1412\n",
      "283/463, train_loss: 0.0918\n",
      "284/463, train_loss: 0.0446\n",
      "285/463, train_loss: 0.0690\n",
      "286/463, train_loss: 0.2373\n",
      "287/463, train_loss: 0.1321\n",
      "288/463, train_loss: 0.3494\n",
      "289/463, train_loss: 0.3982\n",
      "290/463, train_loss: 0.3667\n",
      "291/463, train_loss: 0.1537\n",
      "292/463, train_loss: 0.1921\n",
      "293/463, train_loss: 0.0922\n",
      "294/463, train_loss: 0.0770\n",
      "295/463, train_loss: 0.1257\n",
      "296/463, train_loss: 0.1218\n",
      "297/463, train_loss: 0.1305\n",
      "298/463, train_loss: 0.0884\n",
      "299/463, train_loss: 0.1545\n",
      "300/463, train_loss: 0.1265\n",
      "301/463, train_loss: 0.1726\n",
      "302/463, train_loss: 0.4395\n",
      "303/463, train_loss: 0.0856\n",
      "304/463, train_loss: 0.1709\n",
      "305/463, train_loss: 0.1016\n",
      "306/463, train_loss: 0.3718\n",
      "307/463, train_loss: 0.0818\n",
      "308/463, train_loss: 0.2341\n",
      "309/463, train_loss: 0.0507\n",
      "310/463, train_loss: 0.1359\n",
      "311/463, train_loss: 0.6084\n",
      "312/463, train_loss: 0.1079\n",
      "313/463, train_loss: 0.0834\n",
      "314/463, train_loss: 0.1503\n",
      "315/463, train_loss: 0.4741\n",
      "316/463, train_loss: 0.3962\n",
      "317/463, train_loss: 0.1395\n",
      "318/463, train_loss: 0.1650\n",
      "319/463, train_loss: 0.2257\n",
      "320/463, train_loss: 0.2483\n",
      "321/463, train_loss: 0.1564\n",
      "322/463, train_loss: 0.2217\n",
      "323/463, train_loss: 0.3564\n",
      "324/463, train_loss: 0.4395\n",
      "325/463, train_loss: 0.1338\n",
      "326/463, train_loss: 0.2191\n",
      "327/463, train_loss: 0.1956\n",
      "328/463, train_loss: 0.1686\n",
      "329/463, train_loss: 0.1758\n",
      "330/463, train_loss: 0.4585\n",
      "331/463, train_loss: 0.1748\n",
      "332/463, train_loss: 0.1112\n",
      "333/463, train_loss: 0.1720\n",
      "334/463, train_loss: 0.1263\n",
      "335/463, train_loss: 0.1604\n",
      "336/463, train_loss: 0.1890\n",
      "337/463, train_loss: 0.1935\n",
      "338/463, train_loss: 0.0803\n",
      "339/463, train_loss: 0.1105\n",
      "340/463, train_loss: 0.1948\n",
      "341/463, train_loss: 0.1017\n",
      "342/463, train_loss: 0.1960\n",
      "343/463, train_loss: 0.2429\n",
      "344/463, train_loss: 0.5967\n",
      "345/463, train_loss: 0.1567\n",
      "346/463, train_loss: 0.1093\n",
      "347/463, train_loss: 0.5176\n",
      "348/463, train_loss: 0.0833\n",
      "349/463, train_loss: 0.1373\n",
      "350/463, train_loss: 0.0842\n",
      "351/463, train_loss: 0.4209\n",
      "352/463, train_loss: 0.1197\n",
      "353/463, train_loss: 0.0822\n",
      "354/463, train_loss: 0.1696\n",
      "355/463, train_loss: 0.2190\n",
      "356/463, train_loss: 0.4753\n",
      "357/463, train_loss: 0.6797\n",
      "358/463, train_loss: 0.1506\n",
      "359/463, train_loss: 0.2385\n",
      "360/463, train_loss: 0.1212\n",
      "361/463, train_loss: 0.1490\n",
      "362/463, train_loss: 0.2427\n",
      "363/463, train_loss: 0.4631\n",
      "364/463, train_loss: 0.0779\n",
      "365/463, train_loss: 0.2028\n",
      "366/463, train_loss: 0.1289\n",
      "367/463, train_loss: 0.3530\n",
      "368/463, train_loss: 0.0569\n",
      "369/463, train_loss: 0.0874\n",
      "370/463, train_loss: 0.3164\n",
      "371/463, train_loss: 0.1095\n",
      "372/463, train_loss: 0.1604\n",
      "373/463, train_loss: 0.1194\n",
      "374/463, train_loss: 0.0634\n",
      "375/463, train_loss: 0.3408\n",
      "376/463, train_loss: 0.1511\n",
      "377/463, train_loss: 0.4214\n",
      "378/463, train_loss: 0.2930\n",
      "379/463, train_loss: 0.1488\n",
      "380/463, train_loss: 0.2123\n",
      "381/463, train_loss: 0.2146\n",
      "382/463, train_loss: 0.1313\n",
      "383/463, train_loss: 0.1048\n",
      "384/463, train_loss: 0.0989\n",
      "385/463, train_loss: 0.5454\n",
      "386/463, train_loss: 0.5483\n",
      "387/463, train_loss: 0.1699\n",
      "388/463, train_loss: 0.1012\n",
      "389/463, train_loss: 0.1221\n",
      "390/463, train_loss: 0.1145\n",
      "391/463, train_loss: 0.1473\n",
      "392/463, train_loss: 0.1320\n",
      "393/463, train_loss: 0.5996\n",
      "394/463, train_loss: 0.1494\n",
      "395/463, train_loss: 0.3379\n",
      "396/463, train_loss: 0.1442\n",
      "397/463, train_loss: 0.2673\n",
      "398/463, train_loss: 0.3516\n",
      "399/463, train_loss: 0.1181\n",
      "400/463, train_loss: 0.1299\n",
      "401/463, train_loss: 0.1044\n",
      "402/463, train_loss: 0.5703\n",
      "403/463, train_loss: 0.1356\n",
      "404/463, train_loss: 0.1520\n",
      "405/463, train_loss: 0.0682\n",
      "406/463, train_loss: 0.9341\n",
      "407/463, train_loss: 0.3674\n",
      "408/463, train_loss: 0.1805\n",
      "409/463, train_loss: 0.1572\n",
      "410/463, train_loss: 0.0861\n",
      "411/463, train_loss: 0.1155\n",
      "412/463, train_loss: 0.1531\n",
      "413/463, train_loss: 0.3633\n",
      "414/463, train_loss: 0.0919\n",
      "415/463, train_loss: 0.3406\n",
      "416/463, train_loss: 0.1211\n",
      "417/463, train_loss: 0.1467\n",
      "418/463, train_loss: 0.1104\n",
      "419/463, train_loss: 0.1921\n",
      "420/463, train_loss: 0.1743\n",
      "421/463, train_loss: 0.1370\n",
      "422/463, train_loss: 0.0599\n",
      "423/463, train_loss: 0.1405\n",
      "424/463, train_loss: 0.0945\n",
      "425/463, train_loss: 0.1333\n",
      "426/463, train_loss: 0.1815\n",
      "427/463, train_loss: 0.1637\n",
      "428/463, train_loss: 0.1655\n",
      "429/463, train_loss: 0.2325\n",
      "430/463, train_loss: 0.1714\n",
      "431/463, train_loss: 0.2097\n",
      "432/463, train_loss: 0.2305\n",
      "433/463, train_loss: 0.3093\n",
      "434/463, train_loss: 0.1294\n",
      "435/463, train_loss: 0.1104\n",
      "436/463, train_loss: 0.0880\n",
      "437/463, train_loss: 0.1592\n",
      "438/463, train_loss: 0.1141\n",
      "439/463, train_loss: 0.0794\n",
      "440/463, train_loss: 0.2150\n",
      "441/463, train_loss: 0.0948\n",
      "442/463, train_loss: 0.2014\n",
      "443/463, train_loss: 0.0805\n",
      "444/463, train_loss: 1.3691\n",
      "445/463, train_loss: 0.1057\n",
      "446/463, train_loss: 0.1660\n",
      "447/463, train_loss: 0.1638\n",
      "448/463, train_loss: 0.0884\n",
      "449/463, train_loss: 0.1144\n",
      "450/463, train_loss: 0.0627\n",
      "451/463, train_loss: 0.3230\n",
      "452/463, train_loss: 0.3496\n",
      "453/463, train_loss: 0.0665\n",
      "454/463, train_loss: 0.0715\n",
      "455/463, train_loss: 0.1299\n",
      "456/463, train_loss: 0.0890\n",
      "457/463, train_loss: 0.1156\n",
      "458/463, train_loss: 0.1387\n",
      "459/463, train_loss: 0.0583\n",
      "460/463, train_loss: 0.1544\n",
      "461/463, train_loss: 0.1282\n",
      "462/463, train_loss: 0.2209\n",
      "463/463, train_loss: 0.1005\n",
      "epoch 24 average loss: 0.1976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 11:05:23 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 11:05:26 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 11:05:28 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 25/100\n",
      "1/463, train_loss: 0.1299\n",
      "2/463, train_loss: 0.2681\n",
      "3/463, train_loss: 0.1829\n",
      "4/463, train_loss: 0.0538\n",
      "5/463, train_loss: 0.0911\n",
      "6/463, train_loss: 0.0930\n",
      "7/463, train_loss: 0.1065\n",
      "8/463, train_loss: 0.1387\n",
      "9/463, train_loss: 0.1975\n",
      "10/463, train_loss: 0.1541\n",
      "11/463, train_loss: 0.0696\n",
      "12/463, train_loss: 0.0779\n",
      "13/463, train_loss: 0.1042\n",
      "14/463, train_loss: 0.4668\n",
      "15/463, train_loss: 0.2380\n",
      "16/463, train_loss: 0.0376\n",
      "17/463, train_loss: 0.1121\n",
      "18/463, train_loss: 0.0607\n",
      "19/463, train_loss: 0.0536\n",
      "20/463, train_loss: 0.1081\n",
      "21/463, train_loss: 0.0336\n",
      "22/463, train_loss: 0.0936\n",
      "23/463, train_loss: 0.4468\n",
      "24/463, train_loss: 0.2185\n",
      "25/463, train_loss: 0.1427\n",
      "26/463, train_loss: 0.0760\n",
      "27/463, train_loss: 0.1152\n",
      "28/463, train_loss: 0.4751\n",
      "29/463, train_loss: 0.1242\n",
      "30/463, train_loss: 0.1207\n",
      "31/463, train_loss: 0.3501\n",
      "32/463, train_loss: 0.1539\n",
      "33/463, train_loss: 0.0880\n",
      "34/463, train_loss: 0.3735\n",
      "35/463, train_loss: 0.3120\n",
      "36/463, train_loss: 0.2034\n",
      "37/463, train_loss: 0.2415\n",
      "38/463, train_loss: 0.0773\n",
      "39/463, train_loss: 0.2832\n",
      "40/463, train_loss: 0.2588\n",
      "41/463, train_loss: 0.6909\n",
      "42/463, train_loss: 0.1588\n",
      "43/463, train_loss: 0.0908\n",
      "44/463, train_loss: 0.2131\n",
      "45/463, train_loss: 0.0872\n",
      "46/463, train_loss: 0.2615\n",
      "47/463, train_loss: 0.1934\n",
      "48/463, train_loss: 0.1875\n",
      "49/463, train_loss: 0.0715\n",
      "50/463, train_loss: 0.0960\n",
      "51/463, train_loss: 0.1925\n",
      "52/463, train_loss: 0.1257\n",
      "53/463, train_loss: 0.1219\n",
      "54/463, train_loss: 0.2415\n",
      "55/463, train_loss: 0.0855\n",
      "56/463, train_loss: 0.0628\n",
      "57/463, train_loss: 0.2480\n",
      "58/463, train_loss: 0.0507\n",
      "59/463, train_loss: 0.0762\n",
      "60/463, train_loss: 0.0816\n",
      "61/463, train_loss: 0.4634\n",
      "62/463, train_loss: 0.0728\n",
      "63/463, train_loss: 0.1227\n",
      "64/463, train_loss: 0.1132\n",
      "65/463, train_loss: 0.0969\n",
      "66/463, train_loss: 0.0671\n",
      "67/463, train_loss: 0.1313\n",
      "68/463, train_loss: 0.1272\n",
      "69/463, train_loss: 0.2197\n",
      "70/463, train_loss: 0.6655\n",
      "71/463, train_loss: 0.2920\n",
      "72/463, train_loss: 0.0794\n",
      "73/463, train_loss: 0.0871\n",
      "74/463, train_loss: 0.4419\n",
      "75/463, train_loss: 0.0652\n",
      "76/463, train_loss: 0.0561\n",
      "77/463, train_loss: 0.2185\n",
      "78/463, train_loss: 0.1853\n",
      "79/463, train_loss: 0.1130\n",
      "80/463, train_loss: 0.0824\n",
      "81/463, train_loss: 0.0585\n",
      "82/463, train_loss: 0.1159\n",
      "83/463, train_loss: 0.1189\n",
      "84/463, train_loss: 0.1545\n",
      "85/463, train_loss: 0.0754\n",
      "86/463, train_loss: 0.1710\n",
      "87/463, train_loss: 0.0681\n",
      "88/463, train_loss: 0.2212\n",
      "89/463, train_loss: 0.5737\n",
      "90/463, train_loss: 0.1067\n",
      "91/463, train_loss: 0.1204\n",
      "92/463, train_loss: 0.1733\n",
      "93/463, train_loss: 0.5000\n",
      "94/463, train_loss: 0.1924\n",
      "95/463, train_loss: 0.2302\n",
      "96/463, train_loss: 0.1729\n",
      "97/463, train_loss: 0.0792\n",
      "98/463, train_loss: 0.0807\n",
      "99/463, train_loss: 0.1979\n",
      "100/463, train_loss: 0.1191\n",
      "101/463, train_loss: 0.1068\n",
      "102/463, train_loss: 0.4028\n",
      "103/463, train_loss: 0.0834\n",
      "104/463, train_loss: 0.2327\n",
      "105/463, train_loss: 0.1061\n",
      "106/463, train_loss: 0.0747\n",
      "107/463, train_loss: 0.1694\n",
      "108/463, train_loss: 0.1815\n",
      "109/463, train_loss: 0.0948\n",
      "110/463, train_loss: 0.2468\n",
      "111/463, train_loss: 0.1619\n",
      "112/463, train_loss: 0.1108\n",
      "113/463, train_loss: 0.1671\n",
      "114/463, train_loss: 0.1981\n",
      "115/463, train_loss: 0.0517\n",
      "116/463, train_loss: 0.2957\n",
      "117/463, train_loss: 0.2427\n",
      "118/463, train_loss: 0.1182\n",
      "119/463, train_loss: 0.0646\n",
      "120/463, train_loss: 0.2046\n",
      "121/463, train_loss: 0.0157\n",
      "122/463, train_loss: 0.2039\n",
      "123/463, train_loss: 0.2068\n",
      "124/463, train_loss: 0.2146\n",
      "125/463, train_loss: 0.1157\n",
      "126/463, train_loss: 0.0596\n",
      "127/463, train_loss: 0.1019\n",
      "128/463, train_loss: 0.0886\n",
      "129/463, train_loss: 0.2603\n",
      "130/463, train_loss: 0.0629\n",
      "131/463, train_loss: 0.1425\n",
      "132/463, train_loss: 0.3301\n",
      "133/463, train_loss: 0.1033\n",
      "134/463, train_loss: 0.2112\n",
      "135/463, train_loss: 0.1215\n",
      "136/463, train_loss: 0.3276\n",
      "137/463, train_loss: 0.4287\n",
      "138/463, train_loss: 0.1796\n",
      "139/463, train_loss: 0.5405\n",
      "140/463, train_loss: 0.1553\n",
      "141/463, train_loss: 0.3257\n",
      "142/463, train_loss: 0.0723\n",
      "143/463, train_loss: 0.1797\n",
      "144/463, train_loss: 0.5601\n",
      "145/463, train_loss: 1.0469\n",
      "146/463, train_loss: 0.1814\n",
      "147/463, train_loss: 0.2954\n",
      "148/463, train_loss: 0.1180\n",
      "149/463, train_loss: 0.4177\n",
      "150/463, train_loss: 0.1047\n",
      "151/463, train_loss: 0.1014\n",
      "152/463, train_loss: 0.0851\n",
      "153/463, train_loss: 0.3110\n",
      "154/463, train_loss: 0.2290\n",
      "155/463, train_loss: 0.1387\n",
      "156/463, train_loss: 0.5020\n",
      "157/463, train_loss: 0.2026\n",
      "158/463, train_loss: 0.2632\n",
      "159/463, train_loss: 0.2405\n",
      "160/463, train_loss: 0.3320\n",
      "161/463, train_loss: 0.1037\n",
      "162/463, train_loss: 0.1649\n",
      "163/463, train_loss: 0.1133\n",
      "164/463, train_loss: 0.4282\n",
      "165/463, train_loss: 0.0788\n",
      "166/463, train_loss: 0.1233\n",
      "167/463, train_loss: 0.1844\n",
      "168/463, train_loss: 0.1201\n",
      "169/463, train_loss: 0.3289\n",
      "170/463, train_loss: 0.2546\n",
      "171/463, train_loss: 0.1098\n",
      "172/463, train_loss: 0.1792\n",
      "173/463, train_loss: 0.3440\n",
      "174/463, train_loss: 0.0877\n",
      "175/463, train_loss: 0.3064\n",
      "176/463, train_loss: 0.2322\n",
      "177/463, train_loss: 0.1729\n",
      "178/463, train_loss: 0.0548\n",
      "179/463, train_loss: 0.1284\n",
      "180/463, train_loss: 0.0802\n",
      "181/463, train_loss: 0.2822\n",
      "182/463, train_loss: 0.3230\n",
      "183/463, train_loss: 0.1179\n",
      "184/463, train_loss: 0.1549\n",
      "185/463, train_loss: 0.1683\n",
      "186/463, train_loss: 0.1075\n",
      "187/463, train_loss: 0.3181\n",
      "188/463, train_loss: 0.6333\n",
      "189/463, train_loss: 0.1643\n",
      "190/463, train_loss: 0.2052\n",
      "191/463, train_loss: 0.2028\n",
      "192/463, train_loss: 0.0671\n",
      "193/463, train_loss: 0.4121\n",
      "194/463, train_loss: 0.0980\n",
      "195/463, train_loss: 0.2487\n",
      "196/463, train_loss: 0.1128\n",
      "197/463, train_loss: 0.1548\n",
      "198/463, train_loss: 0.1046\n",
      "199/463, train_loss: 0.0903\n",
      "200/463, train_loss: 0.2120\n",
      "201/463, train_loss: 0.2427\n",
      "202/463, train_loss: 0.1764\n",
      "203/463, train_loss: 0.1569\n",
      "204/463, train_loss: 0.1681\n",
      "205/463, train_loss: 0.0863\n",
      "206/463, train_loss: 0.4841\n",
      "207/463, train_loss: 0.3782\n",
      "208/463, train_loss: 0.0772\n",
      "209/463, train_loss: 0.1650\n",
      "210/463, train_loss: 0.1088\n",
      "211/463, train_loss: 0.1248\n",
      "212/463, train_loss: 0.1365\n",
      "213/463, train_loss: 0.1571\n",
      "214/463, train_loss: 0.4102\n",
      "215/463, train_loss: 0.0994\n",
      "216/463, train_loss: 0.1901\n",
      "217/463, train_loss: 0.0905\n",
      "218/463, train_loss: 0.1703\n",
      "219/463, train_loss: 0.3459\n",
      "220/463, train_loss: 0.1071\n",
      "221/463, train_loss: 0.1343\n",
      "222/463, train_loss: 0.0731\n",
      "223/463, train_loss: 0.0577\n",
      "224/463, train_loss: 0.2461\n",
      "225/463, train_loss: 0.4214\n",
      "226/463, train_loss: 0.1418\n",
      "227/463, train_loss: 0.0945\n",
      "228/463, train_loss: 0.2091\n",
      "229/463, train_loss: 0.3071\n",
      "230/463, train_loss: 0.0898\n",
      "231/463, train_loss: 0.3691\n",
      "232/463, train_loss: 0.1306\n",
      "233/463, train_loss: 0.0486\n",
      "234/463, train_loss: 0.0906\n",
      "235/463, train_loss: 0.1870\n",
      "236/463, train_loss: 0.1691\n",
      "237/463, train_loss: 0.3728\n",
      "238/463, train_loss: 0.1276\n",
      "239/463, train_loss: 0.2419\n",
      "240/463, train_loss: 0.4287\n",
      "241/463, train_loss: 0.1290\n",
      "242/463, train_loss: 0.1533\n",
      "243/463, train_loss: 0.0880\n",
      "244/463, train_loss: 0.1205\n",
      "245/463, train_loss: 0.0692\n",
      "246/463, train_loss: 0.6035\n",
      "247/463, train_loss: 0.2632\n",
      "248/463, train_loss: 0.4419\n",
      "249/463, train_loss: 0.0909\n",
      "250/463, train_loss: 0.1884\n",
      "251/463, train_loss: 0.4543\n",
      "252/463, train_loss: 0.8569\n",
      "253/463, train_loss: 0.4031\n",
      "254/463, train_loss: 0.0707\n",
      "255/463, train_loss: 0.1221\n",
      "256/463, train_loss: 0.1353\n",
      "257/463, train_loss: 0.1213\n",
      "258/463, train_loss: 0.1049\n",
      "259/463, train_loss: 0.1470\n",
      "260/463, train_loss: 0.1351\n",
      "261/463, train_loss: 0.1581\n",
      "262/463, train_loss: 0.1472\n",
      "263/463, train_loss: 0.1527\n",
      "264/463, train_loss: 0.1392\n",
      "265/463, train_loss: 0.1772\n",
      "266/463, train_loss: 0.3274\n",
      "267/463, train_loss: 0.1201\n",
      "268/463, train_loss: 0.3193\n",
      "269/463, train_loss: 0.1091\n",
      "270/463, train_loss: 0.6094\n",
      "271/463, train_loss: 0.1382\n",
      "272/463, train_loss: 0.1660\n",
      "273/463, train_loss: 0.1624\n",
      "274/463, train_loss: 0.1609\n",
      "275/463, train_loss: 0.1021\n",
      "276/463, train_loss: 0.1089\n",
      "277/463, train_loss: 0.2451\n",
      "278/463, train_loss: 0.2089\n",
      "279/463, train_loss: 0.4172\n",
      "280/463, train_loss: 0.1885\n",
      "281/463, train_loss: 0.1024\n",
      "282/463, train_loss: 0.1763\n",
      "283/463, train_loss: 0.0978\n",
      "284/463, train_loss: 0.1334\n",
      "285/463, train_loss: 0.1605\n",
      "286/463, train_loss: 0.4207\n",
      "287/463, train_loss: 0.1410\n",
      "288/463, train_loss: 0.1814\n",
      "289/463, train_loss: 0.1597\n",
      "290/463, train_loss: 0.2993\n",
      "291/463, train_loss: 0.1219\n",
      "292/463, train_loss: 0.0726\n",
      "293/463, train_loss: 0.1993\n",
      "294/463, train_loss: 0.2827\n",
      "295/463, train_loss: 0.4565\n",
      "296/463, train_loss: 0.1704\n",
      "297/463, train_loss: 0.1328\n",
      "298/463, train_loss: 0.1927\n",
      "299/463, train_loss: 0.2786\n",
      "300/463, train_loss: 0.1147\n",
      "301/463, train_loss: 0.2479\n",
      "302/463, train_loss: 0.1841\n",
      "303/463, train_loss: 0.1164\n",
      "304/463, train_loss: 0.0785\n",
      "305/463, train_loss: 0.5078\n",
      "306/463, train_loss: 0.0621\n",
      "307/463, train_loss: 0.1475\n",
      "308/463, train_loss: 0.1732\n",
      "309/463, train_loss: 0.2913\n",
      "310/463, train_loss: 0.1118\n",
      "311/463, train_loss: 0.1926\n",
      "312/463, train_loss: 0.1857\n",
      "313/463, train_loss: 0.2666\n",
      "314/463, train_loss: 0.1708\n",
      "315/463, train_loss: 0.0917\n",
      "316/463, train_loss: 0.0873\n",
      "317/463, train_loss: 0.2778\n",
      "318/463, train_loss: 0.1367\n",
      "319/463, train_loss: 0.4075\n",
      "320/463, train_loss: 0.1158\n",
      "321/463, train_loss: 0.2681\n",
      "322/463, train_loss: 0.6660\n",
      "323/463, train_loss: 0.2822\n",
      "324/463, train_loss: 0.2271\n",
      "325/463, train_loss: 0.1729\n",
      "326/463, train_loss: 0.2084\n",
      "327/463, train_loss: 0.2915\n",
      "328/463, train_loss: 0.0992\n",
      "329/463, train_loss: 0.3506\n",
      "330/463, train_loss: 0.1166\n",
      "331/463, train_loss: 0.4600\n",
      "332/463, train_loss: 0.1033\n",
      "333/463, train_loss: 0.1082\n",
      "334/463, train_loss: 0.2480\n",
      "335/463, train_loss: 0.2189\n",
      "336/463, train_loss: 0.1289\n",
      "337/463, train_loss: 0.3281\n",
      "338/463, train_loss: 0.1224\n",
      "339/463, train_loss: 0.3315\n",
      "340/463, train_loss: 0.0703\n",
      "341/463, train_loss: 0.2102\n",
      "342/463, train_loss: 0.3027\n",
      "343/463, train_loss: 0.5146\n",
      "344/463, train_loss: 0.0730\n",
      "345/463, train_loss: 0.0752\n",
      "346/463, train_loss: 0.1202\n",
      "347/463, train_loss: 0.0885\n",
      "348/463, train_loss: 0.1626\n",
      "349/463, train_loss: 0.1960\n",
      "350/463, train_loss: 0.1376\n",
      "351/463, train_loss: 0.0931\n",
      "352/463, train_loss: 0.0797\n",
      "353/463, train_loss: 0.1378\n",
      "354/463, train_loss: 0.1097\n",
      "355/463, train_loss: 0.1606\n",
      "356/463, train_loss: 0.3899\n",
      "357/463, train_loss: 0.1418\n",
      "358/463, train_loss: 0.1222\n",
      "359/463, train_loss: 0.3848\n",
      "360/463, train_loss: 0.3391\n",
      "361/463, train_loss: 0.1554\n",
      "362/463, train_loss: 0.1409\n",
      "363/463, train_loss: 0.2461\n",
      "364/463, train_loss: 0.3132\n",
      "365/463, train_loss: 0.0834\n",
      "366/463, train_loss: 0.2524\n",
      "367/463, train_loss: 0.1207\n",
      "368/463, train_loss: 0.2002\n",
      "369/463, train_loss: 0.1404\n",
      "370/463, train_loss: 0.1160\n",
      "371/463, train_loss: 0.0617\n",
      "372/463, train_loss: 0.4746\n",
      "373/463, train_loss: 0.1450\n",
      "374/463, train_loss: 0.1313\n",
      "375/463, train_loss: 0.1240\n",
      "376/463, train_loss: 0.1072\n",
      "377/463, train_loss: 0.4731\n",
      "378/463, train_loss: 0.3740\n",
      "379/463, train_loss: 0.3057\n",
      "380/463, train_loss: 0.1164\n",
      "381/463, train_loss: 0.1964\n",
      "382/463, train_loss: 0.2214\n",
      "383/463, train_loss: 0.3467\n",
      "384/463, train_loss: 0.1073\n",
      "385/463, train_loss: 0.1562\n",
      "386/463, train_loss: 0.1274\n",
      "387/463, train_loss: 0.1356\n",
      "388/463, train_loss: 0.2683\n",
      "389/463, train_loss: 0.1587\n",
      "390/463, train_loss: 0.1393\n",
      "391/463, train_loss: 0.0922\n",
      "392/463, train_loss: 0.4189\n",
      "393/463, train_loss: 0.1029\n",
      "394/463, train_loss: 0.1787\n",
      "395/463, train_loss: 0.1038\n",
      "396/463, train_loss: 0.3062\n",
      "397/463, train_loss: 0.3755\n",
      "398/463, train_loss: 0.1781\n",
      "399/463, train_loss: 0.1868\n",
      "400/463, train_loss: 0.2197\n",
      "401/463, train_loss: 0.1572\n",
      "402/463, train_loss: 0.1158\n",
      "403/463, train_loss: 0.2173\n",
      "404/463, train_loss: 0.1436\n",
      "405/463, train_loss: 0.1742\n",
      "406/463, train_loss: 0.1199\n",
      "407/463, train_loss: 0.1095\n",
      "408/463, train_loss: 0.0679\n",
      "409/463, train_loss: 0.2146\n",
      "410/463, train_loss: 0.0984\n",
      "411/463, train_loss: 0.1013\n",
      "412/463, train_loss: 0.1234\n",
      "413/463, train_loss: 0.1819\n",
      "414/463, train_loss: 0.5386\n",
      "415/463, train_loss: 0.1630\n",
      "416/463, train_loss: 0.5923\n",
      "417/463, train_loss: 0.0752\n",
      "418/463, train_loss: 0.1611\n",
      "419/463, train_loss: 0.1794\n",
      "420/463, train_loss: 0.0786\n",
      "421/463, train_loss: 0.0384\n",
      "422/463, train_loss: 0.0851\n",
      "423/463, train_loss: 0.2439\n",
      "424/463, train_loss: 0.1855\n",
      "425/463, train_loss: 0.3008\n",
      "426/463, train_loss: 0.3533\n",
      "427/463, train_loss: 0.0887\n",
      "428/463, train_loss: 0.4856\n",
      "429/463, train_loss: 0.1482\n",
      "430/463, train_loss: 0.1150\n",
      "431/463, train_loss: 0.2498\n",
      "432/463, train_loss: 0.5376\n",
      "433/463, train_loss: 0.1909\n",
      "434/463, train_loss: 0.2881\n",
      "435/463, train_loss: 0.1586\n",
      "436/463, train_loss: 0.0814\n",
      "437/463, train_loss: 0.1426\n",
      "438/463, train_loss: 0.2827\n",
      "439/463, train_loss: 0.1204\n",
      "440/463, train_loss: 0.0934\n",
      "441/463, train_loss: 0.2734\n",
      "442/463, train_loss: 0.1411\n",
      "443/463, train_loss: 0.1487\n",
      "444/463, train_loss: 0.1552\n",
      "445/463, train_loss: 0.3420\n",
      "446/463, train_loss: 0.5039\n",
      "447/463, train_loss: 0.2346\n",
      "448/463, train_loss: 0.5205\n",
      "449/463, train_loss: 0.2454\n",
      "450/463, train_loss: 0.1240\n",
      "451/463, train_loss: 0.4248\n",
      "452/463, train_loss: 0.1261\n",
      "453/463, train_loss: 0.1466\n",
      "454/463, train_loss: 0.3511\n",
      "455/463, train_loss: 0.2098\n",
      "456/463, train_loss: 0.1899\n",
      "457/463, train_loss: 0.4727\n",
      "458/463, train_loss: 0.1292\n",
      "459/463, train_loss: 0.4089\n",
      "460/463, train_loss: 0.1932\n",
      "461/463, train_loss: 0.0983\n",
      "462/463, train_loss: 0.2515\n",
      "463/463, train_loss: 0.1146\n",
      "epoch 25 average loss: 0.2007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 13:18:57 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 13:19:01 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 13:19:03 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mixed/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630.nii.gz\n",
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.4444841753765203, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.4444841753765203, 'AP_IoU_0.10_MaxDet_100': 0.460098498749851, 'nodule_AP_IoU_0.10_MaxDet_100': 0.460098498749851, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.9230769276618958, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.9230769276618958, 'AR_IoU_0.10_MaxDet_100': 0.9487179517745972, 'nodule_AR_IoU_0.10_MaxDet_100': 0.9487179517745972}\n",
      "current epoch: 25 current metric: 0.6941 best metric: 0.7468 at epoch 15\n",
      "----------\n",
      "epoch 26/100\n",
      "1/463, train_loss: 0.1036\n",
      "2/463, train_loss: 0.2839\n",
      "3/463, train_loss: 0.1099\n",
      "4/463, train_loss: 0.1565\n",
      "5/463, train_loss: 0.9873\n",
      "6/463, train_loss: 0.0953\n",
      "7/463, train_loss: 0.0662\n",
      "8/463, train_loss: 0.1780\n",
      "9/463, train_loss: 0.1852\n",
      "10/463, train_loss: 0.0634\n",
      "11/463, train_loss: 0.0788\n",
      "12/463, train_loss: 0.1072\n",
      "13/463, train_loss: 0.1168\n",
      "14/463, train_loss: 0.1093\n",
      "15/463, train_loss: 0.0814\n",
      "16/463, train_loss: 0.1599\n",
      "17/463, train_loss: 0.1241\n",
      "18/463, train_loss: 0.1333\n",
      "19/463, train_loss: 0.1488\n",
      "20/463, train_loss: 0.1831\n",
      "21/463, train_loss: 0.5918\n",
      "22/463, train_loss: 0.0772\n",
      "23/463, train_loss: 0.1096\n",
      "24/463, train_loss: 0.0936\n",
      "25/463, train_loss: 0.3699\n",
      "26/463, train_loss: 0.4568\n",
      "27/463, train_loss: 0.0538\n",
      "28/463, train_loss: 0.1553\n",
      "29/463, train_loss: 0.0891\n",
      "30/463, train_loss: 0.1648\n",
      "31/463, train_loss: 0.4868\n",
      "32/463, train_loss: 0.1360\n",
      "33/463, train_loss: 0.1489\n",
      "34/463, train_loss: 0.1048\n",
      "35/463, train_loss: 0.1466\n",
      "36/463, train_loss: 0.1340\n",
      "37/463, train_loss: 0.1410\n",
      "38/463, train_loss: 0.2791\n",
      "39/463, train_loss: 0.0560\n",
      "40/463, train_loss: 0.2139\n",
      "41/463, train_loss: 0.0973\n",
      "42/463, train_loss: 0.1890\n",
      "43/463, train_loss: 0.5688\n",
      "44/463, train_loss: 0.2749\n",
      "45/463, train_loss: 0.2275\n",
      "46/463, train_loss: 0.1542\n",
      "47/463, train_loss: 0.1813\n",
      "48/463, train_loss: 0.0905\n",
      "49/463, train_loss: 0.1951\n",
      "50/463, train_loss: 0.0469\n",
      "51/463, train_loss: 0.2225\n",
      "52/463, train_loss: 0.0699\n",
      "53/463, train_loss: 0.1306\n",
      "54/463, train_loss: 0.0881\n",
      "55/463, train_loss: 0.2595\n",
      "56/463, train_loss: 0.0797\n",
      "57/463, train_loss: 0.2878\n",
      "58/463, train_loss: 0.1223\n",
      "59/463, train_loss: 0.0584\n",
      "60/463, train_loss: 0.0925\n",
      "61/463, train_loss: 0.2876\n",
      "62/463, train_loss: 0.5718\n",
      "63/463, train_loss: 0.4817\n",
      "64/463, train_loss: 0.3506\n",
      "65/463, train_loss: 0.1538\n",
      "66/463, train_loss: 0.1564\n",
      "67/463, train_loss: 0.3430\n",
      "68/463, train_loss: 0.4087\n",
      "69/463, train_loss: 0.1174\n",
      "70/463, train_loss: 0.0838\n",
      "71/463, train_loss: 0.0999\n",
      "72/463, train_loss: 0.1521\n",
      "73/463, train_loss: 0.1614\n",
      "74/463, train_loss: 0.1399\n",
      "75/463, train_loss: 0.0956\n",
      "76/463, train_loss: 0.1633\n",
      "77/463, train_loss: 0.2585\n",
      "78/463, train_loss: 0.0869\n",
      "79/463, train_loss: 0.0790\n",
      "80/463, train_loss: 0.1243\n",
      "81/463, train_loss: 0.3220\n",
      "82/463, train_loss: 0.0677\n",
      "83/463, train_loss: 0.2416\n",
      "84/463, train_loss: 0.1216\n",
      "85/463, train_loss: 0.1252\n",
      "86/463, train_loss: 0.1741\n",
      "87/463, train_loss: 0.2260\n",
      "88/463, train_loss: 0.2429\n",
      "89/463, train_loss: 0.1029\n",
      "90/463, train_loss: 0.0769\n",
      "91/463, train_loss: 0.0525\n",
      "92/463, train_loss: 0.0821\n",
      "93/463, train_loss: 0.1287\n",
      "94/463, train_loss: 0.3472\n",
      "95/463, train_loss: 0.2344\n",
      "96/463, train_loss: 0.0730\n",
      "97/463, train_loss: 0.2065\n",
      "98/463, train_loss: 0.3701\n",
      "99/463, train_loss: 0.5610\n",
      "100/463, train_loss: 0.1519\n",
      "101/463, train_loss: 0.6650\n",
      "102/463, train_loss: 0.1077\n",
      "103/463, train_loss: 0.1057\n",
      "104/463, train_loss: 0.0649\n",
      "105/463, train_loss: 0.2656\n",
      "106/463, train_loss: 0.1774\n",
      "107/463, train_loss: 0.1870\n",
      "108/463, train_loss: 0.1239\n",
      "109/463, train_loss: 0.0655\n",
      "110/463, train_loss: 0.1670\n",
      "111/463, train_loss: 0.1029\n",
      "112/463, train_loss: 0.1676\n",
      "113/463, train_loss: 0.2412\n",
      "114/463, train_loss: 0.3511\n",
      "115/463, train_loss: 0.1582\n",
      "116/463, train_loss: 0.0823\n",
      "117/463, train_loss: 0.1562\n",
      "118/463, train_loss: 0.1068\n",
      "119/463, train_loss: 0.2698\n",
      "120/463, train_loss: 0.1785\n",
      "121/463, train_loss: 0.0885\n",
      "122/463, train_loss: 0.5215\n",
      "123/463, train_loss: 0.0684\n",
      "124/463, train_loss: 0.0682\n",
      "125/463, train_loss: 0.0620\n",
      "126/463, train_loss: 0.1495\n",
      "127/463, train_loss: 0.0988\n",
      "128/463, train_loss: 0.6909\n",
      "129/463, train_loss: 0.0961\n",
      "130/463, train_loss: 0.1648\n",
      "131/463, train_loss: 0.0836\n",
      "132/463, train_loss: 0.6294\n",
      "133/463, train_loss: 0.5962\n",
      "134/463, train_loss: 0.1498\n",
      "135/463, train_loss: 0.3416\n",
      "136/463, train_loss: 0.1055\n",
      "137/463, train_loss: 0.0791\n",
      "138/463, train_loss: 0.1317\n",
      "139/463, train_loss: 0.1461\n",
      "140/463, train_loss: 0.3403\n",
      "141/463, train_loss: 0.1567\n",
      "142/463, train_loss: 0.1234\n",
      "143/463, train_loss: 0.1738\n",
      "144/463, train_loss: 0.0960\n",
      "145/463, train_loss: 0.5044\n",
      "146/463, train_loss: 0.1538\n",
      "147/463, train_loss: 0.2314\n",
      "148/463, train_loss: 0.1494\n",
      "149/463, train_loss: 0.0869\n",
      "150/463, train_loss: 0.0967\n",
      "151/463, train_loss: 0.2069\n",
      "152/463, train_loss: 0.1855\n",
      "153/463, train_loss: 0.1979\n",
      "154/463, train_loss: 0.1616\n",
      "155/463, train_loss: 0.7715\n",
      "156/463, train_loss: 0.1770\n",
      "157/463, train_loss: 0.2039\n",
      "158/463, train_loss: 0.1047\n",
      "159/463, train_loss: 0.3403\n",
      "160/463, train_loss: 0.2637\n",
      "161/463, train_loss: 0.1565\n",
      "162/463, train_loss: 0.0869\n",
      "163/463, train_loss: 0.0918\n",
      "164/463, train_loss: 0.1248\n",
      "165/463, train_loss: 0.1313\n",
      "166/463, train_loss: 0.0658\n",
      "167/463, train_loss: 0.0732\n",
      "168/463, train_loss: 0.0463\n",
      "169/463, train_loss: 0.1620\n",
      "170/463, train_loss: 0.1144\n",
      "171/463, train_loss: 0.1039\n",
      "172/463, train_loss: 0.1384\n",
      "173/463, train_loss: 0.0808\n",
      "174/463, train_loss: 0.1285\n",
      "175/463, train_loss: 0.1804\n",
      "176/463, train_loss: 0.1078\n",
      "177/463, train_loss: 0.2394\n",
      "178/463, train_loss: 0.6133\n",
      "179/463, train_loss: 0.1885\n",
      "180/463, train_loss: 0.1484\n",
      "181/463, train_loss: 0.2583\n",
      "182/463, train_loss: 0.1356\n",
      "183/463, train_loss: 0.1897\n",
      "184/463, train_loss: 0.3416\n",
      "185/463, train_loss: 0.1777\n",
      "186/463, train_loss: 0.2308\n",
      "187/463, train_loss: 0.1392\n",
      "188/463, train_loss: 0.1597\n",
      "189/463, train_loss: 0.1827\n",
      "190/463, train_loss: 0.0885\n",
      "191/463, train_loss: 0.1360\n",
      "192/463, train_loss: 0.2083\n",
      "193/463, train_loss: 0.2043\n",
      "194/463, train_loss: 0.1202\n",
      "195/463, train_loss: 0.1199\n",
      "196/463, train_loss: 0.1348\n",
      "197/463, train_loss: 0.0548\n",
      "198/463, train_loss: 0.0620\n",
      "199/463, train_loss: 0.0815\n",
      "200/463, train_loss: 0.5381\n",
      "201/463, train_loss: 0.0964\n",
      "202/463, train_loss: 0.4016\n",
      "203/463, train_loss: 0.4014\n",
      "204/463, train_loss: 0.0964\n",
      "205/463, train_loss: 0.0969\n",
      "206/463, train_loss: 0.1733\n",
      "207/463, train_loss: 0.2510\n",
      "208/463, train_loss: 0.1360\n",
      "209/463, train_loss: 0.1031\n",
      "210/463, train_loss: 0.1516\n",
      "211/463, train_loss: 0.6367\n",
      "212/463, train_loss: 0.1401\n",
      "213/463, train_loss: 0.5522\n",
      "214/463, train_loss: 0.0199\n",
      "215/463, train_loss: 0.2258\n",
      "216/463, train_loss: 0.1606\n",
      "217/463, train_loss: 0.0754\n",
      "218/463, train_loss: 0.1154\n",
      "219/463, train_loss: 0.1282\n",
      "220/463, train_loss: 0.0981\n",
      "221/463, train_loss: 0.0939\n",
      "222/463, train_loss: 0.1958\n",
      "223/463, train_loss: 0.2810\n",
      "224/463, train_loss: 0.1315\n",
      "225/463, train_loss: 0.3679\n",
      "226/463, train_loss: 0.3965\n",
      "227/463, train_loss: 0.2600\n",
      "228/463, train_loss: 0.0815\n",
      "229/463, train_loss: 0.0756\n",
      "230/463, train_loss: 0.0667\n",
      "231/463, train_loss: 0.4280\n",
      "232/463, train_loss: 0.1506\n",
      "233/463, train_loss: 0.1406\n",
      "234/463, train_loss: 0.1012\n",
      "235/463, train_loss: 0.1519\n",
      "236/463, train_loss: 0.4893\n",
      "237/463, train_loss: 0.3560\n",
      "238/463, train_loss: 0.1515\n",
      "239/463, train_loss: 0.1240\n",
      "240/463, train_loss: 0.1903\n",
      "241/463, train_loss: 0.1266\n",
      "242/463, train_loss: 0.3091\n",
      "243/463, train_loss: 0.0757\n",
      "244/463, train_loss: 0.4529\n",
      "245/463, train_loss: 0.0912\n",
      "246/463, train_loss: 0.1541\n",
      "247/463, train_loss: 0.1777\n",
      "248/463, train_loss: 0.1431\n",
      "249/463, train_loss: 0.0878\n",
      "250/463, train_loss: 0.4360\n",
      "251/463, train_loss: 0.2181\n",
      "252/463, train_loss: 0.3313\n",
      "253/463, train_loss: 0.1298\n",
      "254/463, train_loss: 0.5420\n",
      "255/463, train_loss: 0.0920\n",
      "256/463, train_loss: 0.0789\n",
      "257/463, train_loss: 0.0455\n",
      "258/463, train_loss: 0.3936\n",
      "259/463, train_loss: 0.1483\n",
      "260/463, train_loss: 0.0778\n",
      "261/463, train_loss: 0.1565\n",
      "262/463, train_loss: 0.1494\n",
      "263/463, train_loss: 0.0859\n",
      "264/463, train_loss: 0.1248\n",
      "265/463, train_loss: 0.1145\n",
      "266/463, train_loss: 0.1437\n",
      "267/463, train_loss: 0.1953\n",
      "268/463, train_loss: 0.0629\n",
      "269/463, train_loss: 0.0544\n",
      "270/463, train_loss: 0.1744\n",
      "271/463, train_loss: 0.0765\n",
      "272/463, train_loss: 0.2043\n",
      "273/463, train_loss: 0.1326\n",
      "274/463, train_loss: 0.0646\n",
      "275/463, train_loss: 0.1927\n",
      "276/463, train_loss: 0.1046\n",
      "277/463, train_loss: 0.1897\n",
      "278/463, train_loss: 0.1534\n",
      "279/463, train_loss: 0.1058\n",
      "280/463, train_loss: 0.2690\n",
      "281/463, train_loss: 0.0753\n",
      "282/463, train_loss: 0.5767\n",
      "283/463, train_loss: 0.0488\n",
      "284/463, train_loss: 0.1013\n",
      "285/463, train_loss: 0.2158\n",
      "286/463, train_loss: 0.2417\n",
      "287/463, train_loss: 0.1604\n",
      "288/463, train_loss: 0.1536\n",
      "289/463, train_loss: 0.0558\n",
      "290/463, train_loss: 0.1779\n",
      "291/463, train_loss: 0.3696\n",
      "292/463, train_loss: 0.0692\n",
      "293/463, train_loss: 0.1824\n",
      "294/463, train_loss: 0.0594\n",
      "295/463, train_loss: 0.1714\n",
      "296/463, train_loss: 0.2004\n",
      "297/463, train_loss: 0.2649\n",
      "298/463, train_loss: 0.0757\n",
      "299/463, train_loss: 0.0915\n",
      "300/463, train_loss: 0.1384\n",
      "301/463, train_loss: 0.1715\n",
      "302/463, train_loss: 0.1160\n",
      "303/463, train_loss: 0.1438\n",
      "304/463, train_loss: 0.0682\n",
      "305/463, train_loss: 0.1113\n",
      "306/463, train_loss: 0.0869\n",
      "307/463, train_loss: 0.1763\n",
      "308/463, train_loss: 0.0759\n",
      "309/463, train_loss: 0.1425\n",
      "310/463, train_loss: 0.1689\n",
      "311/463, train_loss: 0.1267\n",
      "312/463, train_loss: 0.0990\n",
      "313/463, train_loss: 0.1187\n",
      "314/463, train_loss: 0.1548\n",
      "315/463, train_loss: 0.1560\n",
      "316/463, train_loss: 0.3066\n",
      "317/463, train_loss: 0.3857\n",
      "318/463, train_loss: 0.3643\n",
      "319/463, train_loss: 0.1311\n",
      "320/463, train_loss: 0.1086\n",
      "321/463, train_loss: 0.1638\n",
      "322/463, train_loss: 0.2827\n",
      "323/463, train_loss: 0.0640\n",
      "324/463, train_loss: 0.1107\n",
      "325/463, train_loss: 0.0948\n",
      "326/463, train_loss: 0.1196\n",
      "327/463, train_loss: 0.0809\n",
      "328/463, train_loss: 0.3137\n",
      "329/463, train_loss: 0.1580\n",
      "330/463, train_loss: 0.2292\n",
      "331/463, train_loss: 0.0820\n",
      "332/463, train_loss: 0.2104\n",
      "333/463, train_loss: 0.0707\n",
      "334/463, train_loss: 0.0361\n",
      "335/463, train_loss: 0.0751\n",
      "336/463, train_loss: 0.0632\n",
      "337/463, train_loss: 0.1054\n",
      "338/463, train_loss: 0.2081\n",
      "339/463, train_loss: 0.1124\n",
      "340/463, train_loss: 0.1399\n",
      "341/463, train_loss: 0.1801\n",
      "342/463, train_loss: 0.4736\n",
      "343/463, train_loss: 0.2086\n",
      "344/463, train_loss: 0.4561\n",
      "345/463, train_loss: 0.1098\n",
      "346/463, train_loss: 0.2279\n",
      "347/463, train_loss: 0.1091\n",
      "348/463, train_loss: 0.0670\n",
      "349/463, train_loss: 0.3645\n",
      "350/463, train_loss: 0.2200\n",
      "351/463, train_loss: 0.0562\n",
      "352/463, train_loss: 0.0554\n",
      "353/463, train_loss: 0.0792\n",
      "354/463, train_loss: 0.3762\n",
      "355/463, train_loss: 0.2385\n",
      "356/463, train_loss: 0.0871\n",
      "357/463, train_loss: 0.5210\n",
      "358/463, train_loss: 0.1309\n",
      "359/463, train_loss: 0.1858\n",
      "360/463, train_loss: 0.1268\n",
      "361/463, train_loss: 0.1672\n",
      "362/463, train_loss: 0.3145\n",
      "363/463, train_loss: 0.0565\n",
      "364/463, train_loss: 0.0707\n",
      "365/463, train_loss: 0.2419\n",
      "366/463, train_loss: 0.0981\n",
      "367/463, train_loss: 0.1047\n",
      "368/463, train_loss: 0.3638\n",
      "369/463, train_loss: 0.0756\n",
      "370/463, train_loss: 0.2068\n",
      "371/463, train_loss: 0.1394\n",
      "372/463, train_loss: 0.0469\n",
      "373/463, train_loss: 0.1082\n",
      "374/463, train_loss: 0.1315\n",
      "375/463, train_loss: 0.2255\n",
      "376/463, train_loss: 0.2856\n",
      "377/463, train_loss: 0.4629\n",
      "378/463, train_loss: 0.1780\n",
      "379/463, train_loss: 0.1074\n",
      "380/463, train_loss: 0.1931\n",
      "381/463, train_loss: 0.1558\n",
      "382/463, train_loss: 0.3765\n",
      "383/463, train_loss: 0.2859\n",
      "384/463, train_loss: 0.2479\n",
      "385/463, train_loss: 0.2942\n",
      "386/463, train_loss: 0.0788\n",
      "387/463, train_loss: 0.2429\n",
      "388/463, train_loss: 0.5645\n",
      "389/463, train_loss: 0.1124\n",
      "390/463, train_loss: 0.0817\n",
      "391/463, train_loss: 0.1450\n",
      "392/463, train_loss: 0.0693\n",
      "393/463, train_loss: 0.3774\n",
      "394/463, train_loss: 0.2004\n",
      "395/463, train_loss: 0.0772\n",
      "396/463, train_loss: 0.9331\n",
      "397/463, train_loss: 0.4014\n",
      "398/463, train_loss: 0.0681\n",
      "399/463, train_loss: 0.3787\n",
      "400/463, train_loss: 0.1489\n",
      "401/463, train_loss: 0.1504\n",
      "402/463, train_loss: 0.0962\n",
      "403/463, train_loss: 0.1046\n",
      "404/463, train_loss: 0.2725\n",
      "405/463, train_loss: 0.1670\n",
      "406/463, train_loss: 0.1792\n",
      "407/463, train_loss: 0.3945\n",
      "408/463, train_loss: 0.1741\n",
      "409/463, train_loss: 0.0760\n",
      "410/463, train_loss: 0.1418\n",
      "411/463, train_loss: 0.2515\n",
      "412/463, train_loss: 0.3325\n",
      "413/463, train_loss: 0.1290\n",
      "414/463, train_loss: 0.1076\n",
      "415/463, train_loss: 0.1209\n",
      "416/463, train_loss: 0.1296\n",
      "417/463, train_loss: 0.2998\n",
      "418/463, train_loss: 0.0775\n",
      "419/463, train_loss: 0.1946\n",
      "420/463, train_loss: 0.0822\n",
      "421/463, train_loss: 0.1459\n",
      "422/463, train_loss: 0.2109\n",
      "423/463, train_loss: 0.1141\n",
      "424/463, train_loss: 0.0712\n",
      "425/463, train_loss: 0.1016\n",
      "426/463, train_loss: 0.0781\n",
      "427/463, train_loss: 0.4683\n",
      "428/463, train_loss: 0.1649\n",
      "429/463, train_loss: 0.1826\n",
      "430/463, train_loss: 0.0840\n",
      "431/463, train_loss: 0.4004\n",
      "432/463, train_loss: 0.1710\n",
      "433/463, train_loss: 0.0929\n",
      "434/463, train_loss: 0.3823\n",
      "435/463, train_loss: 0.1492\n",
      "436/463, train_loss: 0.1289\n",
      "437/463, train_loss: 0.0966\n",
      "438/463, train_loss: 0.2646\n",
      "439/463, train_loss: 0.0798\n",
      "440/463, train_loss: 0.1599\n",
      "441/463, train_loss: 0.3049\n",
      "442/463, train_loss: 0.1438\n",
      "443/463, train_loss: 0.1096\n",
      "444/463, train_loss: 0.2356\n",
      "445/463, train_loss: 0.2522\n",
      "446/463, train_loss: 0.0818\n",
      "447/463, train_loss: 0.4341\n",
      "448/463, train_loss: 0.4224\n",
      "449/463, train_loss: 0.1741\n",
      "450/463, train_loss: 0.2058\n",
      "451/463, train_loss: 0.1737\n",
      "452/463, train_loss: 0.3357\n",
      "453/463, train_loss: 0.1165\n",
      "454/463, train_loss: 0.1236\n",
      "455/463, train_loss: 0.1646\n",
      "456/463, train_loss: 0.2771\n",
      "457/463, train_loss: 0.0898\n",
      "458/463, train_loss: 0.3391\n",
      "459/463, train_loss: 0.0981\n",
      "460/463, train_loss: 0.0775\n",
      "461/463, train_loss: 0.2010\n",
      "462/463, train_loss: 0.2469\n",
      "463/463, train_loss: 0.1357\n",
      "epoch 26 average loss: 0.1897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 15:49:06 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 15:49:09 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 15:49:11 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 27/100\n",
      "1/463, train_loss: 0.2021\n",
      "2/463, train_loss: 0.1536\n",
      "3/463, train_loss: 0.0936\n",
      "4/463, train_loss: 0.3728\n",
      "5/463, train_loss: 0.1050\n",
      "6/463, train_loss: 0.1174\n",
      "7/463, train_loss: 0.5479\n",
      "8/463, train_loss: 0.1288\n",
      "9/463, train_loss: 0.1311\n",
      "10/463, train_loss: 0.1139\n",
      "11/463, train_loss: 0.3145\n",
      "12/463, train_loss: 0.2310\n",
      "13/463, train_loss: 0.1230\n",
      "14/463, train_loss: 0.5610\n",
      "15/463, train_loss: 0.3250\n",
      "16/463, train_loss: 0.1504\n",
      "17/463, train_loss: 0.1279\n",
      "18/463, train_loss: 0.0731\n",
      "19/463, train_loss: 0.2095\n",
      "20/463, train_loss: 0.0886\n",
      "21/463, train_loss: 0.3010\n",
      "22/463, train_loss: 0.1207\n",
      "23/463, train_loss: 0.6177\n",
      "24/463, train_loss: 0.2803\n",
      "25/463, train_loss: 0.1124\n",
      "26/463, train_loss: 0.1239\n",
      "27/463, train_loss: 0.0736\n",
      "28/463, train_loss: 0.1493\n",
      "29/463, train_loss: 0.1713\n",
      "30/463, train_loss: 0.0649\n",
      "31/463, train_loss: 0.1685\n",
      "32/463, train_loss: 0.1558\n",
      "33/463, train_loss: 0.1138\n",
      "34/463, train_loss: 0.1803\n",
      "35/463, train_loss: 0.0819\n",
      "36/463, train_loss: 0.1580\n",
      "37/463, train_loss: 0.1564\n",
      "38/463, train_loss: 0.1195\n",
      "39/463, train_loss: 0.4155\n",
      "40/463, train_loss: 0.6074\n",
      "41/463, train_loss: 0.2852\n",
      "42/463, train_loss: 0.3057\n",
      "43/463, train_loss: 0.1877\n",
      "44/463, train_loss: 0.3337\n",
      "45/463, train_loss: 0.0629\n",
      "46/463, train_loss: 0.2600\n",
      "47/463, train_loss: 0.0829\n",
      "48/463, train_loss: 0.1559\n",
      "49/463, train_loss: 0.1384\n",
      "50/463, train_loss: 0.0665\n",
      "51/463, train_loss: 0.0545\n",
      "52/463, train_loss: 0.0688\n",
      "53/463, train_loss: 0.3979\n",
      "54/463, train_loss: 0.1294\n",
      "55/463, train_loss: 0.1124\n",
      "56/463, train_loss: 0.1421\n",
      "57/463, train_loss: 0.4243\n",
      "58/463, train_loss: 0.0773\n",
      "59/463, train_loss: 0.1005\n",
      "60/463, train_loss: 0.4971\n",
      "61/463, train_loss: 0.3704\n",
      "62/463, train_loss: 0.1021\n",
      "63/463, train_loss: 0.2554\n",
      "64/463, train_loss: 0.1420\n",
      "65/463, train_loss: 0.0905\n",
      "66/463, train_loss: 0.0831\n",
      "67/463, train_loss: 0.2805\n",
      "68/463, train_loss: 0.3184\n",
      "69/463, train_loss: 0.2476\n",
      "70/463, train_loss: 0.1196\n",
      "71/463, train_loss: 0.1127\n",
      "72/463, train_loss: 0.0938\n",
      "73/463, train_loss: 0.1172\n",
      "74/463, train_loss: 0.1229\n",
      "75/463, train_loss: 0.2150\n",
      "76/463, train_loss: 0.0414\n",
      "77/463, train_loss: 0.3516\n",
      "78/463, train_loss: 0.1973\n",
      "79/463, train_loss: 0.1388\n",
      "80/463, train_loss: 0.0950\n",
      "81/463, train_loss: 0.5151\n",
      "82/463, train_loss: 0.4885\n",
      "83/463, train_loss: 0.2083\n",
      "84/463, train_loss: 0.0919\n",
      "85/463, train_loss: 0.1302\n",
      "86/463, train_loss: 0.1157\n",
      "87/463, train_loss: 0.1768\n",
      "88/463, train_loss: 0.1560\n",
      "89/463, train_loss: 0.1646\n",
      "90/463, train_loss: 0.2646\n",
      "91/463, train_loss: 0.1285\n",
      "92/463, train_loss: 0.0845\n",
      "93/463, train_loss: 0.2429\n",
      "94/463, train_loss: 0.0613\n",
      "95/463, train_loss: 0.1453\n",
      "96/463, train_loss: 0.1273\n",
      "97/463, train_loss: 0.1199\n",
      "98/463, train_loss: 0.2900\n",
      "99/463, train_loss: 0.1296\n",
      "100/463, train_loss: 0.1467\n",
      "101/463, train_loss: 0.1471\n",
      "102/463, train_loss: 0.5332\n",
      "103/463, train_loss: 0.4316\n",
      "104/463, train_loss: 0.1423\n",
      "105/463, train_loss: 0.3530\n",
      "106/463, train_loss: 0.1912\n",
      "107/463, train_loss: 0.2351\n",
      "108/463, train_loss: 0.0723\n",
      "109/463, train_loss: 0.4033\n",
      "110/463, train_loss: 0.1847\n",
      "111/463, train_loss: 0.1414\n",
      "112/463, train_loss: 0.1002\n",
      "113/463, train_loss: 0.2039\n",
      "114/463, train_loss: 0.1328\n",
      "115/463, train_loss: 0.3403\n",
      "116/463, train_loss: 0.2749\n",
      "117/463, train_loss: 0.1173\n",
      "118/463, train_loss: 0.7910\n",
      "119/463, train_loss: 0.5068\n",
      "120/463, train_loss: 0.1111\n",
      "121/463, train_loss: 0.2349\n",
      "122/463, train_loss: 0.0817\n",
      "123/463, train_loss: 0.5439\n",
      "124/463, train_loss: 0.0918\n",
      "125/463, train_loss: 0.2881\n",
      "126/463, train_loss: 0.2061\n",
      "127/463, train_loss: 0.2252\n",
      "128/463, train_loss: 0.1362\n",
      "129/463, train_loss: 0.1245\n",
      "130/463, train_loss: 0.1487\n",
      "131/463, train_loss: 0.3337\n",
      "132/463, train_loss: 0.2119\n",
      "133/463, train_loss: 0.1377\n",
      "134/463, train_loss: 0.1063\n",
      "135/463, train_loss: 0.1265\n",
      "136/463, train_loss: 0.0668\n",
      "137/463, train_loss: 0.2159\n",
      "138/463, train_loss: 0.4399\n",
      "139/463, train_loss: 0.0822\n",
      "140/463, train_loss: 0.3794\n",
      "141/463, train_loss: 0.0729\n",
      "142/463, train_loss: 0.0515\n",
      "143/463, train_loss: 0.0679\n",
      "144/463, train_loss: 0.1229\n",
      "145/463, train_loss: 0.2961\n",
      "146/463, train_loss: 0.4663\n",
      "147/463, train_loss: 0.0497\n",
      "148/463, train_loss: 0.2332\n",
      "149/463, train_loss: 0.1450\n",
      "150/463, train_loss: 0.0859\n",
      "151/463, train_loss: 0.4751\n",
      "152/463, train_loss: 0.2487\n",
      "153/463, train_loss: 0.9746\n",
      "154/463, train_loss: 0.1842\n",
      "155/463, train_loss: 0.1519\n",
      "156/463, train_loss: 0.0635\n",
      "157/463, train_loss: 0.2778\n",
      "158/463, train_loss: 0.3127\n",
      "159/463, train_loss: 0.3010\n",
      "160/463, train_loss: 0.1531\n",
      "161/463, train_loss: 0.1310\n",
      "162/463, train_loss: 0.2236\n",
      "163/463, train_loss: 0.2720\n",
      "164/463, train_loss: 0.1212\n",
      "165/463, train_loss: 0.1431\n",
      "166/463, train_loss: 0.1525\n",
      "167/463, train_loss: 0.2363\n",
      "168/463, train_loss: 0.1138\n",
      "169/463, train_loss: 0.0399\n",
      "170/463, train_loss: 0.0945\n",
      "171/463, train_loss: 0.0641\n",
      "172/463, train_loss: 0.3596\n",
      "173/463, train_loss: 0.1028\n",
      "174/463, train_loss: 0.1385\n",
      "175/463, train_loss: 0.1316\n",
      "176/463, train_loss: 0.0798\n",
      "177/463, train_loss: 0.5796\n",
      "178/463, train_loss: 0.0745\n",
      "179/463, train_loss: 0.1149\n",
      "180/463, train_loss: 0.3013\n",
      "181/463, train_loss: 0.1665\n",
      "182/463, train_loss: 0.1538\n",
      "183/463, train_loss: 0.1080\n",
      "184/463, train_loss: 0.2173\n",
      "185/463, train_loss: 0.2939\n",
      "186/463, train_loss: 0.2822\n",
      "187/463, train_loss: 0.0559\n",
      "188/463, train_loss: 0.1127\n",
      "189/463, train_loss: 0.0945\n",
      "190/463, train_loss: 0.0941\n",
      "191/463, train_loss: 0.1514\n",
      "192/463, train_loss: 0.1440\n",
      "193/463, train_loss: 0.1265\n",
      "194/463, train_loss: 0.1536\n",
      "195/463, train_loss: 0.2815\n",
      "196/463, train_loss: 0.1730\n",
      "197/463, train_loss: 0.0790\n",
      "198/463, train_loss: 0.3025\n",
      "199/463, train_loss: 0.1240\n",
      "200/463, train_loss: 0.2367\n",
      "201/463, train_loss: 0.1002\n",
      "202/463, train_loss: 0.1357\n",
      "203/463, train_loss: 0.2301\n",
      "204/463, train_loss: 0.1058\n",
      "205/463, train_loss: 0.1418\n",
      "206/463, train_loss: 0.0598\n",
      "207/463, train_loss: 0.0210\n",
      "208/463, train_loss: 0.3096\n",
      "209/463, train_loss: 0.0403\n",
      "210/463, train_loss: 0.0798\n",
      "211/463, train_loss: 0.3047\n",
      "212/463, train_loss: 0.7417\n",
      "213/463, train_loss: 0.1566\n",
      "214/463, train_loss: 0.0378\n",
      "215/463, train_loss: 0.0517\n",
      "216/463, train_loss: 0.1831\n",
      "217/463, train_loss: 0.1802\n",
      "218/463, train_loss: 0.1727\n",
      "219/463, train_loss: 0.1195\n",
      "220/463, train_loss: 0.1222\n",
      "221/463, train_loss: 0.0462\n",
      "222/463, train_loss: 0.1344\n",
      "223/463, train_loss: 0.1255\n",
      "224/463, train_loss: 0.1407\n",
      "225/463, train_loss: 0.1610\n",
      "226/463, train_loss: 0.0940\n",
      "227/463, train_loss: 0.0878\n",
      "228/463, train_loss: 0.2859\n",
      "229/463, train_loss: 0.1041\n",
      "230/463, train_loss: 0.2078\n",
      "231/463, train_loss: 0.5688\n",
      "232/463, train_loss: 0.2144\n",
      "233/463, train_loss: 0.0814\n",
      "234/463, train_loss: 0.1002\n",
      "235/463, train_loss: 0.0607\n",
      "236/463, train_loss: 0.0869\n",
      "237/463, train_loss: 0.2019\n",
      "238/463, train_loss: 0.0891\n",
      "239/463, train_loss: 0.1035\n",
      "240/463, train_loss: 0.1528\n",
      "241/463, train_loss: 0.1798\n",
      "242/463, train_loss: 0.1035\n",
      "243/463, train_loss: 0.1024\n",
      "244/463, train_loss: 0.2327\n",
      "245/463, train_loss: 0.1035\n",
      "246/463, train_loss: 0.2451\n",
      "247/463, train_loss: 0.2202\n",
      "248/463, train_loss: 0.0957\n",
      "249/463, train_loss: 0.4995\n",
      "250/463, train_loss: 0.0837\n",
      "251/463, train_loss: 0.2192\n",
      "252/463, train_loss: 0.3032\n",
      "253/463, train_loss: 0.1191\n",
      "254/463, train_loss: 0.1190\n",
      "255/463, train_loss: 0.0817\n",
      "256/463, train_loss: 0.1587\n",
      "257/463, train_loss: 0.4771\n",
      "258/463, train_loss: 0.2032\n",
      "259/463, train_loss: 0.1633\n",
      "260/463, train_loss: 0.1509\n",
      "261/463, train_loss: 0.3567\n",
      "262/463, train_loss: 0.0359\n",
      "263/463, train_loss: 0.0909\n",
      "264/463, train_loss: 0.2617\n",
      "265/463, train_loss: 0.1509\n",
      "266/463, train_loss: 0.1476\n",
      "267/463, train_loss: 0.0963\n",
      "268/463, train_loss: 0.0619\n",
      "269/463, train_loss: 0.2769\n",
      "270/463, train_loss: 0.1438\n",
      "271/463, train_loss: 0.1371\n",
      "272/463, train_loss: 0.1067\n",
      "273/463, train_loss: 0.0925\n",
      "274/463, train_loss: 0.2007\n",
      "275/463, train_loss: 0.1920\n",
      "276/463, train_loss: 0.1017\n",
      "277/463, train_loss: 0.1791\n",
      "278/463, train_loss: 0.3059\n",
      "279/463, train_loss: 0.0933\n",
      "280/463, train_loss: 0.1049\n",
      "281/463, train_loss: 0.1066\n",
      "282/463, train_loss: 0.0847\n",
      "283/463, train_loss: 0.0983\n",
      "284/463, train_loss: 0.1317\n",
      "285/463, train_loss: 0.1034\n",
      "286/463, train_loss: 0.0807\n",
      "287/463, train_loss: 0.0699\n",
      "288/463, train_loss: 0.2290\n",
      "289/463, train_loss: 0.1605\n",
      "290/463, train_loss: 0.1042\n",
      "291/463, train_loss: 0.1428\n",
      "292/463, train_loss: 0.4978\n",
      "293/463, train_loss: 0.2913\n",
      "294/463, train_loss: 0.3679\n",
      "295/463, train_loss: 0.0943\n",
      "296/463, train_loss: 0.2883\n",
      "297/463, train_loss: 0.3638\n",
      "298/463, train_loss: 0.0629\n",
      "299/463, train_loss: 0.0936\n",
      "300/463, train_loss: 0.7056\n",
      "301/463, train_loss: 0.1968\n",
      "302/463, train_loss: 0.2671\n",
      "303/463, train_loss: 0.0870\n",
      "304/463, train_loss: 0.0815\n",
      "305/463, train_loss: 0.2446\n",
      "306/463, train_loss: 0.0884\n",
      "307/463, train_loss: 0.0784\n",
      "308/463, train_loss: 0.3428\n",
      "309/463, train_loss: 0.1832\n",
      "310/463, train_loss: 0.1968\n",
      "311/463, train_loss: 0.0789\n",
      "312/463, train_loss: 0.1412\n",
      "313/463, train_loss: 0.3423\n",
      "314/463, train_loss: 0.1821\n",
      "315/463, train_loss: 0.1868\n",
      "316/463, train_loss: 0.0375\n",
      "317/463, train_loss: 0.1046\n",
      "318/463, train_loss: 0.2744\n",
      "319/463, train_loss: 0.0590\n",
      "320/463, train_loss: 0.0919\n",
      "321/463, train_loss: 0.1031\n",
      "322/463, train_loss: 0.1067\n",
      "323/463, train_loss: 0.0687\n",
      "324/463, train_loss: 0.1450\n",
      "325/463, train_loss: 0.0737\n",
      "326/463, train_loss: 0.1561\n",
      "327/463, train_loss: 0.1627\n",
      "328/463, train_loss: 0.1641\n",
      "329/463, train_loss: 0.3352\n",
      "330/463, train_loss: 0.0556\n",
      "331/463, train_loss: 0.4321\n",
      "332/463, train_loss: 0.1222\n",
      "333/463, train_loss: 0.1775\n",
      "334/463, train_loss: 0.0513\n",
      "335/463, train_loss: 0.0537\n",
      "336/463, train_loss: 0.7217\n",
      "337/463, train_loss: 0.1484\n",
      "338/463, train_loss: 0.9819\n",
      "339/463, train_loss: 0.0841\n",
      "340/463, train_loss: 0.0659\n",
      "341/463, train_loss: 0.4468\n",
      "342/463, train_loss: 0.2268\n",
      "343/463, train_loss: 0.1398\n",
      "344/463, train_loss: 0.1410\n",
      "345/463, train_loss: 0.0761\n",
      "346/463, train_loss: 0.1792\n",
      "347/463, train_loss: 0.2061\n",
      "348/463, train_loss: 0.1187\n",
      "349/463, train_loss: 0.1624\n",
      "350/463, train_loss: 0.0825\n",
      "351/463, train_loss: 0.2974\n",
      "352/463, train_loss: 0.1499\n",
      "353/463, train_loss: 0.0927\n",
      "354/463, train_loss: 0.3005\n",
      "355/463, train_loss: 0.0684\n",
      "356/463, train_loss: 0.1436\n",
      "357/463, train_loss: 0.2307\n",
      "358/463, train_loss: 0.1498\n",
      "359/463, train_loss: 0.0907\n",
      "360/463, train_loss: 0.2003\n",
      "361/463, train_loss: 0.0902\n",
      "362/463, train_loss: 0.2490\n",
      "363/463, train_loss: 0.1074\n",
      "364/463, train_loss: 0.1282\n",
      "365/463, train_loss: 0.1383\n",
      "366/463, train_loss: 0.1710\n",
      "367/463, train_loss: 0.2720\n",
      "368/463, train_loss: 0.6289\n",
      "369/463, train_loss: 0.1713\n",
      "370/463, train_loss: 0.1124\n",
      "371/463, train_loss: 0.0813\n",
      "372/463, train_loss: 0.2269\n",
      "373/463, train_loss: 0.1385\n",
      "374/463, train_loss: 0.0770\n",
      "375/463, train_loss: 0.0996\n",
      "376/463, train_loss: 0.3540\n",
      "377/463, train_loss: 0.1250\n",
      "378/463, train_loss: 0.1008\n",
      "379/463, train_loss: 0.3416\n",
      "380/463, train_loss: 0.1061\n",
      "381/463, train_loss: 0.2000\n",
      "382/463, train_loss: 0.1102\n",
      "383/463, train_loss: 0.3499\n",
      "384/463, train_loss: 0.3574\n",
      "385/463, train_loss: 0.0959\n",
      "386/463, train_loss: 0.1221\n",
      "387/463, train_loss: 0.0692\n",
      "388/463, train_loss: 0.1193\n",
      "389/463, train_loss: 0.1517\n",
      "390/463, train_loss: 0.4065\n",
      "391/463, train_loss: 0.4780\n",
      "392/463, train_loss: 0.1213\n",
      "393/463, train_loss: 0.0795\n",
      "394/463, train_loss: 0.1725\n",
      "395/463, train_loss: 0.0886\n",
      "396/463, train_loss: 0.1731\n",
      "397/463, train_loss: 0.2410\n",
      "398/463, train_loss: 0.1262\n",
      "399/463, train_loss: 0.0832\n",
      "400/463, train_loss: 0.0645\n",
      "401/463, train_loss: 0.1592\n",
      "402/463, train_loss: 0.1162\n",
      "403/463, train_loss: 0.2678\n",
      "404/463, train_loss: 0.0675\n",
      "405/463, train_loss: 0.1042\n",
      "406/463, train_loss: 0.0626\n",
      "407/463, train_loss: 0.2443\n",
      "408/463, train_loss: 0.2153\n",
      "409/463, train_loss: 0.1560\n",
      "410/463, train_loss: 0.3320\n",
      "411/463, train_loss: 0.0666\n",
      "412/463, train_loss: 0.2474\n",
      "413/463, train_loss: 0.0875\n",
      "414/463, train_loss: 0.0942\n",
      "415/463, train_loss: 0.0429\n",
      "416/463, train_loss: 0.1111\n",
      "417/463, train_loss: 0.1510\n",
      "418/463, train_loss: 0.7764\n",
      "419/463, train_loss: 0.1112\n",
      "420/463, train_loss: 0.0981\n",
      "421/463, train_loss: 0.0516\n",
      "422/463, train_loss: 0.1958\n",
      "423/463, train_loss: 0.1326\n",
      "424/463, train_loss: 0.1187\n",
      "425/463, train_loss: 0.0309\n",
      "426/463, train_loss: 0.4941\n",
      "427/463, train_loss: 0.1727\n",
      "428/463, train_loss: 0.2019\n",
      "429/463, train_loss: 0.7744\n",
      "430/463, train_loss: 0.1404\n",
      "431/463, train_loss: 0.1586\n",
      "432/463, train_loss: 0.0846\n",
      "433/463, train_loss: 0.1606\n",
      "434/463, train_loss: 0.1345\n",
      "435/463, train_loss: 0.1287\n",
      "436/463, train_loss: 0.0544\n",
      "437/463, train_loss: 0.0815\n",
      "438/463, train_loss: 0.2006\n",
      "439/463, train_loss: 0.1063\n",
      "440/463, train_loss: 0.4497\n",
      "441/463, train_loss: 0.0566\n",
      "442/463, train_loss: 0.0856\n",
      "443/463, train_loss: 0.0865\n",
      "444/463, train_loss: 0.0367\n",
      "445/463, train_loss: 0.0826\n",
      "446/463, train_loss: 0.5181\n",
      "447/463, train_loss: 0.2974\n",
      "448/463, train_loss: 0.1158\n",
      "449/463, train_loss: 0.4668\n",
      "450/463, train_loss: 0.4243\n",
      "451/463, train_loss: 0.2305\n",
      "452/463, train_loss: 0.2039\n",
      "453/463, train_loss: 0.2981\n",
      "454/463, train_loss: 0.1053\n",
      "455/463, train_loss: 0.1168\n",
      "456/463, train_loss: 0.0632\n",
      "457/463, train_loss: 0.1106\n",
      "458/463, train_loss: 0.3308\n",
      "459/463, train_loss: 0.0519\n",
      "460/463, train_loss: 0.1613\n",
      "461/463, train_loss: 0.0943\n",
      "462/463, train_loss: 0.3052\n",
      "463/463, train_loss: 0.1287\n",
      "epoch 27 average loss: 0.1904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 18:02:44 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 18:02:47 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 18:02:50 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 28/100\n",
      "1/463, train_loss: 0.2030\n",
      "2/463, train_loss: 0.0681\n",
      "3/463, train_loss: 0.1798\n",
      "4/463, train_loss: 0.1711\n",
      "5/463, train_loss: 0.2037\n",
      "6/463, train_loss: 0.1785\n",
      "7/463, train_loss: 0.1301\n",
      "8/463, train_loss: 0.4189\n",
      "9/463, train_loss: 0.0996\n",
      "10/463, train_loss: 0.1023\n",
      "11/463, train_loss: 0.1810\n",
      "12/463, train_loss: 0.1569\n",
      "13/463, train_loss: 0.1037\n",
      "14/463, train_loss: 0.0754\n",
      "15/463, train_loss: 0.1560\n",
      "16/463, train_loss: 0.1541\n",
      "17/463, train_loss: 0.0762\n",
      "18/463, train_loss: 0.1997\n",
      "19/463, train_loss: 0.0890\n",
      "20/463, train_loss: 0.0693\n",
      "21/463, train_loss: 0.1133\n",
      "22/463, train_loss: 0.1755\n",
      "23/463, train_loss: 0.2712\n",
      "24/463, train_loss: 0.1124\n",
      "25/463, train_loss: 0.2129\n",
      "26/463, train_loss: 0.0936\n",
      "27/463, train_loss: 0.1340\n",
      "28/463, train_loss: 0.1158\n",
      "29/463, train_loss: 0.1394\n",
      "30/463, train_loss: 0.1981\n",
      "31/463, train_loss: 0.4507\n",
      "32/463, train_loss: 0.1566\n",
      "33/463, train_loss: 0.1614\n",
      "34/463, train_loss: 0.2004\n",
      "35/463, train_loss: 0.1687\n",
      "36/463, train_loss: 0.1925\n",
      "37/463, train_loss: 0.6450\n",
      "38/463, train_loss: 0.3042\n",
      "39/463, train_loss: 0.2334\n",
      "40/463, train_loss: 0.4541\n",
      "41/463, train_loss: 0.2095\n",
      "42/463, train_loss: 0.0769\n",
      "43/463, train_loss: 0.1111\n",
      "44/463, train_loss: 0.0720\n",
      "45/463, train_loss: 0.3962\n",
      "46/463, train_loss: 0.1222\n",
      "47/463, train_loss: 0.0889\n",
      "48/463, train_loss: 0.2563\n",
      "49/463, train_loss: 0.1362\n",
      "50/463, train_loss: 0.5005\n",
      "51/463, train_loss: 0.5542\n",
      "52/463, train_loss: 0.4346\n",
      "53/463, train_loss: 0.2983\n",
      "54/463, train_loss: 0.1316\n",
      "55/463, train_loss: 0.3276\n",
      "56/463, train_loss: 0.2793\n",
      "57/463, train_loss: 0.0399\n",
      "58/463, train_loss: 0.1909\n",
      "59/463, train_loss: 0.3113\n",
      "60/463, train_loss: 0.2852\n",
      "61/463, train_loss: 0.1019\n",
      "62/463, train_loss: 0.1030\n",
      "63/463, train_loss: 0.1400\n",
      "64/463, train_loss: 0.2034\n",
      "65/463, train_loss: 0.2346\n",
      "66/463, train_loss: 0.1198\n",
      "67/463, train_loss: 0.1129\n",
      "68/463, train_loss: 0.1553\n",
      "69/463, train_loss: 0.1123\n",
      "70/463, train_loss: 0.1663\n",
      "71/463, train_loss: 0.1766\n",
      "72/463, train_loss: 0.5063\n",
      "73/463, train_loss: 0.2732\n",
      "74/463, train_loss: 0.0864\n",
      "75/463, train_loss: 0.1057\n",
      "76/463, train_loss: 0.3665\n",
      "77/463, train_loss: 0.1472\n",
      "78/463, train_loss: 0.1313\n",
      "79/463, train_loss: 0.0737\n",
      "80/463, train_loss: 0.0863\n",
      "81/463, train_loss: 0.1081\n",
      "82/463, train_loss: 0.0613\n",
      "83/463, train_loss: 0.1456\n",
      "84/463, train_loss: 0.1136\n",
      "85/463, train_loss: 0.0729\n",
      "86/463, train_loss: 0.1385\n",
      "87/463, train_loss: 0.3994\n",
      "88/463, train_loss: 0.1967\n",
      "89/463, train_loss: 0.2225\n",
      "90/463, train_loss: 0.0872\n",
      "91/463, train_loss: 0.2856\n",
      "92/463, train_loss: 0.0986\n",
      "93/463, train_loss: 0.1672\n",
      "94/463, train_loss: 0.6079\n",
      "95/463, train_loss: 0.1113\n",
      "96/463, train_loss: 0.1395\n",
      "97/463, train_loss: 0.1028\n",
      "98/463, train_loss: 0.1644\n",
      "99/463, train_loss: 0.2430\n",
      "100/463, train_loss: 0.1417\n",
      "101/463, train_loss: 0.2180\n",
      "102/463, train_loss: 0.1467\n",
      "103/463, train_loss: 0.1407\n",
      "104/463, train_loss: 0.2615\n",
      "105/463, train_loss: 0.1248\n",
      "106/463, train_loss: 0.2871\n",
      "107/463, train_loss: 0.1003\n",
      "108/463, train_loss: 0.1851\n",
      "109/463, train_loss: 0.0936\n",
      "110/463, train_loss: 0.1471\n",
      "111/463, train_loss: 0.1609\n",
      "112/463, train_loss: 0.2615\n",
      "113/463, train_loss: 0.2139\n",
      "114/463, train_loss: 0.2708\n",
      "115/463, train_loss: 0.2010\n",
      "116/463, train_loss: 0.0685\n",
      "117/463, train_loss: 0.0540\n",
      "118/463, train_loss: 0.1405\n",
      "119/463, train_loss: 0.1245\n",
      "120/463, train_loss: 1.3711\n",
      "121/463, train_loss: 0.1217\n",
      "122/463, train_loss: 0.0642\n",
      "123/463, train_loss: 0.0957\n",
      "124/463, train_loss: 0.0569\n",
      "125/463, train_loss: 0.3567\n",
      "126/463, train_loss: 0.1564\n",
      "127/463, train_loss: 0.0863\n",
      "128/463, train_loss: 0.0572\n",
      "129/463, train_loss: 0.0307\n",
      "130/463, train_loss: 0.0576\n",
      "131/463, train_loss: 0.1150\n",
      "132/463, train_loss: 0.2979\n",
      "133/463, train_loss: 0.4741\n",
      "134/463, train_loss: 0.1364\n",
      "135/463, train_loss: 0.1205\n",
      "136/463, train_loss: 0.0764\n",
      "137/463, train_loss: 0.1037\n",
      "138/463, train_loss: 0.2178\n",
      "139/463, train_loss: 0.2390\n",
      "140/463, train_loss: 0.2637\n",
      "141/463, train_loss: 0.2435\n",
      "142/463, train_loss: 0.0900\n",
      "143/463, train_loss: 0.1385\n",
      "144/463, train_loss: 0.2051\n",
      "145/463, train_loss: 0.0704\n",
      "146/463, train_loss: 0.2961\n",
      "147/463, train_loss: 0.1047\n",
      "148/463, train_loss: 0.2729\n",
      "149/463, train_loss: 0.1129\n",
      "150/463, train_loss: 0.0568\n",
      "151/463, train_loss: 0.0486\n",
      "152/463, train_loss: 0.0765\n",
      "153/463, train_loss: 0.2871\n",
      "154/463, train_loss: 0.0723\n",
      "155/463, train_loss: 0.0863\n",
      "156/463, train_loss: 0.1008\n",
      "157/463, train_loss: 0.2225\n",
      "158/463, train_loss: 0.0640\n",
      "159/463, train_loss: 0.1570\n",
      "160/463, train_loss: 0.0249\n",
      "161/463, train_loss: 0.1437\n",
      "162/463, train_loss: 0.2520\n",
      "163/463, train_loss: 0.3110\n",
      "164/463, train_loss: 0.4639\n",
      "165/463, train_loss: 0.4636\n",
      "166/463, train_loss: 0.1329\n",
      "167/463, train_loss: 0.3022\n",
      "168/463, train_loss: 0.1249\n",
      "169/463, train_loss: 0.0822\n",
      "170/463, train_loss: 0.1020\n",
      "171/463, train_loss: 0.0529\n",
      "172/463, train_loss: 0.1274\n",
      "173/463, train_loss: 0.1107\n",
      "174/463, train_loss: 0.6738\n",
      "175/463, train_loss: 0.2327\n",
      "176/463, train_loss: 0.7100\n",
      "177/463, train_loss: 0.0685\n",
      "178/463, train_loss: 0.0949\n",
      "179/463, train_loss: 0.1289\n",
      "180/463, train_loss: 0.1691\n",
      "181/463, train_loss: 0.3345\n",
      "182/463, train_loss: 0.0990\n",
      "183/463, train_loss: 0.2395\n",
      "184/463, train_loss: 0.2108\n",
      "185/463, train_loss: 0.1804\n",
      "186/463, train_loss: 0.2236\n",
      "187/463, train_loss: 0.1929\n",
      "188/463, train_loss: 0.2222\n",
      "189/463, train_loss: 0.1045\n",
      "190/463, train_loss: 0.4712\n",
      "191/463, train_loss: 0.0987\n",
      "192/463, train_loss: 0.3911\n",
      "193/463, train_loss: 0.0931\n",
      "194/463, train_loss: 0.1210\n",
      "195/463, train_loss: 0.3467\n",
      "196/463, train_loss: 0.1123\n",
      "197/463, train_loss: 0.0956\n",
      "198/463, train_loss: 0.3757\n",
      "199/463, train_loss: 0.0934\n",
      "200/463, train_loss: 0.0767\n",
      "201/463, train_loss: 0.3401\n",
      "202/463, train_loss: 0.2563\n",
      "203/463, train_loss: 0.1296\n",
      "204/463, train_loss: 0.1027\n",
      "205/463, train_loss: 0.5195\n",
      "206/463, train_loss: 0.2169\n",
      "207/463, train_loss: 0.1315\n",
      "208/463, train_loss: 0.0576\n",
      "209/463, train_loss: 0.0854\n",
      "210/463, train_loss: 0.1295\n",
      "211/463, train_loss: 0.3650\n",
      "212/463, train_loss: 0.0828\n",
      "213/463, train_loss: 0.1687\n",
      "214/463, train_loss: 0.1117\n",
      "215/463, train_loss: 0.1978\n",
      "216/463, train_loss: 0.2180\n",
      "217/463, train_loss: 0.2032\n",
      "218/463, train_loss: 0.0368\n",
      "219/463, train_loss: 0.2172\n",
      "220/463, train_loss: 0.0779\n",
      "221/463, train_loss: 0.1157\n",
      "222/463, train_loss: 0.0265\n",
      "223/463, train_loss: 0.0261\n",
      "224/463, train_loss: 0.2498\n",
      "225/463, train_loss: 0.1876\n",
      "226/463, train_loss: 0.1619\n",
      "227/463, train_loss: 0.0819\n",
      "228/463, train_loss: 0.0955\n",
      "229/463, train_loss: 0.1654\n",
      "230/463, train_loss: 0.1053\n",
      "231/463, train_loss: 0.1606\n",
      "232/463, train_loss: 0.0574\n",
      "233/463, train_loss: 0.3311\n",
      "234/463, train_loss: 0.1132\n",
      "235/463, train_loss: 0.1112\n",
      "236/463, train_loss: 0.1067\n",
      "237/463, train_loss: 0.2057\n",
      "238/463, train_loss: 0.3521\n",
      "239/463, train_loss: 0.1085\n",
      "240/463, train_loss: 0.0804\n",
      "241/463, train_loss: 0.0827\n",
      "242/463, train_loss: 0.1072\n",
      "243/463, train_loss: 0.1172\n",
      "244/463, train_loss: 0.0656\n",
      "245/463, train_loss: 0.0715\n",
      "246/463, train_loss: 0.0500\n",
      "247/463, train_loss: 0.0890\n",
      "248/463, train_loss: 0.1174\n",
      "249/463, train_loss: 0.5576\n",
      "250/463, train_loss: 0.2231\n",
      "251/463, train_loss: 0.0892\n",
      "252/463, train_loss: 0.2651\n",
      "253/463, train_loss: 0.1383\n",
      "254/463, train_loss: 0.5015\n",
      "255/463, train_loss: 0.6147\n",
      "256/463, train_loss: 0.2246\n",
      "257/463, train_loss: 0.1868\n",
      "258/463, train_loss: 0.0960\n",
      "259/463, train_loss: 0.1553\n",
      "260/463, train_loss: 0.0730\n",
      "261/463, train_loss: 0.3672\n",
      "262/463, train_loss: 0.1549\n",
      "263/463, train_loss: 0.0648\n",
      "264/463, train_loss: 0.1785\n",
      "265/463, train_loss: 0.1141\n",
      "266/463, train_loss: 0.0599\n",
      "267/463, train_loss: 0.2148\n",
      "268/463, train_loss: 0.3301\n",
      "269/463, train_loss: 0.1711\n",
      "270/463, train_loss: 0.1211\n",
      "271/463, train_loss: 0.2473\n",
      "272/463, train_loss: 0.2214\n",
      "273/463, train_loss: 0.2218\n",
      "274/463, train_loss: 0.5508\n",
      "275/463, train_loss: 0.0965\n",
      "276/463, train_loss: 0.1918\n",
      "277/463, train_loss: 0.0603\n",
      "278/463, train_loss: 0.2258\n",
      "279/463, train_loss: 0.4053\n",
      "280/463, train_loss: 0.1237\n",
      "281/463, train_loss: 0.0426\n",
      "282/463, train_loss: 0.3352\n",
      "283/463, train_loss: 0.2742\n",
      "284/463, train_loss: 0.0826\n",
      "285/463, train_loss: 0.3420\n",
      "286/463, train_loss: 0.2859\n",
      "287/463, train_loss: 0.5332\n",
      "288/463, train_loss: 0.1592\n",
      "289/463, train_loss: 0.0747\n",
      "290/463, train_loss: 0.1000\n",
      "291/463, train_loss: 0.1353\n",
      "292/463, train_loss: 0.1432\n",
      "293/463, train_loss: 0.4180\n",
      "294/463, train_loss: 0.1780\n",
      "295/463, train_loss: 0.1034\n",
      "296/463, train_loss: 0.1003\n",
      "297/463, train_loss: 0.0837\n",
      "298/463, train_loss: 0.2085\n",
      "299/463, train_loss: 0.1550\n",
      "300/463, train_loss: 0.0756\n",
      "301/463, train_loss: 0.1191\n",
      "302/463, train_loss: 0.1663\n",
      "303/463, train_loss: 0.2095\n",
      "304/463, train_loss: 0.0602\n",
      "305/463, train_loss: 0.2092\n",
      "306/463, train_loss: 0.4331\n",
      "307/463, train_loss: 0.1765\n",
      "308/463, train_loss: 0.2690\n",
      "309/463, train_loss: 0.1046\n",
      "310/463, train_loss: 0.0447\n",
      "311/463, train_loss: 0.0719\n",
      "312/463, train_loss: 0.1992\n",
      "313/463, train_loss: 0.6255\n",
      "314/463, train_loss: 0.0866\n",
      "315/463, train_loss: 0.1516\n",
      "316/463, train_loss: 0.3079\n",
      "317/463, train_loss: 0.1753\n",
      "318/463, train_loss: 0.1968\n",
      "319/463, train_loss: 1.0156\n",
      "320/463, train_loss: 0.1172\n",
      "321/463, train_loss: 0.1343\n",
      "322/463, train_loss: 0.1047\n",
      "323/463, train_loss: 0.5229\n",
      "324/463, train_loss: 0.5381\n",
      "325/463, train_loss: 0.0978\n",
      "326/463, train_loss: 0.0649\n",
      "327/463, train_loss: 0.1685\n",
      "328/463, train_loss: 0.2136\n",
      "329/463, train_loss: 0.1494\n",
      "330/463, train_loss: 0.1664\n",
      "331/463, train_loss: 0.0916\n",
      "332/463, train_loss: 0.1907\n",
      "333/463, train_loss: 0.0694\n",
      "334/463, train_loss: 0.6729\n",
      "335/463, train_loss: 0.4395\n",
      "336/463, train_loss: 0.3799\n",
      "337/463, train_loss: 0.1093\n",
      "338/463, train_loss: 0.3508\n",
      "339/463, train_loss: 0.1088\n",
      "340/463, train_loss: 0.4148\n",
      "341/463, train_loss: 0.0900\n",
      "342/463, train_loss: 0.1232\n",
      "343/463, train_loss: 0.1764\n",
      "344/463, train_loss: 0.2421\n",
      "345/463, train_loss: 0.0928\n",
      "346/463, train_loss: 0.1226\n",
      "347/463, train_loss: 0.0637\n",
      "348/463, train_loss: 0.2588\n",
      "349/463, train_loss: 0.0702\n",
      "350/463, train_loss: 0.2388\n",
      "351/463, train_loss: 0.0854\n",
      "352/463, train_loss: 0.1937\n",
      "353/463, train_loss: 0.1364\n",
      "354/463, train_loss: 0.2347\n",
      "355/463, train_loss: 0.1033\n",
      "356/463, train_loss: 0.1296\n",
      "357/463, train_loss: 0.1030\n",
      "358/463, train_loss: 0.1230\n",
      "359/463, train_loss: 0.0744\n",
      "360/463, train_loss: 0.1090\n",
      "361/463, train_loss: 0.0684\n",
      "362/463, train_loss: 0.1247\n",
      "363/463, train_loss: 0.0607\n",
      "364/463, train_loss: 0.0798\n",
      "365/463, train_loss: 0.1041\n",
      "366/463, train_loss: 0.0672\n",
      "367/463, train_loss: 0.0757\n",
      "368/463, train_loss: 0.5640\n",
      "369/463, train_loss: 0.1418\n",
      "370/463, train_loss: 0.1541\n",
      "371/463, train_loss: 0.3181\n",
      "372/463, train_loss: 0.1782\n",
      "373/463, train_loss: 0.1558\n",
      "374/463, train_loss: 0.2351\n",
      "375/463, train_loss: 0.3054\n",
      "376/463, train_loss: 0.0892\n",
      "377/463, train_loss: 0.0613\n",
      "378/463, train_loss: 0.1338\n",
      "379/463, train_loss: 0.5513\n",
      "380/463, train_loss: 0.0927\n",
      "381/463, train_loss: 0.1085\n",
      "382/463, train_loss: 0.4377\n",
      "383/463, train_loss: 0.1361\n",
      "384/463, train_loss: 0.1407\n",
      "385/463, train_loss: 0.1697\n",
      "386/463, train_loss: 0.1274\n",
      "387/463, train_loss: 0.3289\n",
      "388/463, train_loss: 0.0724\n",
      "389/463, train_loss: 0.2360\n",
      "390/463, train_loss: 0.0943\n",
      "391/463, train_loss: 0.1515\n",
      "392/463, train_loss: 0.6772\n",
      "393/463, train_loss: 0.1008\n",
      "394/463, train_loss: 0.1510\n",
      "395/463, train_loss: 0.6069\n",
      "396/463, train_loss: 0.2805\n",
      "397/463, train_loss: 0.1307\n",
      "398/463, train_loss: 0.1539\n",
      "399/463, train_loss: 0.2000\n",
      "400/463, train_loss: 0.1748\n",
      "401/463, train_loss: 0.1041\n",
      "402/463, train_loss: 0.0551\n",
      "403/463, train_loss: 0.1234\n",
      "404/463, train_loss: 0.2371\n",
      "405/463, train_loss: 0.1268\n",
      "406/463, train_loss: 0.1035\n",
      "407/463, train_loss: 0.0589\n",
      "408/463, train_loss: 0.1112\n",
      "409/463, train_loss: 0.1831\n",
      "410/463, train_loss: 0.1538\n",
      "411/463, train_loss: 0.0908\n",
      "412/463, train_loss: 0.1562\n",
      "413/463, train_loss: 0.1827\n",
      "414/463, train_loss: 0.1466\n",
      "415/463, train_loss: 0.0814\n",
      "416/463, train_loss: 0.0810\n",
      "417/463, train_loss: 0.1725\n",
      "418/463, train_loss: 0.1052\n",
      "419/463, train_loss: 0.1541\n",
      "420/463, train_loss: 0.0938\n",
      "421/463, train_loss: 0.0827\n",
      "422/463, train_loss: 0.0704\n",
      "423/463, train_loss: 0.3037\n",
      "424/463, train_loss: 0.1017\n",
      "425/463, train_loss: 0.0901\n",
      "426/463, train_loss: 0.0607\n",
      "427/463, train_loss: 0.2327\n",
      "428/463, train_loss: 0.1796\n",
      "429/463, train_loss: 0.1537\n",
      "430/463, train_loss: 0.1368\n",
      "431/463, train_loss: 0.0675\n",
      "432/463, train_loss: 0.1646\n",
      "433/463, train_loss: 0.2866\n",
      "434/463, train_loss: 0.2876\n",
      "435/463, train_loss: 0.1619\n",
      "436/463, train_loss: 0.0864\n",
      "437/463, train_loss: 0.1484\n",
      "438/463, train_loss: 0.1041\n",
      "439/463, train_loss: 0.1963\n",
      "440/463, train_loss: 0.1282\n",
      "441/463, train_loss: 0.3384\n",
      "442/463, train_loss: 0.0435\n",
      "443/463, train_loss: 0.4565\n",
      "444/463, train_loss: 0.1486\n",
      "445/463, train_loss: 0.1396\n",
      "446/463, train_loss: 0.3550\n",
      "447/463, train_loss: 0.0765\n",
      "448/463, train_loss: 0.0865\n",
      "449/463, train_loss: 0.1672\n",
      "450/463, train_loss: 0.1599\n",
      "451/463, train_loss: 0.0829\n",
      "452/463, train_loss: 0.0881\n",
      "453/463, train_loss: 0.0839\n",
      "454/463, train_loss: 0.3506\n",
      "455/463, train_loss: 0.4443\n",
      "456/463, train_loss: 0.1015\n",
      "457/463, train_loss: 0.0973\n",
      "458/463, train_loss: 0.0624\n",
      "459/463, train_loss: 0.1937\n",
      "460/463, train_loss: 0.1459\n",
      "461/463, train_loss: 0.0770\n",
      "462/463, train_loss: 0.1118\n",
      "463/463, train_loss: 0.2018\n",
      "epoch 28 average loss: 0.1880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 20:16:25 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 20:16:28 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 20:16:31 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 29/100\n",
      "1/463, train_loss: 0.0764\n",
      "2/463, train_loss: 0.3945\n",
      "3/463, train_loss: 0.1036\n",
      "4/463, train_loss: 0.1138\n",
      "5/463, train_loss: 0.3472\n",
      "6/463, train_loss: 0.3479\n",
      "7/463, train_loss: 0.0406\n",
      "8/463, train_loss: 0.2896\n",
      "9/463, train_loss: 0.1125\n",
      "10/463, train_loss: 0.3281\n",
      "11/463, train_loss: 0.5254\n",
      "12/463, train_loss: 0.1615\n",
      "13/463, train_loss: 0.2627\n",
      "14/463, train_loss: 0.2500\n",
      "15/463, train_loss: 0.1967\n",
      "16/463, train_loss: 0.4097\n",
      "17/463, train_loss: 0.1599\n",
      "18/463, train_loss: 0.1012\n",
      "19/463, train_loss: 0.1481\n",
      "20/463, train_loss: 0.1055\n",
      "21/463, train_loss: 0.0975\n",
      "22/463, train_loss: 0.1670\n",
      "23/463, train_loss: 0.0946\n",
      "24/463, train_loss: 0.3320\n",
      "25/463, train_loss: 0.5078\n",
      "26/463, train_loss: 0.1039\n",
      "27/463, train_loss: 0.3943\n",
      "28/463, train_loss: 0.1547\n",
      "29/463, train_loss: 0.0844\n",
      "30/463, train_loss: 0.1111\n",
      "31/463, train_loss: 0.1897\n",
      "32/463, train_loss: 0.5098\n",
      "33/463, train_loss: 0.3718\n",
      "34/463, train_loss: 0.1538\n",
      "35/463, train_loss: 0.1807\n",
      "36/463, train_loss: 0.1769\n",
      "37/463, train_loss: 0.1835\n",
      "38/463, train_loss: 0.1029\n",
      "39/463, train_loss: 0.2493\n",
      "40/463, train_loss: 0.1009\n",
      "41/463, train_loss: 0.0990\n",
      "42/463, train_loss: 0.0764\n",
      "43/463, train_loss: 0.1057\n",
      "44/463, train_loss: 0.0624\n",
      "45/463, train_loss: 0.3809\n",
      "46/463, train_loss: 0.1670\n",
      "47/463, train_loss: 0.3616\n",
      "48/463, train_loss: 0.0927\n",
      "49/463, train_loss: 0.2861\n",
      "50/463, train_loss: 0.8745\n",
      "51/463, train_loss: 0.1064\n",
      "52/463, train_loss: 0.1177\n",
      "53/463, train_loss: 0.2622\n",
      "54/463, train_loss: 0.1833\n",
      "55/463, train_loss: 0.1061\n",
      "56/463, train_loss: 0.1060\n",
      "57/463, train_loss: 0.1638\n",
      "58/463, train_loss: 0.1141\n",
      "59/463, train_loss: 0.1130\n",
      "60/463, train_loss: 0.1942\n",
      "61/463, train_loss: 0.1281\n",
      "62/463, train_loss: 0.2737\n",
      "63/463, train_loss: 0.1359\n",
      "64/463, train_loss: 0.1194\n",
      "65/463, train_loss: 0.4041\n",
      "66/463, train_loss: 0.1797\n",
      "67/463, train_loss: 0.0652\n",
      "68/463, train_loss: 0.2297\n",
      "69/463, train_loss: 0.1492\n",
      "70/463, train_loss: 0.3188\n",
      "71/463, train_loss: 0.0873\n",
      "72/463, train_loss: 0.2185\n",
      "73/463, train_loss: 0.0980\n",
      "74/463, train_loss: 0.2112\n",
      "75/463, train_loss: 0.2949\n",
      "76/463, train_loss: 0.2037\n",
      "77/463, train_loss: 0.2991\n",
      "78/463, train_loss: 0.1041\n",
      "79/463, train_loss: 0.0903\n",
      "80/463, train_loss: 0.2053\n",
      "81/463, train_loss: 0.1195\n",
      "82/463, train_loss: 0.4067\n",
      "83/463, train_loss: 0.1820\n",
      "84/463, train_loss: 0.0472\n",
      "85/463, train_loss: 0.0711\n",
      "86/463, train_loss: 0.0997\n",
      "87/463, train_loss: 0.0693\n",
      "88/463, train_loss: 0.3379\n",
      "89/463, train_loss: 0.3994\n",
      "90/463, train_loss: 0.1051\n",
      "91/463, train_loss: 0.3108\n",
      "92/463, train_loss: 0.2256\n",
      "93/463, train_loss: 0.1500\n",
      "94/463, train_loss: 0.2013\n",
      "95/463, train_loss: 0.3716\n",
      "96/463, train_loss: 0.1537\n",
      "97/463, train_loss: 0.1063\n",
      "98/463, train_loss: 0.2720\n",
      "99/463, train_loss: 0.1086\n",
      "100/463, train_loss: 0.0938\n",
      "101/463, train_loss: 0.2380\n",
      "102/463, train_loss: 0.1095\n",
      "103/463, train_loss: 0.0903\n",
      "104/463, train_loss: 0.1987\n",
      "105/463, train_loss: 0.1094\n",
      "106/463, train_loss: 0.4858\n",
      "107/463, train_loss: 0.0797\n",
      "108/463, train_loss: 0.3970\n",
      "109/463, train_loss: 0.1508\n",
      "110/463, train_loss: 0.0656\n",
      "111/463, train_loss: 0.0637\n",
      "112/463, train_loss: 0.0823\n",
      "113/463, train_loss: 0.1322\n",
      "114/463, train_loss: 0.1005\n",
      "115/463, train_loss: 0.5112\n",
      "116/463, train_loss: 0.1204\n",
      "117/463, train_loss: 0.2354\n",
      "118/463, train_loss: 0.0809\n",
      "119/463, train_loss: 0.0939\n",
      "120/463, train_loss: 0.1476\n",
      "121/463, train_loss: 0.2913\n",
      "122/463, train_loss: 0.0781\n",
      "123/463, train_loss: 0.0757\n",
      "124/463, train_loss: 0.1406\n",
      "125/463, train_loss: 0.0873\n",
      "126/463, train_loss: 0.0247\n",
      "127/463, train_loss: 0.1282\n",
      "128/463, train_loss: 0.0826\n",
      "129/463, train_loss: 0.0796\n",
      "130/463, train_loss: 0.0573\n",
      "131/463, train_loss: 0.1107\n",
      "132/463, train_loss: 0.1526\n",
      "133/463, train_loss: 0.0900\n",
      "134/463, train_loss: 0.2065\n",
      "135/463, train_loss: 0.0968\n",
      "136/463, train_loss: 0.0984\n",
      "137/463, train_loss: 0.0900\n",
      "138/463, train_loss: 0.0786\n",
      "139/463, train_loss: 0.5659\n",
      "140/463, train_loss: 0.0936\n",
      "141/463, train_loss: 0.0972\n",
      "142/463, train_loss: 0.1741\n",
      "143/463, train_loss: 0.1487\n",
      "144/463, train_loss: 0.1035\n",
      "145/463, train_loss: 0.0788\n",
      "146/463, train_loss: 0.1116\n",
      "147/463, train_loss: 0.0679\n",
      "148/463, train_loss: 0.8232\n",
      "149/463, train_loss: 0.4153\n",
      "150/463, train_loss: 0.1128\n",
      "151/463, train_loss: 0.0737\n",
      "152/463, train_loss: 0.2305\n",
      "153/463, train_loss: 0.1085\n",
      "154/463, train_loss: 0.1533\n",
      "155/463, train_loss: 0.4204\n",
      "156/463, train_loss: 0.1052\n",
      "157/463, train_loss: 0.1309\n",
      "158/463, train_loss: 0.1660\n",
      "159/463, train_loss: 0.1995\n",
      "160/463, train_loss: 0.0742\n",
      "161/463, train_loss: 0.1105\n",
      "162/463, train_loss: 0.2186\n",
      "163/463, train_loss: 0.1262\n",
      "164/463, train_loss: 0.3516\n",
      "165/463, train_loss: 0.1118\n",
      "166/463, train_loss: 0.0915\n",
      "167/463, train_loss: 0.1094\n",
      "168/463, train_loss: 0.1252\n",
      "169/463, train_loss: 0.1353\n",
      "170/463, train_loss: 0.0642\n",
      "171/463, train_loss: 0.2026\n",
      "172/463, train_loss: 0.1703\n",
      "173/463, train_loss: 0.0709\n",
      "174/463, train_loss: 0.1194\n",
      "175/463, train_loss: 0.1172\n",
      "176/463, train_loss: 0.1249\n",
      "177/463, train_loss: 0.1642\n",
      "178/463, train_loss: 0.6455\n",
      "179/463, train_loss: 0.4331\n",
      "180/463, train_loss: 0.1713\n",
      "181/463, train_loss: 0.8267\n",
      "182/463, train_loss: 0.1384\n",
      "183/463, train_loss: 0.2444\n",
      "184/463, train_loss: 0.1323\n",
      "185/463, train_loss: 0.1110\n",
      "186/463, train_loss: 0.2382\n",
      "187/463, train_loss: 0.1484\n",
      "188/463, train_loss: 0.2393\n",
      "189/463, train_loss: 0.0985\n",
      "190/463, train_loss: 0.1284\n",
      "191/463, train_loss: 0.2742\n",
      "192/463, train_loss: 0.0698\n",
      "193/463, train_loss: 0.3965\n",
      "194/463, train_loss: 0.0668\n",
      "195/463, train_loss: 0.0682\n",
      "196/463, train_loss: 0.3716\n",
      "197/463, train_loss: 0.3882\n",
      "198/463, train_loss: 0.0841\n",
      "199/463, train_loss: 0.1465\n",
      "200/463, train_loss: 0.0713\n",
      "201/463, train_loss: 0.0881\n",
      "202/463, train_loss: 0.0795\n",
      "203/463, train_loss: 0.2002\n",
      "204/463, train_loss: 0.1027\n",
      "205/463, train_loss: 0.0938\n",
      "206/463, train_loss: 0.0820\n",
      "207/463, train_loss: 0.2144\n",
      "208/463, train_loss: 0.1141\n",
      "209/463, train_loss: 0.1714\n",
      "210/463, train_loss: 0.1182\n",
      "211/463, train_loss: 0.1625\n",
      "212/463, train_loss: 0.1040\n",
      "213/463, train_loss: 0.0914\n",
      "214/463, train_loss: 0.2141\n",
      "215/463, train_loss: 0.0635\n",
      "216/463, train_loss: 0.0733\n",
      "217/463, train_loss: 0.1133\n",
      "218/463, train_loss: 0.0344\n",
      "219/463, train_loss: 0.0626\n",
      "220/463, train_loss: 0.4014\n",
      "221/463, train_loss: 0.0643\n",
      "222/463, train_loss: 0.0966\n",
      "223/463, train_loss: 0.1637\n",
      "224/463, train_loss: 0.1926\n",
      "225/463, train_loss: 0.1030\n",
      "226/463, train_loss: 0.2445\n",
      "227/463, train_loss: 0.3855\n",
      "228/463, train_loss: 0.1462\n",
      "229/463, train_loss: 0.1628\n",
      "230/463, train_loss: 0.1440\n",
      "231/463, train_loss: 0.4648\n",
      "232/463, train_loss: 0.4155\n",
      "233/463, train_loss: 0.0786\n",
      "234/463, train_loss: 0.2827\n",
      "235/463, train_loss: 0.0825\n",
      "236/463, train_loss: 0.2405\n",
      "237/463, train_loss: 0.3286\n",
      "238/463, train_loss: 0.0823\n",
      "239/463, train_loss: 0.2168\n",
      "240/463, train_loss: 0.1663\n",
      "241/463, train_loss: 0.0897\n",
      "242/463, train_loss: 0.1714\n",
      "243/463, train_loss: 0.3208\n",
      "244/463, train_loss: 0.1576\n",
      "245/463, train_loss: 0.1501\n",
      "246/463, train_loss: 0.1542\n",
      "247/463, train_loss: 0.2073\n",
      "248/463, train_loss: 0.1097\n",
      "249/463, train_loss: 0.0946\n",
      "250/463, train_loss: 0.1377\n",
      "251/463, train_loss: 0.2257\n",
      "252/463, train_loss: 0.0587\n",
      "253/463, train_loss: 0.1405\n",
      "254/463, train_loss: 0.2725\n",
      "255/463, train_loss: 0.1198\n",
      "256/463, train_loss: 0.2476\n",
      "257/463, train_loss: 0.1561\n",
      "258/463, train_loss: 0.2192\n",
      "259/463, train_loss: 0.3113\n",
      "260/463, train_loss: 0.0812\n",
      "261/463, train_loss: 0.0547\n",
      "262/463, train_loss: 0.1398\n",
      "263/463, train_loss: 0.5610\n",
      "264/463, train_loss: 0.0751\n",
      "265/463, train_loss: 0.1719\n",
      "266/463, train_loss: 0.5005\n",
      "267/463, train_loss: 0.1368\n",
      "268/463, train_loss: 0.1390\n",
      "269/463, train_loss: 0.1005\n",
      "270/463, train_loss: 0.8315\n",
      "271/463, train_loss: 0.1353\n",
      "272/463, train_loss: 0.2759\n",
      "273/463, train_loss: 0.2216\n",
      "274/463, train_loss: 0.2438\n",
      "275/463, train_loss: 0.2163\n",
      "276/463, train_loss: 0.1458\n",
      "277/463, train_loss: 0.1456\n",
      "278/463, train_loss: 0.1201\n",
      "279/463, train_loss: 0.3262\n",
      "280/463, train_loss: 0.3218\n",
      "281/463, train_loss: 0.1368\n",
      "282/463, train_loss: 0.2185\n",
      "283/463, train_loss: 0.0581\n",
      "284/463, train_loss: 0.0909\n",
      "285/463, train_loss: 0.0504\n",
      "286/463, train_loss: 0.1982\n",
      "287/463, train_loss: 0.0774\n",
      "288/463, train_loss: 0.1384\n",
      "289/463, train_loss: 0.3230\n",
      "290/463, train_loss: 0.1431\n",
      "291/463, train_loss: 0.1033\n",
      "292/463, train_loss: 0.1012\n",
      "293/463, train_loss: 0.0907\n",
      "294/463, train_loss: 0.1145\n",
      "295/463, train_loss: 0.0449\n",
      "296/463, train_loss: 0.1589\n",
      "297/463, train_loss: 0.1636\n",
      "298/463, train_loss: 0.0954\n",
      "299/463, train_loss: 0.1312\n",
      "300/463, train_loss: 0.1076\n",
      "301/463, train_loss: 0.0756\n",
      "302/463, train_loss: 0.4385\n",
      "303/463, train_loss: 0.0958\n",
      "304/463, train_loss: 0.1555\n",
      "305/463, train_loss: 0.3628\n",
      "306/463, train_loss: 0.0635\n",
      "307/463, train_loss: 0.0729\n",
      "308/463, train_loss: 0.1176\n",
      "309/463, train_loss: 0.0682\n",
      "310/463, train_loss: 0.0574\n",
      "311/463, train_loss: 0.0791\n",
      "312/463, train_loss: 0.1244\n",
      "313/463, train_loss: 0.1621\n",
      "314/463, train_loss: 0.1289\n",
      "315/463, train_loss: 0.1058\n",
      "316/463, train_loss: 0.1432\n",
      "317/463, train_loss: 0.1365\n",
      "318/463, train_loss: 0.1599\n",
      "319/463, train_loss: 0.1573\n",
      "320/463, train_loss: 0.1224\n",
      "321/463, train_loss: 0.1637\n",
      "322/463, train_loss: 0.1171\n",
      "323/463, train_loss: 0.1062\n",
      "324/463, train_loss: 0.1132\n",
      "325/463, train_loss: 0.0876\n",
      "326/463, train_loss: 0.1201\n",
      "327/463, train_loss: 0.0929\n",
      "328/463, train_loss: 0.3638\n",
      "329/463, train_loss: 0.1245\n",
      "330/463, train_loss: 0.1406\n",
      "331/463, train_loss: 0.1250\n",
      "332/463, train_loss: 0.1232\n",
      "333/463, train_loss: 0.0901\n",
      "334/463, train_loss: 0.1087\n",
      "335/463, train_loss: 0.3320\n",
      "336/463, train_loss: 0.1295\n",
      "337/463, train_loss: 0.1066\n",
      "338/463, train_loss: 0.1530\n",
      "339/463, train_loss: 0.1604\n",
      "340/463, train_loss: 0.0939\n",
      "341/463, train_loss: 0.1741\n",
      "342/463, train_loss: 0.1038\n",
      "343/463, train_loss: 0.1824\n",
      "344/463, train_loss: 0.0438\n",
      "345/463, train_loss: 0.0734\n",
      "346/463, train_loss: 0.1372\n",
      "347/463, train_loss: 0.1318\n",
      "348/463, train_loss: 0.2954\n",
      "349/463, train_loss: 0.1526\n",
      "350/463, train_loss: 0.0310\n",
      "351/463, train_loss: 0.1525\n",
      "352/463, train_loss: 0.0936\n",
      "353/463, train_loss: 0.2686\n",
      "354/463, train_loss: 0.0873\n",
      "355/463, train_loss: 0.2900\n",
      "356/463, train_loss: 0.1987\n",
      "357/463, train_loss: 0.3027\n",
      "358/463, train_loss: 0.0969\n",
      "359/463, train_loss: 0.7046\n",
      "360/463, train_loss: 0.4692\n",
      "361/463, train_loss: 0.1311\n",
      "362/463, train_loss: 0.1032\n",
      "363/463, train_loss: 0.3794\n",
      "364/463, train_loss: 0.1338\n",
      "365/463, train_loss: 0.0875\n",
      "366/463, train_loss: 0.0484\n",
      "367/463, train_loss: 0.0948\n",
      "368/463, train_loss: 0.4160\n",
      "369/463, train_loss: 0.5869\n",
      "370/463, train_loss: 0.1477\n",
      "371/463, train_loss: 0.5957\n",
      "372/463, train_loss: 0.2563\n",
      "373/463, train_loss: 0.1349\n",
      "374/463, train_loss: 0.1294\n",
      "375/463, train_loss: 0.0587\n",
      "376/463, train_loss: 0.0746\n",
      "377/463, train_loss: 0.2120\n",
      "378/463, train_loss: 0.2378\n",
      "379/463, train_loss: 0.1300\n",
      "380/463, train_loss: 0.1550\n",
      "381/463, train_loss: 0.0977\n",
      "382/463, train_loss: 0.1017\n",
      "383/463, train_loss: 0.2478\n",
      "384/463, train_loss: 0.2185\n",
      "385/463, train_loss: 0.1487\n",
      "386/463, train_loss: 0.1029\n",
      "387/463, train_loss: 0.2852\n",
      "388/463, train_loss: 0.5259\n",
      "389/463, train_loss: 0.2062\n",
      "390/463, train_loss: 0.1831\n",
      "391/463, train_loss: 0.1749\n",
      "392/463, train_loss: 0.1030\n",
      "393/463, train_loss: 0.2278\n",
      "394/463, train_loss: 0.2610\n",
      "395/463, train_loss: 0.3997\n",
      "396/463, train_loss: 0.0865\n",
      "397/463, train_loss: 0.1484\n",
      "398/463, train_loss: 0.0752\n",
      "399/463, train_loss: 0.0897\n",
      "400/463, train_loss: 0.1205\n",
      "401/463, train_loss: 0.0768\n",
      "402/463, train_loss: 0.1650\n",
      "403/463, train_loss: 0.1554\n",
      "404/463, train_loss: 0.1523\n",
      "405/463, train_loss: 0.1392\n",
      "406/463, train_loss: 0.2476\n",
      "407/463, train_loss: 0.4253\n",
      "408/463, train_loss: 0.0610\n",
      "409/463, train_loss: 0.0353\n",
      "410/463, train_loss: 0.1488\n",
      "411/463, train_loss: 0.0929\n",
      "412/463, train_loss: 0.0762\n",
      "413/463, train_loss: 0.2471\n",
      "414/463, train_loss: 0.1801\n",
      "415/463, train_loss: 0.1223\n",
      "416/463, train_loss: 0.2881\n",
      "417/463, train_loss: 0.2356\n",
      "418/463, train_loss: 0.2866\n",
      "419/463, train_loss: 0.2686\n",
      "420/463, train_loss: 0.0679\n",
      "421/463, train_loss: 0.1375\n",
      "422/463, train_loss: 0.0830\n",
      "423/463, train_loss: 0.0452\n",
      "424/463, train_loss: 0.0740\n",
      "425/463, train_loss: 0.8184\n",
      "426/463, train_loss: 0.3359\n",
      "427/463, train_loss: 0.0861\n",
      "428/463, train_loss: 0.1572\n",
      "429/463, train_loss: 0.1219\n",
      "430/463, train_loss: 0.0807\n",
      "431/463, train_loss: 0.1240\n",
      "432/463, train_loss: 0.0559\n",
      "433/463, train_loss: 0.3096\n",
      "434/463, train_loss: 0.0850\n",
      "435/463, train_loss: 0.1230\n",
      "436/463, train_loss: 0.2563\n",
      "437/463, train_loss: 0.1766\n",
      "438/463, train_loss: 0.0896\n",
      "439/463, train_loss: 0.0715\n",
      "440/463, train_loss: 0.1021\n",
      "441/463, train_loss: 0.0891\n",
      "442/463, train_loss: 0.0974\n",
      "443/463, train_loss: 0.3779\n",
      "444/463, train_loss: 0.1052\n",
      "445/463, train_loss: 0.0681\n",
      "446/463, train_loss: 0.0721\n",
      "447/463, train_loss: 0.1354\n",
      "448/463, train_loss: 0.1238\n",
      "449/463, train_loss: 0.0767\n",
      "450/463, train_loss: 0.1246\n",
      "451/463, train_loss: 0.3652\n",
      "452/463, train_loss: 0.3115\n",
      "453/463, train_loss: 0.1122\n",
      "454/463, train_loss: 0.1863\n",
      "455/463, train_loss: 0.1219\n",
      "456/463, train_loss: 0.0826\n",
      "457/463, train_loss: 0.0632\n",
      "458/463, train_loss: 0.1401\n",
      "459/463, train_loss: 0.0986\n",
      "460/463, train_loss: 0.2053\n",
      "461/463, train_loss: 0.0972\n",
      "462/463, train_loss: 0.0873\n",
      "463/463, train_loss: 0.0526\n",
      "epoch 29 average loss: 0.1798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/22 22:30:12 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 22:30:15 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/22 22:30:18 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 30/100\n",
      "1/463, train_loss: 0.0914\n",
      "2/463, train_loss: 0.1387\n",
      "3/463, train_loss: 0.1440\n",
      "4/463, train_loss: 0.1859\n",
      "5/463, train_loss: 0.1165\n",
      "6/463, train_loss: 0.2610\n",
      "7/463, train_loss: 0.2808\n",
      "8/463, train_loss: 0.0723\n",
      "9/463, train_loss: 0.4216\n",
      "10/463, train_loss: 0.1333\n",
      "11/463, train_loss: 0.1132\n",
      "12/463, train_loss: 0.1494\n",
      "13/463, train_loss: 0.0878\n",
      "14/463, train_loss: 0.3979\n",
      "15/463, train_loss: 0.1168\n",
      "16/463, train_loss: 0.0753\n",
      "17/463, train_loss: 0.1714\n",
      "18/463, train_loss: 0.1791\n",
      "19/463, train_loss: 0.1755\n",
      "20/463, train_loss: 0.3066\n",
      "21/463, train_loss: 0.0911\n",
      "22/463, train_loss: 0.3125\n",
      "23/463, train_loss: 0.1131\n",
      "24/463, train_loss: 0.2460\n",
      "25/463, train_loss: 0.0552\n",
      "26/463, train_loss: 0.0920\n",
      "27/463, train_loss: 0.2019\n",
      "28/463, train_loss: 0.1061\n",
      "29/463, train_loss: 0.5332\n",
      "30/463, train_loss: 0.2039\n",
      "31/463, train_loss: 0.3535\n",
      "32/463, train_loss: 0.2485\n",
      "33/463, train_loss: 0.0919\n",
      "34/463, train_loss: 0.0657\n",
      "35/463, train_loss: 0.0879\n",
      "36/463, train_loss: 0.1609\n",
      "37/463, train_loss: 0.1562\n",
      "38/463, train_loss: 0.1787\n",
      "39/463, train_loss: 0.1899\n",
      "40/463, train_loss: 0.1124\n",
      "41/463, train_loss: 0.2230\n",
      "42/463, train_loss: 0.0793\n",
      "43/463, train_loss: 0.0796\n",
      "44/463, train_loss: 0.3208\n",
      "45/463, train_loss: 0.0603\n",
      "46/463, train_loss: 0.0579\n",
      "47/463, train_loss: 0.0997\n",
      "48/463, train_loss: 0.0697\n",
      "49/463, train_loss: 0.0348\n",
      "50/463, train_loss: 0.0500\n",
      "51/463, train_loss: 0.0967\n",
      "52/463, train_loss: 0.0603\n",
      "53/463, train_loss: 0.1763\n",
      "54/463, train_loss: 0.1697\n",
      "55/463, train_loss: 0.1628\n",
      "56/463, train_loss: 0.1267\n",
      "57/463, train_loss: 0.0436\n",
      "58/463, train_loss: 0.1508\n",
      "59/463, train_loss: 0.1609\n",
      "60/463, train_loss: 0.1692\n",
      "61/463, train_loss: 0.3071\n",
      "62/463, train_loss: 0.1729\n",
      "63/463, train_loss: 0.0968\n",
      "64/463, train_loss: 0.1272\n",
      "65/463, train_loss: 0.5010\n",
      "66/463, train_loss: 0.2050\n",
      "67/463, train_loss: 0.1514\n",
      "68/463, train_loss: 0.2296\n",
      "69/463, train_loss: 0.0815\n",
      "70/463, train_loss: 0.1648\n",
      "71/463, train_loss: 0.1683\n",
      "72/463, train_loss: 0.5010\n",
      "73/463, train_loss: 0.1149\n",
      "74/463, train_loss: 0.0781\n",
      "75/463, train_loss: 0.3101\n",
      "76/463, train_loss: 0.0704\n",
      "77/463, train_loss: 0.1700\n",
      "78/463, train_loss: 0.0844\n",
      "79/463, train_loss: 0.1776\n",
      "80/463, train_loss: 0.0992\n",
      "81/463, train_loss: 0.1017\n",
      "82/463, train_loss: 0.4253\n",
      "83/463, train_loss: 0.1393\n",
      "84/463, train_loss: 0.0682\n",
      "85/463, train_loss: 0.1304\n",
      "86/463, train_loss: 0.0979\n",
      "87/463, train_loss: 0.2769\n",
      "88/463, train_loss: 0.3750\n",
      "89/463, train_loss: 0.1079\n",
      "90/463, train_loss: 0.2185\n",
      "91/463, train_loss: 0.0475\n",
      "92/463, train_loss: 0.4614\n",
      "93/463, train_loss: 0.1759\n",
      "94/463, train_loss: 0.1514\n",
      "95/463, train_loss: 0.0731\n",
      "96/463, train_loss: 0.3081\n",
      "97/463, train_loss: 0.1669\n",
      "98/463, train_loss: 0.1042\n",
      "99/463, train_loss: 0.3733\n",
      "100/463, train_loss: 0.0822\n",
      "101/463, train_loss: 0.0735\n",
      "102/463, train_loss: 0.0953\n",
      "103/463, train_loss: 0.1241\n",
      "104/463, train_loss: 0.1533\n",
      "105/463, train_loss: 0.1310\n",
      "106/463, train_loss: 0.3601\n",
      "107/463, train_loss: 0.1901\n",
      "108/463, train_loss: 0.2107\n",
      "109/463, train_loss: 0.2729\n",
      "110/463, train_loss: 0.2363\n",
      "111/463, train_loss: 0.2778\n",
      "112/463, train_loss: 0.3970\n",
      "113/463, train_loss: 0.1562\n",
      "114/463, train_loss: 0.5474\n",
      "115/463, train_loss: 0.0386\n",
      "116/463, train_loss: 0.1118\n",
      "117/463, train_loss: 0.7158\n",
      "118/463, train_loss: 0.1196\n",
      "119/463, train_loss: 0.2964\n",
      "120/463, train_loss: 0.3235\n",
      "121/463, train_loss: 0.2069\n",
      "122/463, train_loss: 0.1587\n",
      "123/463, train_loss: 0.2488\n",
      "124/463, train_loss: 0.0873\n",
      "125/463, train_loss: 0.0870\n",
      "126/463, train_loss: 0.1584\n",
      "127/463, train_loss: 0.2133\n",
      "128/463, train_loss: 0.2825\n",
      "129/463, train_loss: 0.1725\n",
      "130/463, train_loss: 0.0746\n",
      "131/463, train_loss: 0.0854\n",
      "132/463, train_loss: 0.0886\n",
      "133/463, train_loss: 0.0583\n",
      "134/463, train_loss: 0.1724\n",
      "135/463, train_loss: 0.1644\n",
      "136/463, train_loss: 0.1343\n",
      "137/463, train_loss: 0.1665\n",
      "138/463, train_loss: 0.1292\n",
      "139/463, train_loss: 0.1508\n",
      "140/463, train_loss: 0.0997\n",
      "141/463, train_loss: 0.6245\n",
      "142/463, train_loss: 0.0828\n",
      "143/463, train_loss: 0.1608\n",
      "144/463, train_loss: 0.0730\n",
      "145/463, train_loss: 0.2026\n",
      "146/463, train_loss: 0.2832\n",
      "147/463, train_loss: 0.1963\n",
      "148/463, train_loss: 0.0645\n",
      "149/463, train_loss: 0.1572\n",
      "150/463, train_loss: 0.1296\n",
      "151/463, train_loss: 0.4053\n",
      "152/463, train_loss: 0.0792\n",
      "153/463, train_loss: 0.1262\n",
      "154/463, train_loss: 0.4312\n",
      "155/463, train_loss: 0.4939\n",
      "156/463, train_loss: 0.0844\n",
      "157/463, train_loss: 0.0974\n",
      "158/463, train_loss: 0.0514\n",
      "159/463, train_loss: 0.4458\n",
      "160/463, train_loss: 0.1261\n",
      "161/463, train_loss: 0.0606\n",
      "162/463, train_loss: 0.0424\n",
      "163/463, train_loss: 0.1527\n",
      "164/463, train_loss: 0.1653\n",
      "165/463, train_loss: 0.0914\n",
      "166/463, train_loss: 0.4043\n",
      "167/463, train_loss: 0.1536\n",
      "168/463, train_loss: 0.3376\n",
      "169/463, train_loss: 0.4519\n",
      "170/463, train_loss: 0.2971\n",
      "171/463, train_loss: 0.1759\n",
      "172/463, train_loss: 0.0965\n",
      "173/463, train_loss: 0.1758\n",
      "174/463, train_loss: 0.3276\n",
      "175/463, train_loss: 0.1053\n",
      "176/463, train_loss: 0.0940\n",
      "177/463, train_loss: 0.1472\n",
      "178/463, train_loss: 0.1392\n",
      "179/463, train_loss: 0.0756\n",
      "180/463, train_loss: 0.2024\n",
      "181/463, train_loss: 0.0809\n",
      "182/463, train_loss: 0.0933\n",
      "183/463, train_loss: 0.1882\n",
      "184/463, train_loss: 0.0421\n",
      "185/463, train_loss: 0.1868\n",
      "186/463, train_loss: 0.1066\n",
      "187/463, train_loss: 0.0754\n",
      "188/463, train_loss: 0.1118\n",
      "189/463, train_loss: 0.1772\n",
      "190/463, train_loss: 0.5059\n",
      "191/463, train_loss: 0.0684\n",
      "192/463, train_loss: 0.5762\n",
      "193/463, train_loss: 0.7178\n",
      "194/463, train_loss: 0.0858\n",
      "195/463, train_loss: 0.0638\n",
      "196/463, train_loss: 0.1431\n",
      "197/463, train_loss: 0.1252\n",
      "198/463, train_loss: 0.0754\n",
      "199/463, train_loss: 0.0831\n",
      "200/463, train_loss: 0.0890\n",
      "201/463, train_loss: 0.1676\n",
      "202/463, train_loss: 0.3582\n",
      "203/463, train_loss: 0.0915\n",
      "204/463, train_loss: 0.2000\n",
      "205/463, train_loss: 0.1216\n",
      "206/463, train_loss: 0.1454\n",
      "207/463, train_loss: 0.0823\n",
      "208/463, train_loss: 0.3997\n",
      "209/463, train_loss: 0.1448\n",
      "210/463, train_loss: 0.0913\n",
      "211/463, train_loss: 0.5166\n",
      "212/463, train_loss: 0.1510\n",
      "213/463, train_loss: 0.7007\n",
      "214/463, train_loss: 0.0834\n",
      "215/463, train_loss: 0.1384\n",
      "216/463, train_loss: 0.3745\n",
      "217/463, train_loss: 0.1213\n",
      "218/463, train_loss: 1.2305\n",
      "219/463, train_loss: 0.2234\n",
      "220/463, train_loss: 0.5962\n",
      "221/463, train_loss: 0.0875\n",
      "222/463, train_loss: 0.1824\n",
      "223/463, train_loss: 0.1382\n",
      "224/463, train_loss: 0.1189\n",
      "225/463, train_loss: 0.1265\n",
      "226/463, train_loss: 0.1337\n",
      "227/463, train_loss: 0.3262\n",
      "228/463, train_loss: 0.0812\n",
      "229/463, train_loss: 0.6426\n",
      "230/463, train_loss: 0.4219\n",
      "231/463, train_loss: 0.4436\n",
      "232/463, train_loss: 0.1301\n",
      "233/463, train_loss: 0.4219\n",
      "234/463, train_loss: 0.1082\n",
      "235/463, train_loss: 0.3660\n",
      "236/463, train_loss: 0.1868\n",
      "237/463, train_loss: 0.1962\n",
      "238/463, train_loss: 0.1450\n",
      "239/463, train_loss: 0.0562\n",
      "240/463, train_loss: 0.1247\n",
      "241/463, train_loss: 0.1453\n",
      "242/463, train_loss: 0.1643\n",
      "243/463, train_loss: 0.1082\n",
      "244/463, train_loss: 0.2054\n",
      "245/463, train_loss: 0.1070\n",
      "246/463, train_loss: 0.2352\n",
      "247/463, train_loss: 0.0803\n",
      "248/463, train_loss: 0.1224\n",
      "249/463, train_loss: 0.1755\n",
      "250/463, train_loss: 0.1444\n",
      "251/463, train_loss: 0.1005\n",
      "252/463, train_loss: 0.3884\n",
      "253/463, train_loss: 0.0403\n",
      "254/463, train_loss: 0.1857\n",
      "255/463, train_loss: 0.1357\n",
      "256/463, train_loss: 0.3118\n",
      "257/463, train_loss: 0.0518\n",
      "258/463, train_loss: 0.1938\n",
      "259/463, train_loss: 0.1554\n",
      "260/463, train_loss: 0.1060\n",
      "261/463, train_loss: 0.3647\n",
      "262/463, train_loss: 0.3501\n",
      "263/463, train_loss: 0.0970\n",
      "264/463, train_loss: 0.1326\n",
      "265/463, train_loss: 0.0931\n",
      "266/463, train_loss: 0.1141\n",
      "267/463, train_loss: 0.1656\n",
      "268/463, train_loss: 0.1204\n",
      "269/463, train_loss: 0.1959\n",
      "270/463, train_loss: 0.4048\n",
      "271/463, train_loss: 0.1760\n",
      "272/463, train_loss: 0.2725\n",
      "273/463, train_loss: 0.2427\n",
      "274/463, train_loss: 0.3005\n",
      "275/463, train_loss: 0.2097\n",
      "276/463, train_loss: 0.0676\n",
      "277/463, train_loss: 0.1305\n",
      "278/463, train_loss: 0.0873\n",
      "279/463, train_loss: 0.2211\n",
      "280/463, train_loss: 0.1661\n",
      "281/463, train_loss: 0.1279\n",
      "282/463, train_loss: 0.2068\n",
      "283/463, train_loss: 0.1005\n",
      "284/463, train_loss: 0.3914\n",
      "285/463, train_loss: 0.1180\n",
      "286/463, train_loss: 0.1099\n",
      "287/463, train_loss: 0.0746\n",
      "288/463, train_loss: 0.1154\n",
      "289/463, train_loss: 0.0789\n",
      "290/463, train_loss: 0.3936\n",
      "291/463, train_loss: 0.1272\n",
      "292/463, train_loss: 0.2410\n",
      "293/463, train_loss: 0.1779\n",
      "294/463, train_loss: 0.0793\n",
      "295/463, train_loss: 0.1523\n",
      "296/463, train_loss: 0.0826\n",
      "297/463, train_loss: 0.0918\n",
      "298/463, train_loss: 0.0516\n",
      "299/463, train_loss: 0.4219\n",
      "300/463, train_loss: 0.0805\n",
      "301/463, train_loss: 0.1506\n",
      "302/463, train_loss: 0.1588\n",
      "303/463, train_loss: 0.1202\n",
      "304/463, train_loss: 0.1460\n",
      "305/463, train_loss: 0.2261\n",
      "306/463, train_loss: 0.0836\n",
      "307/463, train_loss: 0.1077\n",
      "308/463, train_loss: 0.0620\n",
      "309/463, train_loss: 0.4092\n",
      "310/463, train_loss: 0.0873\n",
      "311/463, train_loss: 0.1111\n",
      "312/463, train_loss: 0.1738\n",
      "313/463, train_loss: 0.6187\n",
      "314/463, train_loss: 0.5811\n",
      "315/463, train_loss: 0.1406\n",
      "316/463, train_loss: 0.1904\n",
      "317/463, train_loss: 0.1460\n",
      "318/463, train_loss: 0.1084\n",
      "319/463, train_loss: 0.0546\n",
      "320/463, train_loss: 0.0966\n",
      "321/463, train_loss: 0.2410\n",
      "322/463, train_loss: 0.1631\n",
      "323/463, train_loss: 0.1233\n",
      "324/463, train_loss: 0.1517\n",
      "325/463, train_loss: 0.1061\n",
      "326/463, train_loss: 0.1383\n",
      "327/463, train_loss: 0.0792\n",
      "328/463, train_loss: 0.1071\n",
      "329/463, train_loss: 0.0868\n",
      "330/463, train_loss: 0.2886\n",
      "331/463, train_loss: 0.0540\n",
      "332/463, train_loss: 0.3447\n",
      "333/463, train_loss: 0.5117\n",
      "334/463, train_loss: 0.1775\n",
      "335/463, train_loss: 0.0723\n",
      "336/463, train_loss: 0.2146\n",
      "337/463, train_loss: 0.1620\n",
      "338/463, train_loss: 0.1467\n",
      "339/463, train_loss: 0.1294\n",
      "340/463, train_loss: 0.3699\n",
      "341/463, train_loss: 0.1257\n",
      "342/463, train_loss: 0.0735\n",
      "343/463, train_loss: 0.1338\n",
      "344/463, train_loss: 0.0706\n",
      "345/463, train_loss: 0.1436\n",
      "346/463, train_loss: 0.4082\n",
      "347/463, train_loss: 0.0938\n",
      "348/463, train_loss: 0.4456\n",
      "349/463, train_loss: 0.1084\n",
      "350/463, train_loss: 0.0935\n",
      "351/463, train_loss: 0.0985\n",
      "352/463, train_loss: 0.2056\n",
      "353/463, train_loss: 0.1973\n",
      "354/463, train_loss: 0.1895\n",
      "355/463, train_loss: 0.1621\n",
      "356/463, train_loss: 0.1792\n",
      "357/463, train_loss: 0.1589\n",
      "358/463, train_loss: 0.0586\n",
      "359/463, train_loss: 0.1162\n",
      "360/463, train_loss: 0.1351\n",
      "361/463, train_loss: 0.1277\n",
      "362/463, train_loss: 0.2209\n",
      "363/463, train_loss: 0.1046\n",
      "364/463, train_loss: 0.1348\n",
      "365/463, train_loss: 0.1044\n",
      "366/463, train_loss: 0.1979\n",
      "367/463, train_loss: 0.1637\n",
      "368/463, train_loss: 0.1055\n",
      "369/463, train_loss: 0.1030\n",
      "370/463, train_loss: 0.0635\n",
      "371/463, train_loss: 0.2075\n",
      "372/463, train_loss: 0.0598\n",
      "373/463, train_loss: 0.0652\n",
      "374/463, train_loss: 0.1643\n",
      "375/463, train_loss: 0.2910\n",
      "376/463, train_loss: 0.1196\n",
      "377/463, train_loss: 0.0782\n",
      "378/463, train_loss: 0.3867\n",
      "379/463, train_loss: 0.0562\n",
      "380/463, train_loss: 0.2117\n",
      "381/463, train_loss: 0.1492\n",
      "382/463, train_loss: 0.2505\n",
      "383/463, train_loss: 0.0754\n",
      "384/463, train_loss: 0.1101\n",
      "385/463, train_loss: 0.1840\n",
      "386/463, train_loss: 0.0798\n",
      "387/463, train_loss: 0.2419\n",
      "388/463, train_loss: 0.1992\n",
      "389/463, train_loss: 0.1445\n",
      "390/463, train_loss: 0.1946\n",
      "391/463, train_loss: 0.0973\n",
      "392/463, train_loss: 0.1816\n",
      "393/463, train_loss: 0.1278\n",
      "394/463, train_loss: 0.0507\n",
      "395/463, train_loss: 0.0696\n",
      "396/463, train_loss: 0.0046\n",
      "397/463, train_loss: 0.1725\n",
      "398/463, train_loss: 0.1265\n",
      "399/463, train_loss: 0.1436\n",
      "400/463, train_loss: 0.1643\n",
      "401/463, train_loss: 0.0482\n",
      "402/463, train_loss: 0.3711\n",
      "403/463, train_loss: 0.3496\n",
      "404/463, train_loss: 0.3845\n",
      "405/463, train_loss: 0.0857\n",
      "406/463, train_loss: 0.0798\n",
      "407/463, train_loss: 0.0737\n",
      "408/463, train_loss: 0.0157\n",
      "409/463, train_loss: 0.5522\n",
      "410/463, train_loss: 0.0607\n",
      "411/463, train_loss: 0.0770\n",
      "412/463, train_loss: 0.0798\n",
      "413/463, train_loss: 0.2708\n",
      "414/463, train_loss: 0.1478\n",
      "415/463, train_loss: 0.1572\n",
      "416/463, train_loss: 0.2710\n",
      "417/463, train_loss: 0.2993\n",
      "418/463, train_loss: 0.0804\n",
      "419/463, train_loss: 0.1493\n",
      "420/463, train_loss: 0.3296\n",
      "421/463, train_loss: 0.1385\n",
      "422/463, train_loss: 0.2141\n",
      "423/463, train_loss: 0.1072\n",
      "424/463, train_loss: 0.6367\n",
      "425/463, train_loss: 0.3474\n",
      "426/463, train_loss: 0.1724\n",
      "427/463, train_loss: 0.1092\n",
      "428/463, train_loss: 0.1343\n",
      "429/463, train_loss: 0.0837\n",
      "430/463, train_loss: 0.0646\n",
      "431/463, train_loss: 0.1193\n",
      "432/463, train_loss: 0.1210\n",
      "433/463, train_loss: 0.2681\n",
      "434/463, train_loss: 0.0757\n",
      "435/463, train_loss: 0.0927\n",
      "436/463, train_loss: 0.0883\n",
      "437/463, train_loss: 0.1063\n",
      "438/463, train_loss: 0.2440\n",
      "439/463, train_loss: 0.0913\n",
      "440/463, train_loss: 0.3110\n",
      "441/463, train_loss: 0.0535\n",
      "442/463, train_loss: 0.1066\n",
      "443/463, train_loss: 0.2773\n",
      "444/463, train_loss: 0.0729\n",
      "445/463, train_loss: 0.0601\n",
      "446/463, train_loss: 0.2305\n",
      "447/463, train_loss: 0.2460\n",
      "448/463, train_loss: 0.4880\n",
      "449/463, train_loss: 0.1302\n",
      "450/463, train_loss: 0.2727\n",
      "451/463, train_loss: 0.0891\n",
      "452/463, train_loss: 0.1587\n",
      "453/463, train_loss: 0.0922\n",
      "454/463, train_loss: 0.0790\n",
      "455/463, train_loss: 0.0763\n",
      "456/463, train_loss: 0.2017\n",
      "457/463, train_loss: 0.1882\n",
      "458/463, train_loss: 0.1790\n",
      "459/463, train_loss: 0.4824\n",
      "460/463, train_loss: 0.2010\n",
      "461/463, train_loss: 0.0494\n",
      "462/463, train_loss: 0.1127\n",
      "463/463, train_loss: 0.1394\n",
      "epoch 30 average loss: 0.1839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023/09/23 00:43:28 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/23 00:43:30 WARNING mlflow.utils.requirements_utils: Found torch version (2.0.1+cu118) contains a local version label (+cu118). MLflow logged a pip requirement for this package as 'torch==2.0.1' without the local version label to make it installable from PyPI. To specify pip requirements containing local version labels, please use `conda_env` or `pip_requirements`.\n",
      "2023/09/23 00:43:33 WARNING mlflow.models.model: Logging model metadata to the tracking server has failed. The model artifacts have been logged successfully under /mlflow/5/93273c7eb040429a9470b98ebd6ffb12/artifacts. Set logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)` to see the full traceback.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/mixed/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630/1.3.6.1.4.1.14519.5.2.1.6279.6001.153536305742006952753134773630.nii.gz\n",
      "{'mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.5246881924598294, 'nodule_mAP_IoU_0.10_0.50_0.05_MaxDet_100': 0.5246881924598294, 'AP_IoU_0.10_MaxDet_100': 0.5431999499623729, 'nodule_AP_IoU_0.10_MaxDet_100': 0.5431999499623729, 'mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.9145299196243286, 'nodule_mAR_IoU_0.10_0.50_0.05_MaxDet_100': 0.9145299196243286, 'AR_IoU_0.10_MaxDet_100': 0.9487179517745972, 'nodule_AR_IoU_0.10_MaxDet_100': 0.9487179517745972}\n",
      "current epoch: 30 current metric: 0.7328 best metric: 0.7468 at epoch 15\n",
      "----------\n",
      "epoch 31/100\n",
      "1/463, train_loss: 0.2045\n",
      "2/463, train_loss: 0.3049\n",
      "3/463, train_loss: 0.1057\n",
      "4/463, train_loss: 0.2588\n",
      "5/463, train_loss: 0.1213\n",
      "6/463, train_loss: 0.1765\n",
      "7/463, train_loss: 0.0973\n",
      "8/463, train_loss: 0.1333\n",
      "9/463, train_loss: 0.3308\n",
      "10/463, train_loss: 0.0224\n",
      "11/463, train_loss: 0.0906\n",
      "12/463, train_loss: 0.1173\n",
      "13/463, train_loss: 0.1233\n",
      "14/463, train_loss: 0.1218\n",
      "15/463, train_loss: 0.7119\n",
      "16/463, train_loss: 0.3193\n",
      "17/463, train_loss: 0.0967\n",
      "18/463, train_loss: 0.1017\n",
      "19/463, train_loss: 0.1201\n",
      "20/463, train_loss: 0.0985\n",
      "21/463, train_loss: 0.1238\n",
      "22/463, train_loss: 0.2095\n",
      "23/463, train_loss: 0.2798\n",
      "24/463, train_loss: 0.0780\n",
      "25/463, train_loss: 0.0880\n",
      "26/463, train_loss: 0.1538\n",
      "27/463, train_loss: 0.4214\n",
      "28/463, train_loss: 0.0671\n",
      "29/463, train_loss: 0.1165\n",
      "30/463, train_loss: 0.7979\n",
      "31/463, train_loss: 0.1761\n",
      "32/463, train_loss: 0.2217\n",
      "33/463, train_loss: 0.1509\n",
      "34/463, train_loss: 0.1394\n",
      "35/463, train_loss: 0.1372\n",
      "36/463, train_loss: 0.1635\n",
      "37/463, train_loss: 0.1572\n",
      "38/463, train_loss: 0.0751\n",
      "39/463, train_loss: 0.4475\n",
      "40/463, train_loss: 0.0905\n",
      "41/463, train_loss: 0.1571\n",
      "42/463, train_loss: 0.2903\n",
      "43/463, train_loss: 0.0829\n",
      "44/463, train_loss: 0.1489\n",
      "45/463, train_loss: 0.2646\n",
      "46/463, train_loss: 0.1224\n",
      "47/463, train_loss: 0.2549\n",
      "48/463, train_loss: 0.0890\n",
      "49/463, train_loss: 0.2544\n",
      "50/463, train_loss: 0.1379\n",
      "51/463, train_loss: 0.1099\n",
      "52/463, train_loss: 0.1979\n",
      "53/463, train_loss: 0.2128\n",
      "54/463, train_loss: 0.0767\n",
      "55/463, train_loss: 0.1499\n",
      "56/463, train_loss: 0.2866\n",
      "57/463, train_loss: 0.1061\n",
      "58/463, train_loss: 0.1908\n",
      "59/463, train_loss: 0.0851\n",
      "60/463, train_loss: 0.0563\n",
      "61/463, train_loss: 0.1466\n",
      "62/463, train_loss: 0.1409\n",
      "63/463, train_loss: 0.1841\n",
      "64/463, train_loss: 0.0475\n",
      "65/463, train_loss: 0.0848\n",
      "66/463, train_loss: 0.1980\n",
      "67/463, train_loss: 0.1907\n",
      "68/463, train_loss: 0.5664\n",
      "69/463, train_loss: 0.1144\n",
      "70/463, train_loss: 0.0775\n",
      "71/463, train_loss: 0.4414\n",
      "72/463, train_loss: 0.5342\n",
      "73/463, train_loss: 0.0329\n",
      "74/463, train_loss: 0.0874\n",
      "75/463, train_loss: 0.2041\n",
      "76/463, train_loss: 0.1204\n",
      "77/463, train_loss: 0.2435\n",
      "78/463, train_loss: 0.0721\n",
      "79/463, train_loss: 0.1115\n",
      "80/463, train_loss: 0.1091\n",
      "81/463, train_loss: 0.2544\n",
      "82/463, train_loss: 0.1981\n",
      "83/463, train_loss: 0.0935\n",
      "84/463, train_loss: 0.1110\n",
      "85/463, train_loss: 0.1862\n",
      "86/463, train_loss: 0.2434\n",
      "87/463, train_loss: 0.1268\n",
      "88/463, train_loss: 0.1486\n",
      "89/463, train_loss: 0.1270\n",
      "90/463, train_loss: 0.2617\n",
      "91/463, train_loss: 0.1530\n",
      "92/463, train_loss: 0.1227\n",
      "93/463, train_loss: 0.1453\n",
      "94/463, train_loss: 0.1698\n",
      "95/463, train_loss: 0.0722\n",
      "96/463, train_loss: 0.0920\n",
      "97/463, train_loss: 0.2249\n",
      "98/463, train_loss: 0.1329\n",
      "99/463, train_loss: 0.1182\n",
      "100/463, train_loss: 0.1320\n",
      "101/463, train_loss: 0.1147\n",
      "102/463, train_loss: 0.4709\n",
      "103/463, train_loss: 0.1543\n",
      "104/463, train_loss: 0.8418\n",
      "105/463, train_loss: 0.0811\n",
      "106/463, train_loss: 0.2378\n",
      "107/463, train_loss: 0.1077\n",
      "108/463, train_loss: 0.1121\n",
      "109/463, train_loss: 0.2225\n",
      "110/463, train_loss: 0.3755\n",
      "111/463, train_loss: 0.1437\n",
      "112/463, train_loss: 0.0752\n",
      "113/463, train_loss: 0.1201\n",
      "114/463, train_loss: 0.1683\n",
      "115/463, train_loss: 0.0651\n",
      "116/463, train_loss: 0.1665\n",
      "117/463, train_loss: 0.2334\n",
      "118/463, train_loss: 0.1098\n",
      "119/463, train_loss: 0.2327\n",
      "120/463, train_loss: 0.1687\n",
      "121/463, train_loss: 0.1194\n",
      "122/463, train_loss: 0.0945\n",
      "123/463, train_loss: 0.1123\n",
      "124/463, train_loss: 0.1464\n",
      "125/463, train_loss: 0.0822\n",
      "126/463, train_loss: 0.3010\n",
      "127/463, train_loss: 0.0626\n",
      "128/463, train_loss: 0.1653\n",
      "129/463, train_loss: 0.1792\n",
      "130/463, train_loss: 0.2935\n",
      "131/463, train_loss: 0.2957\n",
      "132/463, train_loss: 0.1475\n",
      "133/463, train_loss: 0.2803\n",
      "134/463, train_loss: 0.1935\n",
      "135/463, train_loss: 0.0789\n",
      "136/463, train_loss: 0.2312\n",
      "137/463, train_loss: 0.2502\n",
      "138/463, train_loss: 0.1497\n",
      "139/463, train_loss: 0.1487\n",
      "140/463, train_loss: 0.1639\n",
      "141/463, train_loss: 0.2186\n",
      "142/463, train_loss: 0.0642\n",
      "143/463, train_loss: 0.0515\n",
      "144/463, train_loss: 0.1224\n",
      "145/463, train_loss: 0.0872\n",
      "146/463, train_loss: 0.0836\n",
      "147/463, train_loss: 0.1458\n",
      "148/463, train_loss: 0.2446\n",
      "149/463, train_loss: 0.1877\n",
      "150/463, train_loss: 0.1484\n",
      "151/463, train_loss: 0.0753\n",
      "152/463, train_loss: 0.0290\n",
      "153/463, train_loss: 0.2568\n",
      "154/463, train_loss: 0.0493\n",
      "155/463, train_loss: 0.1172\n",
      "156/463, train_loss: 0.1963\n",
      "157/463, train_loss: 0.1674\n",
      "158/463, train_loss: 0.1366\n",
      "159/463, train_loss: 0.1862\n",
      "160/463, train_loss: 0.1489\n",
      "161/463, train_loss: 0.1049\n",
      "162/463, train_loss: 0.5337\n",
      "163/463, train_loss: 0.1289\n",
      "164/463, train_loss: 0.2350\n",
      "165/463, train_loss: 0.1947\n",
      "166/463, train_loss: 0.4241\n",
      "167/463, train_loss: 0.2769\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m scheduler_warmup\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     34\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     35\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     36\u001b[0m         batch_data_ii[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m batch_data_i \u001b[38;5;129;01min\u001b[39;00m batch_data \u001b[38;5;28;01mfor\u001b[39;00m batch_data_ii \u001b[38;5;129;01min\u001b[39;00m batch_data_i\n\u001b[1;32m     37\u001b[0m     ]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 633\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    636\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    676\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 677\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    678\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    679\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/dataset.py:109\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(index, collections\u001b[38;5;241m.\u001b[39mabc\u001b[38;5;241m.\u001b[39mSequence):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# dataset[[1, 3, 4]]\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Subset(dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m, indices\u001b[38;5;241m=\u001b[39mindex)\n\u001b[0;32m--> 109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/dataset.py:95\u001b[0m, in \u001b[0;36mDataset._transform\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03mFetch single data item from `self.data`.\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     94\u001b[0m data_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata[index]\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_i\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m data_i\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MONAIEnvVars\u001b[38;5;241m.\u001b[39mdebug():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/compose.py:322\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, input_, start, end, threading, lazy)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_, start\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, threading\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, lazy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 322\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mexecute_compose\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m        \u001b[49m\u001b[43mend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m        \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mthreading\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreading\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlog_stats\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/compose.py:111\u001b[0m, in \u001b[0;36mexecute_compose\u001b[0;34m(data, transforms, map_items, unpack_items, start, end, lazy, overrides, threading, log_stats)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m threading:\n\u001b[1;32m    110\u001b[0m         _transform \u001b[38;5;241m=\u001b[39m deepcopy(_transform) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_transform, ThreadUnsafe) \u001b[38;5;28;01melse\u001b[39;00m _transform\n\u001b[0;32m--> 111\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43mapply_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_stats\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m apply_pending_transforms(data, \u001b[38;5;28;01mNone\u001b[39;00m, overrides, logger_name\u001b[38;5;241m=\u001b[39mlog_stats)\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/transform.py:141\u001b[0m, in \u001b[0;36mapply_transform\u001b[0;34m(transform, data, map_items, unpack_items, log_stats, lazy, overrides)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m map_items:\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_apply_transform(transform, item, unpack_items, lazy, overrides, log_stats) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m data]\n\u001b[0;32m--> 141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43munpack_items\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moverrides\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_stats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    143\u001b[0m     \u001b[38;5;66;03m# if in debug mode, don't swallow exception so that the breakpoint\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# appears where the exception was raised.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m MONAIEnvVars\u001b[38;5;241m.\u001b[39mdebug():\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/transform.py:98\u001b[0m, in \u001b[0;36m_apply_transform\u001b[0;34m(transform, data, unpack_parameters, lazy, overrides, logger_name)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m unpack_parameters:\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m transform(\u001b[38;5;241m*\u001b[39mdata)\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m transform(data, lazy\u001b[38;5;241m=\u001b[39mlazy) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(transform, LazyTrait) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/io/dictionary.py:164\u001b[0m, in \u001b[0;36mLoadImaged.__call__\u001b[0;34m(self, data, reader)\u001b[0m\n\u001b[1;32m    162\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(data)\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, meta_key, meta_key_postfix \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_iterator(d, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_keys, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta_key_postfix):\n\u001b[0;32m--> 164\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loader\u001b[38;5;241m.\u001b[39mimage_only:\n\u001b[1;32m    166\u001b[0m         d[key] \u001b[38;5;241m=\u001b[39m data\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/transforms/io/array.py:290\u001b[0m, in \u001b[0;36mLoadImage.__call__\u001b[0;34m(self, filename, reader)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    283\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cannot find a suitable reader for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    Please install the reader libraries, see also the installation instructions:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   The current registered: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreaders\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    287\u001b[0m     )\n\u001b[1;32m    289\u001b[0m img_array: NdarrayOrTensor\n\u001b[0;32m--> 290\u001b[0m img_array, meta_data \u001b[38;5;241m=\u001b[39m \u001b[43mreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m img_array \u001b[38;5;241m=\u001b[39m convert_to_dst_type(img_array, dst\u001b[38;5;241m=\u001b[39mimg_array, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(meta_data, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/image_reader.py:937\u001b[0m, in \u001b[0;36mNibabelReader.get_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    935\u001b[0m header[MetaKeys\u001b[38;5;241m.\u001b[39mSPATIAL_SHAPE] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_spatial_shape(i)\n\u001b[1;32m    936\u001b[0m header[MetaKeys\u001b[38;5;241m.\u001b[39mSPACE] \u001b[38;5;241m=\u001b[39m SpaceKeys\u001b[38;5;241m.\u001b[39mRAS\n\u001b[0;32m--> 937\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_array_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msqueeze_non_spatial_dims:\n\u001b[1;32m    939\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape), \u001b[38;5;28mlen\u001b[39m(header[MetaKeys\u001b[38;5;241m.\u001b[39mSPATIAL_SHAPE]), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/monai/data/image_reader.py:1011\u001b[0m, in \u001b[0;36mNibabelReader._get_array_data\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m   1003\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_array_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1005\u001b[0m \u001b[38;5;124;03m    Get the raw array data of the image, converted to Numpy array.\u001b[39;00m\n\u001b[1;32m   1006\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1009\u001b[0m \n\u001b[1;32m   1010\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1011\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nibabel/arrayproxy.py:439\u001b[0m, in \u001b[0;36mArrayProxy.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Read data from file and apply scaling, casting to ``dtype``\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \n\u001b[1;32m    421\u001b[0m \u001b[38;5;124;03m    If ``dtype`` is unspecified, the dtype of the returned array is the\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m \u001b[38;5;124;03m        Scaled image data with type `dtype`.\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 439\u001b[0m     arr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_scaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    441\u001b[0m         arr \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nibabel/arrayproxy.py:406\u001b[0m, in \u001b[0;36mArrayProxy._get_scaled\u001b[0;34m(self, dtype, slicer)\u001b[0m\n\u001b[1;32m    404\u001b[0m     scl_inter \u001b[38;5;241m=\u001b[39m scl_inter\u001b[38;5;241m.\u001b[39mastype(use_dtype)\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Read array and upcast as necessary for big slopes, intercepts\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m scaled \u001b[38;5;241m=\u001b[39m apply_read_scaling(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_unscaled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mslicer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mslicer\u001b[49m\u001b[43m)\u001b[49m, scl_slope, scl_inter)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    408\u001b[0m     scaled \u001b[38;5;241m=\u001b[39m scaled\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mpromote_types(scaled\u001b[38;5;241m.\u001b[39mdtype, dtype), copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nibabel/arrayproxy.py:376\u001b[0m, in \u001b[0;36mArrayProxy._get_unscaled\u001b[0;34m(self, slicer)\u001b[0m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m canonical_slicers(slicer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m==\u001b[39m canonical_slicers(\n\u001b[1;32m    373\u001b[0m     (), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shape, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    374\u001b[0m ):\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m--> 376\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marray_from_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfileobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m            \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m            \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmmap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mmap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_fileobj() \u001b[38;5;28;01mas\u001b[39;00m fileobj:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fileslice(\n\u001b[1;32m    386\u001b[0m         fileobj,\n\u001b[1;32m    387\u001b[0m         slicer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m         lock\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock,\n\u001b[1;32m    393\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/nibabel/volumeutils.py:465\u001b[0m, in \u001b[0;36marray_from_file\u001b[0;34m(shape, in_dtype, infile, offset, order, mmap)\u001b[0m\n\u001b[1;32m    463\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(infile, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreadinto\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    464\u001b[0m     data_bytes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(n_bytes)\n\u001b[0;32m--> 465\u001b[0m     n_read \u001b[38;5;241m=\u001b[39m \u001b[43minfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadinto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_bytes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m     needs_copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/gzip.py:292\u001b[0m, in \u001b[0;36mGzipFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01merrno\u001b[39;00m\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(errno\u001b[38;5;241m.\u001b[39mEBADF, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread() on write-only GzipFile object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.8/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/lib/python3.8/gzip.py:485\u001b[0m, in \u001b[0;36m_GzipReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new_member \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# Read a chunk of data from the file\u001b[39;00m\n\u001b[0;32m--> 485\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEFAULT_BUFFER_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m uncompress \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(buf, size)\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39munconsumed_tail \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m/usr/lib/python3.8/gzip.py:87\u001b[0m, in \u001b[0;36m_PaddedFile.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 87\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_length:\n\u001b[1;32m     89\u001b[0m         read \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_len = len_train_ds // train_loader.batch_size\n",
    "\n",
    "with mlflow.start_run(description=description) as run:\n",
    "\n",
    "    mlflow.log_param(\"gt_box_mode\", gt_box_mode)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"patch_size\", patch_size)\n",
    "    mlflow.log_param(\"data_list_file_path\", data_list_file_path)\n",
    "    mlflow.log_param(\"data_base_dir\", data_base_dir)\n",
    "    mlflow.log_param(\"amp\", amp)\n",
    "    \n",
    "    mlflow.log_param(\"n_input_channels\", n_input_channels)\n",
    "    mlflow.log_param(\"spatial_dims\", spatial_dims)\n",
    "    mlflow.log_param(\"balanced_sampler_pos_fraction\", balanced_sampler_pos_fraction)\n",
    "    mlflow.log_param(\"score_thresh\", score_thresh)\n",
    "    mlflow.log_param(\"nms_thresh\", nms_thresh)\n",
    "    \n",
    "    mlflow.log_param(\"initial_lr\", lr)\n",
    "    mlflow.log_param(\"val_interval\", val_interval)\n",
    "    mlflow.log_param(\"max_epochs\", max_epochs)\n",
    "    mlflow.log_param(\"w_cls\", w_cls)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        detector.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_cls_loss = 0\n",
    "        epoch_box_reg_loss = 0\n",
    "        step = 0\n",
    "        scheduler_warmup.step()\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = [\n",
    "                batch_data_ii[\"image\"].to(device) for batch_data_i in batch_data for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "            targets = [\n",
    "                dict(\n",
    "                    label=batch_data_ii[\"label\"].to(device),\n",
    "                    box=batch_data_ii[\"box\"].to(device)\n",
    "                )\n",
    "                for batch_data_i in batch_data\n",
    "                for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "\n",
    "            for param in detector.network.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            if amp and (scaler is not None):\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = detector(inputs, targets)\n",
    "                    loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = detector(inputs, targets)\n",
    "                loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # saving into mlflow\n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_cls_loss += outputs[detector.cls_key].detach().item()\n",
    "            epoch_box_reg_loss += outputs[detector.box_reg_key].detach().item()\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            mlflow.log_metric(\"train_loss\", loss.detach().item(), epoch_len * epoch + step)\n",
    "\n",
    "        del inputs, batch_data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_cls_loss /= step\n",
    "        epoch_box_reg_loss /= step\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        mlflow.log_metric(\"avg_train_loss\", epoch_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_cls_loss\", epoch_cls_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_box_reg_loss\", epoch_box_reg_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"train_lr\", optimizer.param_groups[0][\"lr\"], epoch + 1)\n",
    "\n",
    "        # saving last trained model\n",
    "        mlflow.pytorch.log_model(detector.network, \"model\")\n",
    "\n",
    "        # validation for model selection\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            detector.eval()\n",
    "            val_outputs_all = []\n",
    "            val_targets_all = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    # if all val_data_i[\"image\"] smaller than val_patch_size, no need to use inferer\n",
    "                    # otherwise, need inferer to handle large input images.\n",
    "                    use_inferer = not all(\n",
    "                        [val_data_i[\"image\"][0, ...].numel() < np.prod(val_patch_size) for val_data_i in val_data]\n",
    "                    )\n",
    "                    val_inputs = [val_data_i.pop(\"image\").to(device) for val_data_i in val_data]\n",
    "\n",
    "                    if amp:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "                    else:\n",
    "                        val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "\n",
    "                    # save outputs for evaluation\n",
    "                    val_outputs_all += val_outputs\n",
    "                    val_targets_all += val_data\n",
    "\n",
    "            # visualize an inference image and boxes\n",
    "            print(val_data[0][\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "            draw_img = visualize_one_xy_slice_in_3d_image(\n",
    "                gt_boxes=val_data[0][\"box\"].cpu().detach().numpy(),\n",
    "                image=val_inputs[0][0, ...].cpu().detach().numpy(),\n",
    "                pred_boxes=val_outputs[0][detector.target_box_key].cpu().detach().numpy(),\n",
    "            )\n",
    "            # mlflow.log_image(draw_img.transpose([2, 1, 0]), \"val_img_xy.png\")\n",
    "            mlflow.log_image(draw_img, str(epoch + 1) + \"_val_img_xy.png\")\n",
    "\n",
    "            # compute metrics\n",
    "            del val_inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            results_metric = matching_batch(\n",
    "                iou_fn=box_utils.box_iou,\n",
    "                iou_thresholds=coco_metric.iou_thresholds,\n",
    "                pred_boxes=[\n",
    "                    val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_scores=[\n",
    "                    val_data_i[detector.pred_score_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                gt_boxes=[val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_targets_all],\n",
    "                gt_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_targets_all\n",
    "                ]\n",
    "            )\n",
    "            val_epoch_metric_dict = coco_metric(results_metric)[0]\n",
    "            print(val_epoch_metric_dict)\n",
    "\n",
    "            # write metrics\n",
    "            for k in val_epoch_metric_dict.keys():\n",
    "                mlflow.log_metric(\"val_\" + k, val_epoch_metric_dict[k], epoch + 1)\n",
    "            val_epoch_metric = val_epoch_metric_dict.values()\n",
    "            val_epoch_metric = sum(val_epoch_metric) / len(val_epoch_metric)\n",
    "            mlflow.log_metric(\"val_metric\", val_epoch_metric, epoch + 1)\n",
    "\n",
    "            # save best trained model\n",
    "            if val_epoch_metric > best_val_epoch_metric:\n",
    "                best_val_epoch_metric = val_epoch_metric\n",
    "                best_val_epoch = epoch + 1\n",
    "                mlflow.pytorch.log_model(detector.network, \"best_model\")\n",
    "            print(\n",
    "                \"current epoch: {} current metric: {:.4f} \"\n",
    "                \"best metric: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, val_epoch_metric, best_val_epoch_metric, best_val_epoch\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(f\"train completed, best_metric: {best_val_epoch_metric:.4f} \" f\"at epoch: {best_val_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79dfad-c3c3-4e70-9f34-cee54b016a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
