{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6948feac-77f3-43ab-91e3-5e2a79c3dca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['MLFLOW_TRACKING_USERNAME'] = 'Eddy'\n",
    "os.environ['MLFLOW_TRACKING_PASSWORD'] = 'Usp1#'\n",
    "os.environ['MLFLOW_EXPERIMENT_NAME'] = 'LUNA16 + HC + MSD - without lung seg'\n",
    "description = \"Mixed dataset training without lung segmentation\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8931abf-4b0b-4731-adf9-7c5b2840c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import mlflow\n",
    "import gc\n",
    "\n",
    "from visualize_image import visualize_one_xy_slice_in_3d_image\n",
    "from loading_dataset import load_data\n",
    "from model import load_model\n",
    "import numpy as np\n",
    "from monai.data import box_utils\n",
    "from monai.apps.detection.metrics.matching import matching_batch\n",
    "from warmup_scheduler import GradualWarmupScheduler\n",
    "from monai.apps.detection.metrics.coco import COCOMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2c25b3-b7bc-4abc-856a-f2a4d88259bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_box_mode = 'cccwhd'\n",
    "batch_size = 8\n",
    "patch_size = [96,96,40]\n",
    "data_list_file_path = '/data/output/mixed_data/mixed_train_val0.json'\n",
    "data_base_dir = ''\n",
    "# data_base_dir = '/data/HC_Images_resample/'\n",
    "# data_base_dir = '/data/MSD_Images_resample/'\n",
    "# data_base_dir = '/data/LUNA16_Images_resample/'\n",
    "amp=True\n",
    "\n",
    "returned_layers = [1,2]\n",
    "base_anchor_shapes = [[6,8,4],[8,6,5],[10,10,6]]\n",
    "conv1_t_stride = [2,2,1]\n",
    "n_input_channels = 1\n",
    "spatial_dims = 3\n",
    "fg_labels = [0]\n",
    "verbose = False\n",
    "balanced_sampler_pos_fraction = 0.3\n",
    "score_thresh = 0.02\n",
    "nms_thresh = 0.22\n",
    "val_patch_size = [256,256,104]\n",
    "\n",
    "lr = 1e-2\n",
    "val_interval = 5\n",
    "coco_metric = COCOMetric(classes=[\"nodule\"], iou_list=[0.1], max_detection=[100])\n",
    "best_val_epoch_metric = 0.0\n",
    "best_val_epoch = -1\n",
    "max_epochs = 100\n",
    "w_cls = 1.0\n",
    "\n",
    "compute_dtype = torch.float32\n",
    "if amp:\n",
    "    compute_dtype = torch.float16\n",
    "\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff675dc4-c1a7-4f49-8c72-938dca13e743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.2.0\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 2.0.1+cu118\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: c33f1ba588ee00229a309000e888f9817b4f1934\n",
      "MONAI __file__: /usr/local/lib/python3.8/dist-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: 5.3.0\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: 0.21.0\n",
      "Pillow version: 10.0.0\n",
      "Tensorboard version: 2.14.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.2+cu118\n",
      "tqdm version: 4.66.1\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.5\n",
      "pandas version: 2.0.3\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: 2.6.0\n",
      "pynrrd version: 1.0.0\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import monai\n",
    "\n",
    "monai.config.print_config()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1ad2cc-31a4-4f4d-a131-256b6cfd9e61",
   "metadata": {},
   "source": [
    "### loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c9bf6dd-f147-4974-8b8b-54204a268866",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "monai.transforms.io.dictionary LoadImaged.__init__:image_only: Current default value of argument `image_only=False` has been deprecated since version 1.1. It will be changed to `image_only=True` in version 1.3.\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, len_train_ds = load_data(\n",
    "    gt_box_mode, patch_size, batch_size, amp, data_list_file_path, data_base_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b9e514-7ab6-45dd-bd31-2f7bf68ff2d6",
   "metadata": {},
   "source": [
    "### loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bf21214-041f-401e-a2f7-f0072929bf90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "detector, device = load_model(\n",
    "    returned_layers, base_anchor_shapes, conv1_t_stride, n_input_channels,\n",
    "    spatial_dims, fg_labels, verbose, balanced_sampler_pos_fraction,\n",
    "    score_thresh, nms_thresh, val_patch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7938680-00c0-4d79-bb31-4320bac9e332",
   "metadata": {},
   "source": [
    "### Initialize training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7489f0c5-5ffc-4005-994d-8d74c85cae8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(\n",
    "    detector.network.parameters(),\n",
    "    lr,\n",
    "    momentum=0.9,\n",
    "    weight_decay=3e-5,\n",
    "    nesterov=True\n",
    ")\n",
    "\n",
    "after_scheduler = torch.optim.lr_scheduler.StepLR(\n",
    "    optimizer, step_size=150, gamma=0.1\n",
    ")\n",
    "scheduler_warmup = GradualWarmupScheduler(\n",
    "    optimizer, multiplier=1, total_epoch=10, after_scheduler=after_scheduler\n",
    ")\n",
    "scaler = torch.cuda.amp.GradScaler() if amp else None\n",
    "optimizer.zero_grad()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6ca37e-e17d-4e84-b9bc-4029ba3dcaf2",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5beae8c-e323-4dd2-84b6-eeebf82bf10c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/100\n",
      "1/463, train_loss: 1.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epoch_len = len_train_ds // train_loader.batch_size\n",
    "\n",
    "with mlflow.start_run(description=description) as run:\n",
    "\n",
    "    mlflow.log_param(\"gt_box_mode\", gt_box_mode)\n",
    "    mlflow.log_param(\"batch_size\", batch_size)\n",
    "    mlflow.log_param(\"patch_size\", patch_size)\n",
    "    mlflow.log_param(\"data_list_file_path\", data_list_file_path)\n",
    "    mlflow.log_param(\"data_base_dir\", data_base_dir)\n",
    "    mlflow.log_param(\"amp\", amp)\n",
    "    \n",
    "    mlflow.log_param(\"n_input_channels\", n_input_channels)\n",
    "    mlflow.log_param(\"spatial_dims\", spatial_dims)\n",
    "    mlflow.log_param(\"balanced_sampler_pos_fraction\", balanced_sampler_pos_fraction)\n",
    "    mlflow.log_param(\"score_thresh\", score_thresh)\n",
    "    mlflow.log_param(\"nms_thresh\", nms_thresh)\n",
    "    \n",
    "    mlflow.log_param(\"initial_lr\", lr)\n",
    "    mlflow.log_param(\"val_interval\", val_interval)\n",
    "    mlflow.log_param(\"max_epochs\", max_epochs)\n",
    "    mlflow.log_param(\"w_cls\", w_cls)\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        print(\"-\" * 10)\n",
    "        print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "        detector.train()\n",
    "        epoch_loss = 0\n",
    "        epoch_cls_loss = 0\n",
    "        epoch_box_reg_loss = 0\n",
    "        step = 0\n",
    "        scheduler_warmup.step()\n",
    "\n",
    "        for batch_data in train_loader:\n",
    "            step += 1\n",
    "            inputs = [\n",
    "                batch_data_ii[\"image\"].to(device) for batch_data_i in batch_data for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "            targets = [\n",
    "                dict(\n",
    "                    label=batch_data_ii[\"label\"].to(device),\n",
    "                    box=batch_data_ii[\"box\"].to(device)\n",
    "                )\n",
    "                for batch_data_i in batch_data\n",
    "                for batch_data_ii in batch_data_i\n",
    "            ]\n",
    "\n",
    "            for param in detector.network.parameters():\n",
    "                param.grad = None\n",
    "\n",
    "            if amp and (scaler is not None):\n",
    "                with torch.cuda.amp.autocast():\n",
    "                    outputs = detector(inputs, targets)\n",
    "                    loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                outputs = detector(inputs, targets)\n",
    "                loss = w_cls * outputs[detector.cls_key] + outputs[detector.box_reg_key]\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # saving into mlflow\n",
    "            epoch_loss += loss.detach().item()\n",
    "            epoch_cls_loss += outputs[detector.cls_key].detach().item()\n",
    "            epoch_box_reg_loss += outputs[detector.box_reg_key].detach().item()\n",
    "            print(f\"{step}/{epoch_len}, train_loss: {loss.item():.4f}\")\n",
    "            mlflow.log_metric(\"train_loss\", loss.detach().item(), epoch_len * epoch + step)\n",
    "\n",
    "        del inputs, batch_data\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "        epoch_loss /= step\n",
    "        epoch_cls_loss /= step\n",
    "        epoch_box_reg_loss /= step\n",
    "        print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "        mlflow.log_metric(\"avg_train_loss\", epoch_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_cls_loss\", epoch_cls_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"avg_train_box_reg_loss\", epoch_box_reg_loss, epoch + 1)\n",
    "        mlflow.log_metric(\"train_lr\", optimizer.param_groups[0][\"lr\"], epoch + 1)\n",
    "\n",
    "        # saving last trained model\n",
    "        mlflow.pytorch.log_model(detector.network, \"model\")\n",
    "\n",
    "        # validation for model selection\n",
    "        if (epoch + 1) % val_interval == 0:\n",
    "            detector.eval()\n",
    "            val_outputs_all = []\n",
    "            val_targets_all = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for val_data in val_loader:\n",
    "                    # if all val_data_i[\"image\"] smaller than val_patch_size, no need to use inferer\n",
    "                    # otherwise, need inferer to handle large input images.\n",
    "                    use_inferer = not all(\n",
    "                        [val_data_i[\"image\"][0, ...].numel() < np.prod(val_patch_size) for val_data_i in val_data]\n",
    "                    )\n",
    "                    val_inputs = [val_data_i.pop(\"image\").to(device) for val_data_i in val_data]\n",
    "\n",
    "                    if amp:\n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "                    else:\n",
    "                        val_outputs = detector(val_inputs, use_inferer=use_inferer)\n",
    "\n",
    "                    # save outputs for evaluation\n",
    "                    val_outputs_all += val_outputs\n",
    "                    val_targets_all += val_data\n",
    "\n",
    "            # visualize an inference image and boxes\n",
    "            print(val_data[0][\"image_meta_dict\"][\"filename_or_obj\"])\n",
    "            draw_img = visualize_one_xy_slice_in_3d_image(\n",
    "                gt_boxes=val_data[0][\"box\"].cpu().detach().numpy(),\n",
    "                image=val_inputs[0][0, ...].cpu().detach().numpy(),\n",
    "                pred_boxes=val_outputs[0][detector.target_box_key].cpu().detach().numpy(),\n",
    "            )\n",
    "            # mlflow.log_image(draw_img.transpose([2, 1, 0]), \"val_img_xy.png\")\n",
    "            mlflow.log_image(draw_img, str(epoch + 1) + \"_val_img_xy.png\")\n",
    "\n",
    "            # compute metrics\n",
    "            del val_inputs\n",
    "            torch.cuda.empty_cache()\n",
    "            results_metric = matching_batch(\n",
    "                iou_fn=box_utils.box_iou,\n",
    "                iou_thresholds=coco_metric.iou_thresholds,\n",
    "                pred_boxes=[\n",
    "                    val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                pred_scores=[\n",
    "                    val_data_i[detector.pred_score_key].cpu().detach().numpy() for val_data_i in val_outputs_all\n",
    "                ],\n",
    "                gt_boxes=[val_data_i[detector.target_box_key].cpu().detach().numpy() for val_data_i in val_targets_all],\n",
    "                gt_classes=[\n",
    "                    val_data_i[detector.target_label_key].cpu().detach().numpy() for val_data_i in val_targets_all\n",
    "                ]\n",
    "            )\n",
    "            val_epoch_metric_dict = coco_metric(results_metric)[0]\n",
    "            print(val_epoch_metric_dict)\n",
    "\n",
    "            # write metrics\n",
    "            for k in val_epoch_metric_dict.keys():\n",
    "                mlflow.log_metric(\"val_\" + k, val_epoch_metric_dict[k], epoch + 1)\n",
    "            val_epoch_metric = val_epoch_metric_dict.values()\n",
    "            val_epoch_metric = sum(val_epoch_metric) / len(val_epoch_metric)\n",
    "            mlflow.log_metric(\"val_metric\", val_epoch_metric, epoch + 1)\n",
    "\n",
    "            # save best trained model\n",
    "            if val_epoch_metric > best_val_epoch_metric:\n",
    "                best_val_epoch_metric = val_epoch_metric\n",
    "                best_val_epoch = epoch + 1\n",
    "                mlflow.pytorch.log_model(detector.network, \"best_model\")\n",
    "            print(\n",
    "                \"current epoch: {} current metric: {:.4f} \"\n",
    "                \"best metric: {:.4f} at epoch {}\".format(\n",
    "                    epoch + 1, val_epoch_metric, best_val_epoch_metric, best_val_epoch\n",
    "                )\n",
    "            )\n",
    "\n",
    "    print(f\"train completed, best_metric: {best_val_epoch_metric:.4f} \" f\"at epoch: {best_val_epoch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79dfad-c3c3-4e70-9f34-cee54b016a7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
